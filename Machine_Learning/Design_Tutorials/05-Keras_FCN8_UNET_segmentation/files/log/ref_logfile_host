/**

* Â© Copyright (C) 2016-2020 Xilinx, Inc
*
* Licensed under the Apache License, Version 2.0 (the "License"). You may
* not use this file except in compliance with the License. A copy of the
* License is located at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
* WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
* License for the specific language governing permissions and limitations
* under the License.
*/

/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/run_fcn8ups.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/run_fcn8.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_vck190/code/run_cnn_py_fps.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_vck190/code/build_app.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_vck190/run_all_target.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/run_unet.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/run_all.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu102/code/build.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu102/code/run_cnn_py_fps.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu102/code/build_app.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu102/run_all_target.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu102/run_all.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu104/code/run_cnn_py_fps.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu104/code/build_app.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/target_zcu104/run_all_target.sh
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/open_pb_graph_in_tensorBoard.sh

bash:
##################################################################################
A) CLEAN PREVIOUS DIRECTORIES
##################################################################################


##################################################################################
Step1: CREATE DATA AND FOLDERS
##################################################################################

Archive:  ./build/../dataset1.zip
   creating: ./build/dataset1/annotations_prepped_test/
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08085.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08039.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07995.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08067.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08073.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08123.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08159.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08157.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08047.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08075.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08121.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08053.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08119.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08103.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07981.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08051.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07973.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08059.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08105.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08153.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08127.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08011.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07989.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07997.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08015.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08139.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08095.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07993.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08061.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07985.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08081.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08071.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08001.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08007.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08143.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07969.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08089.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08055.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08035.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08107.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08025.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08023.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08077.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08125.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08041.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08115.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08017.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08141.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08043.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08117.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08079.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08013.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08135.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08003.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07987.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08113.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08129.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07977.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08045.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07971.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08101.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07967.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08063.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08065.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08133.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08111.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07991.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08021.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08005.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07999.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07963.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08151.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07961.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08145.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08027.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08037.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08099.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08019.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08031.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08091.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08097.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08029.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08155.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08069.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07975.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07965.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07959.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08109.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08131.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08083.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08049.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08147.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08137.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08087.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08033.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07983.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08057.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_07979.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08009.png
 extracting: ./build/dataset1/annotations_prepped_test/0016E5_08093.png
  inflating: ./build/dataset1/annotations_prepped_test/0016E5_08149.png
   creating: ./build/dataset1/images_prepped_train/
  inflating: ./build/dataset1/images_prepped_train/0016E5_01740.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02910.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01170.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06390.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03870.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01440.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008520.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03120.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006840.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06750.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02820.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04860.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006930.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02640.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04530.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04380.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008220.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01890.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05280.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05250.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04890.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01320.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03810.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007020.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01080.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00720.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03390.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05730.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008490.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01470.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02040.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007830.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06960.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01590.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01860.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007650.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00630.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007860.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07080.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06930.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08490.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01050.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03720.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01500.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02130.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06360.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08550.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07740.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08250.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02580.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01260.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07920.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05670.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06270.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007440.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02160.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07800.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007770.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007110.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007140.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00990.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02310.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05100.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05760.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07890.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007230.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06570.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006870.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00750.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03510.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05400.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08370.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02520.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00690.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01230.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08310.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007320.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02370.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06330.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05040.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01530.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07290.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02460.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f00990.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006990.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00480.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008190.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05790.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02340.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08400.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01950.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07170.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01650.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04740.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03480.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03180.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05850.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02250.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008130.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01770.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05370.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06000.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06840.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00600.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006900.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05220.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02190.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07500.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06510.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08220.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05340.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008100.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007530.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03780.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04410.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03660.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03150.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07050.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05550.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08640.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05010.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03300.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05640.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008310.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05310.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02850.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08610.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04440.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01920.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04500.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02310.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06600.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007050.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00450.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008010.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01350.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05880.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04650.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01080.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05970.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06150.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008460.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01830.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02760.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07560.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00840.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007410.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03060.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01800.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01860.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05430.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07770.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03600.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03030.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00660.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06450.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02040.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01410.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007800.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01380.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007350.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01560.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01980.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01710.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04800.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02430.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07620.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03540.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007740.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06990.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01680.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01290.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05070.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04620.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02790.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007590.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07860.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03840.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04950.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02250.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007950.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04470.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02220.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05490.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05820.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02340.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02130.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008340.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008160.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07260.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006780.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007170.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07350.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00570.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06720.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08190.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08580.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01020.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01440.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008400.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07320.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01350.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02280.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01050.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01200.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01890.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04770.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f00960.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05700.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01560.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01410.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05190.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06210.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01020.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007500.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01770.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06780.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03210.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03900.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06120.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07830.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01110.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03750.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00510.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007200.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02280.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008280.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01470.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07110.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01620.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01530.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06540.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06180.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03330.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07380.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06240.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07200.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06300.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07140.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05910.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01140.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02940.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04680.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07470.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02670.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008370.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01200.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06870.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03450.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00390.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01710.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03240.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07590.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00960.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02370.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007620.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05520.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08460.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02100.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05610.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00870.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08430.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01290.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00420.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007920.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06060.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01260.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02970.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07230.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02070.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07410.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02730.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02220.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01680.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01650.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06420.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02010.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01590.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007080.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03930.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03270.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01620.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006750.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03570.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03420.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f00930.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007890.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01140.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05580.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02400.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06630.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04560.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01230.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04830.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08280.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05130.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06030.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07680.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008430.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01920.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007710.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07440.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007380.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03090.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05940.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02880.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08520.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05160.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01170.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007680.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01980.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00780.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00901.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01830.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03690.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006690.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03360.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01950.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06090.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04350.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008040.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03630.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01740.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07530.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07650.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00930.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02070.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01320.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00540.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01800.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02160.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02100.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02700.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02400.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07020.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008070.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f03000.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06900.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04710.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007560.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007290.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_008250.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06480.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007980.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01500.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_08340.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02550.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007470.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006810.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006960.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_00810.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04590.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02490.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_05460.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02610.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_07710.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_006720.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06810.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_01380.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_02010.png
  inflating: ./build/dataset1/images_prepped_train/0001TP_007260.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f02190.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06660.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04920.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_06690.png
  inflating: ./build/dataset1/images_prepped_train/0016E5_04980.png
  inflating: ./build/dataset1/images_prepped_train/0006R0_f01110.png
   creating: ./build/dataset1/annotations_prepped_train/
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01740.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02910.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01170.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06390.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03870.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01440.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008520.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03120.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006840.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06750.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02820.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04860.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006930.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02640.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04530.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04380.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008220.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01890.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05280.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05250.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04890.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01320.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03810.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007020.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01080.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00720.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03390.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05730.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008490.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01470.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02040.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007830.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06960.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01590.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01860.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007650.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00630.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007860.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07080.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06930.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08490.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01050.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03720.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01500.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02130.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06360.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_08550.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07740.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08250.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02580.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01260.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07920.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05670.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06270.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007440.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02160.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07800.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007770.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007110.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007140.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00990.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02310.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05100.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05760.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07890.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007230.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06570.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006870.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_00750.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03510.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05400.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08370.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f02520.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00690.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01230.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_08310.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007320.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02370.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06330.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05040.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01530.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07290.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02460.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f00990.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006990.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00480.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008190.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05790.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02340.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08400.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01950.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07170.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01650.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04740.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03480.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03180.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05850.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02250.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008130.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01770.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05370.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06000.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06840.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00600.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006900.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05220.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02190.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07500.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06510.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08220.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05340.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008100.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007530.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03780.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04410.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03660.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03150.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07050.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05550.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08640.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05010.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03300.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05640.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008310.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05310.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02850.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08610.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04440.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01920.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04500.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02310.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06600.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007050.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00450.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008010.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01350.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05880.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04650.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01080.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05970.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06150.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008460.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01830.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02760.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07560.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00840.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007410.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03060.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01800.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01860.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05430.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07770.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03600.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03030.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00660.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06450.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f02040.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01410.png
  inflating: ./build/dataset1/annotations_prepped_train/0001TP_007800.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01380.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007350.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01560.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01980.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01710.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04800.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02430.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07620.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03540.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007740.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06990.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01680.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01290.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05070.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_04620.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02790.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007590.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07860.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03840.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04950.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02250.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007950.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04470.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02220.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05490.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05820.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02340.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02130.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008340.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008160.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07260.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006780.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007170.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07350.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00570.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06720.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08190.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08580.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01020.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01440.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008400.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07320.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01350.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02280.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01050.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01200.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01890.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04770.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f00960.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05700.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f01560.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01410.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05190.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06210.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01020.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007500.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01770.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06780.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03210.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03900.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06120.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07830.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01110.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03750.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00510.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007200.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02280.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008280.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01470.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07110.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01620.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01530.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06540.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06180.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03330.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07380.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06240.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07200.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06300.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07140.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05910.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01140.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02940.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04680.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07470.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02670.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008370.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01200.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06870.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03450.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00390.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01710.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03240.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07590.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00960.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02370.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007620.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05520.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08460.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02100.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05610.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00870.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_08430.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01290.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_00420.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007920.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06060.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01260.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02970.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07230.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_02070.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07410.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02730.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02220.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01680.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01650.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06420.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f02010.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01590.png
  inflating: ./build/dataset1/annotations_prepped_train/0001TP_007080.png
 extracting: ./build/dataset1/annotations_prepped_train/0006R0_f03930.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03270.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01620.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006750.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03570.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03420.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f00930.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007890.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01140.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05580.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02400.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06630.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04560.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01230.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04830.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_08280.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05130.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_06030.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07680.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008430.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01920.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007710.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07440.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007380.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03090.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05940.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02880.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_08520.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_05160.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_01170.png
  inflating: ./build/dataset1/annotations_prepped_train/0001TP_007680.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01980.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00780.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00901.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01830.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03690.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006690.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03360.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01950.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06090.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_04350.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008040.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03630.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01740.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_07530.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07650.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00930.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02070.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01320.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00540.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01800.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_02160.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02100.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02700.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_02400.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07020.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008070.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f03000.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06900.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04710.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007560.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007290.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_008250.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06480.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007980.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01500.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_08340.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02550.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007470.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006810.png
  inflating: ./build/dataset1/annotations_prepped_train/0001TP_006960.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_00810.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04590.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02490.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_05460.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02610.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_07710.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_006720.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06810.png
 extracting: ./build/dataset1/annotations_prepped_train/0016E5_01380.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_02010.png
 extracting: ./build/dataset1/annotations_prepped_train/0001TP_007260.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f02190.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06660.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04920.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_06690.png
  inflating: ./build/dataset1/annotations_prepped_train/0016E5_04980.png
  inflating: ./build/dataset1/annotations_prepped_train/0006R0_f01110.png
   creating: ./build/dataset1/images_prepped_test/
  inflating: ./build/dataset1/images_prepped_test/0016E5_08085.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08039.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07995.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08067.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08073.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08123.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08159.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08157.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08047.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08075.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08121.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08053.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08119.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08103.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07981.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08051.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07973.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08059.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08105.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08153.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08127.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08011.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07989.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07997.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08015.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08139.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08095.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07993.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08061.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07985.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08081.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08071.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08001.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08007.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08143.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07969.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08089.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08055.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08035.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08107.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08025.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08023.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08077.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08125.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08041.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08115.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08017.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08141.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08043.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08117.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08079.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08013.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08135.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08003.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07987.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08113.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08129.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07977.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08045.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07971.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08101.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07967.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08063.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08065.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08133.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08111.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07991.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08021.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08005.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07999.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07963.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08151.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07961.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08145.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08027.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08037.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08099.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08019.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08031.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08091.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08097.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08029.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08155.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08069.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07975.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07965.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07959.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08109.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08131.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08083.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08049.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08147.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08137.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08087.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08033.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07983.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08057.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_07979.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08009.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08093.png
  inflating: ./build/dataset1/images_prepped_test/0016E5_08149.png
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code


python 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56)
[GCC 7.3.0]
keras version 2.2.4-tf
tensorflow version 1.15.2
channels_last




written 311 training images
written 104 calibr.  images
written  56 valid.   images
written 101 test     images
--2021-01-04 13:41:48--  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
Resolving github.com (github.com)... 140.82.121.4
Connecting to github.com (github.com)|140.82.121.4|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210104T134148Z&X-Amz-Expires=300&X-Amz-Signature=b8f081818ac02492f4b7a569f0a503cfd84796af2b56217d7ad3207fe8f76544&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream [following]
--2021-01-04 13:41:49--  https://github-production-release-asset-2e65be.s3.amazonaws.com/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210104%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210104T134148Z&X-Amz-Expires=300&X-Amz-Signature=b8f081818ac02492f4b7a569f0a503cfd84796af2b56217d7ad3207fe8f76544&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream
Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.153.188
Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.153.188|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 58889256 (56M) [application/octet-stream]
Saving to: âvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5.1â

     0K .......... .......... .......... .......... ..........  0%  232K 4m7s
    50K .......... .......... .......... .......... ..........  0% 10.2M 2m6s
   100K .......... .......... .......... .......... ..........  0%  471K 2m5s
   150K .......... .......... .......... .......... ..........  0% 10.3M 95s
   200K .......... .......... .......... .......... ..........  0%  497K 99s
   250K .......... .......... .......... .......... ..........  0% 10.5M 83s
   300K .......... .......... .......... .......... ..........  0% 10.3M 72s
   350K .......... .......... .......... .......... ..........  0% 9.88M 64s
   400K .......... .......... .......... .......... ..........  0% 10.3M 57s
   450K .......... .......... .......... .......... ..........  0%  567K 61s
   500K .......... .......... .......... .......... ..........  0% 9.91M 56s
   550K .......... .......... .......... .......... ..........  1% 10.2M 52s
   600K .......... .......... .......... .......... ..........  1% 10.3M 48s
   650K .......... .......... .......... .......... ..........  1% 10.2M 45s
   700K .......... .......... .......... .......... ..........  1% 10.3M 43s
   750K .......... .......... .......... .......... ..........  1% 10.3M 40s
   800K .......... .......... .......... .......... ..........  1% 10.2M 38s
   850K .......... .......... .......... .......... ..........  1% 14.2M 36s
   900K .......... .......... .......... .......... ..........  1% 10.6M 35s
   950K .......... .......... .......... .......... ..........  1% 10.1M 33s
  1000K .......... .......... .......... .......... ..........  1%  775K 35s
  1050K .......... .......... .......... .......... ..........  1% 10.4M 34s
  1100K .......... .......... .......... .......... ..........  1% 9.72M 32s
  1150K .......... .......... .......... .......... ..........  2% 10.3M 31s
  1200K .......... .......... .......... .......... ..........  2% 10.2M 30s
  1250K .......... .......... .......... .......... ..........  2% 9.84M 29s
  1300K .......... .......... .......... .......... ..........  2% 11.0M 28s
  1350K .......... .......... .......... .......... ..........  2% 9.75M 27s
  1400K .......... .......... .......... .......... ..........  2% 10.3M 27s
  1450K .......... .......... .......... .......... ..........  2% 9.83M 26s
  1500K .......... .......... .......... .......... ..........  2% 10.3M 25s
  1550K .......... .......... .......... .......... ..........  2% 10.2M 25s
  1600K .......... .......... .......... .......... ..........  2% 10.3M 24s
  1650K .......... .......... .......... .......... ..........  2% 10.3M 23s
  1700K .......... .......... .......... .......... ..........  3% 14.2M 23s
  1750K .......... .......... .......... .......... ..........  3% 10.4M 22s
  1800K .......... .......... .......... .......... ..........  3% 9.79M 22s
  1850K .......... .......... .......... .......... ..........  3% 10.3M 21s
  1900K .......... .......... .......... .......... ..........  3% 10.3M 21s
  1950K .......... .......... .......... .......... ..........  3% 9.68M 21s
  2000K .......... .......... .......... .......... ..........  3% 3.16M 20s
  2050K .......... .......... .......... .......... ..........  3% 10.2M 20s
  2100K .......... .......... .......... .......... ..........  3% 10.2M 20s
  2150K .......... .......... .......... .......... ..........  3% 10.1M 19s
  2200K .......... .......... .......... .......... ..........  3% 10.3M 19s
  2250K .......... .......... .......... .......... ..........  3% 10.3M 19s
  2300K .......... .......... .......... .......... ..........  4% 10.2M 18s
  2350K .......... .......... .......... .......... ..........  4% 10.3M 18s
  2400K .......... .......... .......... .......... ..........  4% 10.7M 18s
  2450K .......... .......... .......... .......... ..........  4% 10.8M 18s
  2500K .......... .......... .......... .......... ..........  4% 10.7M 17s
  2550K .......... .......... .......... .......... ..........  4% 10.9M 17s
  2600K .......... .......... .......... .......... ..........  4% 8.26M 17s
  2650K .......... .......... .......... .......... ..........  4% 10.9M 17s
  2700K .......... .......... .......... .......... ..........  4% 11.0M 16s
  2750K .......... .......... .......... .......... ..........  4% 10.7M 16s
  2800K .......... .......... .......... .......... ..........  4% 10.8M 16s
  2850K .......... .......... .......... .......... ..........  5% 10.2M 16s
  2900K .......... .......... .......... .......... ..........  5% 11.4M 16s
  2950K .......... .......... .......... .......... ..........  5% 11.0M 15s
  3000K .......... .......... .......... .......... ..........  5% 8.29M 15s
  3050K .......... .......... .......... .......... ..........  5% 9.77M 15s
  3100K .......... .......... .......... .......... ..........  5% 11.4M 15s
  3150K .......... .......... .......... .......... ..........  5% 10.9M 15s
  3200K .......... .......... .......... .......... ..........  5% 10.5M 14s
  3250K .......... .......... .......... .......... ..........  5% 11.0M 14s
  3300K .......... .......... .......... .......... ..........  5% 10.8M 14s
  3350K .......... .......... .......... .......... ..........  5% 10.7M 14s
  3400K .......... .......... .......... .......... ..........  5% 8.29M 14s
  3450K .......... .......... .......... .......... ..........  6% 10.8M 14s
  3500K .......... .......... .......... .......... ..........  6% 10.8M 14s
  3550K .......... .......... .......... .......... ..........  6% 10.9M 13s
  3600K .......... .......... .......... .......... ..........  6% 10.6M 13s
  3650K .......... .......... .......... .......... ..........  6% 10.9M 13s
  3700K .......... .......... .......... .......... ..........  6% 10.8M 13s
  3750K .......... .......... .......... .......... ..........  6% 10.4M 13s
  3800K .......... .......... .......... .......... ..........  6% 8.23M 13s
  3850K .......... .......... .......... .......... ..........  6% 10.4M 13s
  3900K .......... .......... .......... .......... ..........  6% 10.9M 13s
  3950K .......... .......... .......... .......... ..........  6% 11.3M 13s
  4000K .......... .......... .......... .......... ..........  7% 10.2M 12s
  4050K .......... .......... .......... .......... ..........  7% 10.8M 12s
  4100K .......... .......... .......... .......... ..........  7% 10.7M 12s
  4150K .......... .......... .......... .......... ..........  7% 11.1M 12s
  4200K .......... .......... .......... .......... ..........  7% 8.23M 12s
  4250K .......... .......... .......... .......... ..........  7% 11.4M 12s
  4300K .......... .......... .......... .......... ..........  7% 10.4M 12s
  4350K .......... .......... .......... .......... ..........  7% 10.9M 12s
  4400K .......... .......... .......... .......... ..........  7% 10.5M 12s
  4450K .......... .......... .......... .......... ..........  7% 10.4M 12s
  4500K .......... .......... .......... .......... ..........  7% 11.5M 12s
  4550K .......... .......... .......... .......... ..........  7% 10.9M 11s
  4600K .......... .......... .......... .......... ..........  8% 8.22M 11s
  4650K .......... .......... .......... .......... ..........  8% 10.8M 11s
  4700K .......... .......... .......... .......... ..........  8% 10.9M 11s
  4750K .......... .......... .......... .......... ..........  8% 10.8M 11s
  4800K .......... .......... .......... .......... ..........  8% 10.6M 11s
  4850K .......... .......... .......... .......... ..........  8% 11.0M 11s
  4900K .......... .......... .......... .......... ..........  8% 10.3M 11s
  4950K .......... .......... .......... .......... ..........  8% 10.9M 11s
  5000K .......... .......... .......... .......... ..........  8% 8.29M 11s
  5050K .......... .......... .......... .......... ..........  8% 10.9M 11s
  5100K .......... .......... .......... .......... ..........  8% 10.3M 11s
  5150K .......... .......... .......... .......... ..........  9% 10.9M 11s
  5200K .......... .......... .......... .......... ..........  9% 10.6M 11s
  5250K .......... .......... .......... .......... ..........  9% 10.9M 10s
  5300K .......... .......... .......... .......... ..........  9% 10.8M 10s
  5350K .......... .......... .......... .......... ..........  9% 11.0M 10s
  5400K .......... .......... .......... .......... ..........  9% 8.51M 10s
  5450K .......... .......... .......... .......... ..........  9% 10.4M 10s
  5500K .......... .......... .......... .......... ..........  9% 10.8M 10s
  5550K .......... .......... .......... .......... ..........  9% 10.9M 10s
  5600K .......... .......... .......... .......... ..........  9% 10.7M 10s
  5650K .......... .......... .......... .......... ..........  9% 11.4M 10s
  5700K .......... .......... .......... .......... ..........  9% 10.3M 10s
  5750K .......... .......... .......... .......... .......... 10% 10.8M 10s
  5800K .......... .......... .......... .......... .......... 10% 7.95M 10s
  5850K .......... .......... .......... .......... .......... 10% 11.3M 10s
  5900K .......... .......... .......... .......... .......... 10% 10.4M 10s
  5950K .......... .......... .......... .......... .......... 10% 10.7M 10s
  6000K .......... .......... .......... .......... .......... 10% 10.7M 10s
  6050K .......... .......... .......... .......... .......... 10% 10.7M 10s
  6100K .......... .......... .......... .......... .......... 10% 11.0M 10s
  6150K .......... .......... .......... .......... .......... 10% 10.6M 9s
  6200K .......... .......... .......... .......... .......... 10% 8.36M 9s
  6250K .......... .......... .......... .......... .......... 10% 10.7M 9s
  6300K .......... .......... .......... .......... .......... 11% 11.0M 9s
  6350K .......... .......... .......... .......... .......... 11% 10.8M 9s
  6400K .......... .......... .......... .......... .......... 11% 10.7M 9s
  6450K .......... .......... .......... .......... .......... 11% 10.2M 9s
  6500K .......... .......... .......... .......... .......... 11% 10.8M 9s
  6550K .......... .......... .......... .......... .......... 11% 11.0M 9s
  6600K .......... .......... .......... .......... .......... 11% 8.25M 9s
  6650K .......... .......... .......... .......... .......... 11% 10.9M 9s
  6700K .......... .......... .......... .......... .......... 11% 10.9M 9s
  6750K .......... .......... .......... .......... .......... 11% 10.9M 9s
  6800K .......... .......... .......... .......... .......... 11% 10.5M 9s
  6850K .......... .......... .......... .......... .......... 11% 11.0M 9s
  6900K .......... .......... .......... .......... .......... 12% 10.8M 9s
  6950K .......... .......... .......... .......... .......... 12% 10.8M 9s
  7000K .......... .......... .......... .......... .......... 12% 8.21M 9s
  7050K .......... .......... .......... .......... .......... 12% 10.9M 9s
  7100K .......... .......... .......... .......... .......... 12% 10.9M 9s
  7150K .......... .......... .......... .......... .......... 12% 10.2M 9s
  7200K .......... .......... .......... .......... .......... 12% 10.6M 9s
  7250K .......... .......... .......... .......... .......... 12% 10.9M 9s
  7300K .......... .......... .......... .......... .......... 12% 10.9M 9s
  7350K .......... .......... .......... .......... .......... 12% 10.8M 9s
  7400K .......... .......... .......... .......... .......... 12% 8.27M 8s
  7450K .......... .......... .......... .......... .......... 13% 11.0M 8s
  7500K .......... .......... .......... .......... .......... 13% 10.7M 8s
  7550K .......... .......... .......... .......... .......... 13% 10.9M 8s
  7600K .......... .......... .......... .......... .......... 13% 10.2M 8s
  7650K .......... .......... .......... .......... .......... 13% 11.4M 8s
  7700K .......... .......... .......... .......... .......... 13% 10.9M 8s
  7750K .......... .......... .......... .......... .......... 13% 10.3M 8s
  7800K .......... .......... .......... .......... .......... 13% 7.95M 8s
  7850K .......... .......... .......... .......... .......... 13% 11.4M 8s
  7900K .......... .......... .......... .......... .......... 13% 10.9M 8s
  7950K .......... .......... .......... .......... .......... 13% 10.7M 8s
  8000K .......... .......... .......... .......... .......... 13% 10.8M 8s
  8050K .......... .......... .......... .......... .......... 14% 10.2M 8s
  8100K .......... .......... .......... .......... .......... 14% 11.4M 8s
  8150K .......... .......... .......... .......... .......... 14% 10.9M 8s
  8200K .......... .......... .......... .......... .......... 14% 8.28M 8s
  8250K .......... .......... .......... .......... .......... 14% 10.8M 8s
  8300K .......... .......... .......... .......... .......... 14% 10.9M 8s
  8350K .......... .......... .......... .......... .......... 14% 10.9M 8s
  8400K .......... .......... .......... .......... .......... 14% 10.6M 8s
  8450K .......... .......... .......... .......... .......... 14% 10.9M 8s
  8500K .......... .......... .......... .......... .......... 14% 10.3M 8s
  8550K .......... .......... .......... .......... .......... 14% 10.9M 8s
  8600K .......... .......... .......... .......... .......... 15% 7.96M 8s
  8650K .......... .......... .......... .......... .......... 15% 10.8M 8s
  8700K .......... .......... .......... .......... .......... 15% 11.3M 8s
  8750K .......... .......... .......... .......... .......... 15% 10.4M 8s
  8800K .......... .......... .......... .......... .......... 15% 10.6M 8s
  8850K .......... .......... .......... .......... .......... 15% 10.8M 8s
  8900K .......... .......... .......... .......... .......... 15% 11.0M 8s
  8950K .......... .......... .......... .......... .......... 15% 11.3M 8s
  9000K .......... .......... .......... .......... .......... 15% 8.26M 8s
  9050K .......... .......... .......... .......... .......... 15% 10.4M 8s
  9100K .......... .......... .......... .......... .......... 15% 10.9M 8s
  9150K .......... .......... .......... .......... .......... 15% 11.3M 8s
  9200K .......... .......... .......... .......... .......... 16% 9.70M 7s
  9250K .......... .......... .......... .......... .......... 16% 11.3M 7s
  9300K .......... .......... .......... .......... .......... 16% 10.5M 7s
  9350K .......... .......... .......... .......... .......... 16% 10.7M 7s
  9400K .......... .......... .......... .......... .......... 16% 8.27M 7s
  9450K .......... .......... .......... .......... .......... 16% 11.3M 7s
  9500K .......... .......... .......... .......... .......... 16% 10.4M 7s
  9550K .......... .......... .......... .......... .......... 16% 11.3M 7s
  9600K .......... .......... .......... .......... .......... 16% 10.7M 7s
  9650K .......... .......... .......... .......... .......... 16% 10.4M 7s
  9700K .......... .......... .......... .......... .......... 16% 11.0M 7s
  9750K .......... .......... .......... .......... .......... 17% 10.8M 7s
  9800K .......... .......... .......... .......... .......... 17% 8.26M 7s
  9850K .......... .......... .......... .......... .......... 17% 10.2M 7s
  9900K .......... .......... .......... .......... .......... 17% 11.0M 7s
  9950K .......... .......... .......... .......... .......... 17% 10.9M 7s
 10000K .......... .......... .......... .......... .......... 17% 10.7M 7s
 10050K .......... .......... .......... .......... .......... 17% 10.7M 7s
 10100K .......... .......... .......... .......... .......... 17% 10.8M 7s
 10150K .......... .......... .......... .......... .......... 17% 10.8M 7s
 10200K .......... .......... .......... .......... .......... 17% 8.30M 7s
 10250K .......... .......... .......... .......... .......... 17% 10.8M 7s
 10300K .......... .......... .......... .......... .......... 17% 10.9M 7s
 10350K .......... .......... .......... .......... .......... 18% 10.8M 7s
 10400K .......... .......... .......... .......... .......... 18% 11.2M 7s
 10450K .......... .......... .......... .......... .......... 18% 10.3M 7s
 10500K .......... .......... .......... .......... .......... 18% 10.8M 7s
 10550K .......... .......... .......... .......... .......... 18% 10.3M 7s
 10600K .......... .......... .......... .......... .......... 18% 8.52M 7s
 10650K .......... .......... .......... .......... .......... 18% 10.4M 7s
 10700K .......... .......... .......... .......... .......... 18% 10.7M 7s
 10750K .......... .......... .......... .......... .......... 18% 10.9M 7s
 10800K .......... .......... .......... .......... .......... 18% 10.7M 7s
 10850K .......... .......... .......... .......... .......... 18% 10.9M 7s
 10900K .......... .......... .......... .......... .......... 19% 10.7M 7s
 10950K .......... .......... .......... .......... .......... 19% 10.8M 7s
 11000K .......... .......... .......... .......... .......... 19% 8.27M 7s
 11050K .......... .......... .......... .......... .......... 19% 11.0M 7s
 11100K .......... .......... .......... .......... .......... 19% 10.7M 7s
 11150K .......... .......... .......... .......... .......... 19% 11.0M 7s
 11200K .......... .......... .......... .......... .......... 19% 10.6M 7s
 11250K .......... .......... .......... .......... .......... 19% 10.2M 7s
 11300K .......... .......... .......... .......... .......... 19% 11.0M 7s
 11350K .......... .......... .......... .......... .......... 19% 10.2M 7s
 11400K .......... .......... .......... .......... .......... 19% 8.66M 7s
 11450K .......... .......... .......... .......... .......... 19% 10.9M 7s
 11500K .......... .......... .......... .......... .......... 20% 10.9M 7s
 11550K .......... .......... .......... .......... .......... 20% 10.7M 7s
 11600K .......... .......... .......... .......... .......... 20% 10.8M 7s
 11650K .......... .......... .......... .......... .......... 20% 10.8M 7s
 11700K .......... .......... .......... .......... .......... 20% 10.8M 6s
 11750K .......... .......... .......... .......... .......... 20% 10.9M 6s
 11800K .......... .......... .......... .......... .......... 20% 8.19M 6s
 11850K .......... .......... .......... .......... .......... 20% 10.9M 6s
 11900K .......... .......... .......... .......... .......... 20% 10.3M 6s
 11950K .......... .......... .......... .......... .......... 20% 10.9M 6s
 12000K .......... .......... .......... .......... .......... 20% 10.5M 6s
 12050K .......... .......... .......... .......... .......... 21% 11.0M 6s
 12100K .......... .......... .......... .......... .......... 21% 10.7M 6s
 12150K .......... .......... .......... .......... .......... 21% 11.0M 6s
 12200K .......... .......... .......... .......... .......... 21% 8.23M 6s
 12250K .......... .......... .......... .......... .......... 21% 10.8M 6s
 12300K .......... .......... .......... .......... .......... 21% 10.9M 6s
 12350K .......... .......... .......... .......... .......... 21% 10.4M 6s
 12400K .......... .......... .......... .......... .......... 21% 10.6M 6s
 12450K .......... .......... .......... .......... .......... 21% 11.6M 6s
 12500K .......... .......... .......... .......... .......... 21% 10.2M 6s
 12550K .......... .......... .......... .......... .......... 21% 11.4M 6s
 12600K .......... .......... .......... .......... .......... 21% 7.94M 6s
 12650K .......... .......... .......... .......... .......... 22% 11.0M 6s
 12700K .......... .......... .......... .......... .......... 22% 10.7M 6s
 12750K .......... .......... .......... .......... .......... 22% 11.0M 6s
 12800K .......... .......... .......... .......... .......... 22% 10.0M 6s
 12850K .......... .......... .......... .......... .......... 22% 11.3M 6s
 12900K .......... .......... .......... .......... .......... 22% 10.9M 6s
 12950K .......... .......... .......... .......... .......... 22% 10.9M 6s
 13000K .......... .......... .......... .......... .......... 22% 8.24M 6s
 13050K .......... .......... .......... .......... .......... 22% 10.8M 6s
 13100K .......... .......... .......... .......... .......... 22% 10.9M 6s
 13150K .......... .......... .......... .......... .......... 22% 10.8M 6s
 13200K .......... .......... .......... .......... .......... 23% 10.7M 6s
 13250K .......... .......... .......... .......... .......... 23% 10.8M 6s
 13300K .......... .......... .......... .......... .......... 23% 10.3M 6s
 13350K .......... .......... .......... .......... .......... 23% 10.5M 6s
 13400K .......... .......... .......... .......... .......... 23% 8.22M 6s
 13450K .......... .......... .......... .......... .......... 23% 10.8M 6s
 13500K .......... .......... .......... .......... .......... 23% 10.9M 6s
 13550K .......... .......... .......... .......... .......... 23% 10.8M 6s
 13600K .......... .......... .......... .......... .......... 23% 10.6M 6s
 13650K .......... .......... .......... .......... .......... 23% 10.9M 6s
 13700K .......... .......... .......... .......... .......... 23% 11.5M 6s
 13750K .......... .......... .......... .......... .......... 23% 10.4M 6s
 13800K .......... .......... .......... .......... .......... 24% 8.22M 6s
 13850K .......... .......... .......... .......... .......... 24% 10.9M 6s
 13900K .......... .......... .......... .......... .......... 24% 11.3M 6s
 13950K .......... .......... .......... .......... .......... 24% 9.89M 6s
 14000K .......... .......... .......... .......... .......... 24% 11.2M 6s
 14050K .......... .......... .......... .......... .......... 24% 10.4M 6s
 14100K .......... .......... .......... .......... .......... 24% 10.7M 6s
 14150K .......... .......... .......... .......... .......... 24% 11.3M 6s
 14200K .......... .......... .......... .......... .......... 24% 8.27M 6s
 14250K .......... .......... .......... .......... .......... 24% 10.4M 6s
 14300K .......... .......... .......... .......... .......... 24% 11.3M 6s
 14350K .......... .......... .......... .......... .......... 25% 10.9M 6s
 14400K .......... .......... .......... .......... .......... 25% 10.2M 6s
 14450K .......... .......... .......... .......... .......... 25% 10.8M 6s
 14500K .......... .......... .......... .......... .......... 25% 10.7M 6s
 14550K .......... .......... .......... .......... .......... 25% 10.9M 6s
 14600K .......... .......... .......... .......... .......... 25% 8.28M 6s
 14650K .......... .......... .......... .......... .......... 25% 10.4M 6s
 14700K .......... .......... .......... .......... .......... 25% 10.7M 6s
 14750K .......... .......... .......... .......... .......... 25% 11.0M 6s
 14800K .......... .......... .......... .......... .......... 25% 10.7M 6s
 14850K .......... .......... .......... .......... .......... 25% 10.7M 6s
 14900K .......... .......... .......... .......... .......... 25% 10.9M 6s
 14950K .......... .......... .......... .......... .......... 26% 10.8M 6s
 15000K .......... .......... .......... .......... .......... 26% 8.30M 6s
 15050K .......... .......... .......... .......... .......... 26% 10.8M 6s
 15100K .......... .......... .......... .......... .......... 26% 10.9M 6s
 15150K .......... .......... .......... .......... .......... 26% 10.9M 6s
 15200K .......... .......... .......... .......... .......... 26% 10.7M 6s
 15250K .......... .......... .......... .......... .......... 26% 10.7M 6s
 15300K .......... .......... .......... .......... .......... 26% 10.9M 6s
 15350K .......... .......... .......... .......... .......... 26% 10.3M 6s
 15400K .......... .......... .......... .......... .......... 26% 8.28M 5s
 15450K .......... .......... .......... .......... .......... 26% 11.3M 5s
 15500K .......... .......... .......... .......... .......... 27% 10.3M 5s
 15550K .......... .......... .......... .......... .......... 27% 10.9M 5s
 15600K .......... .......... .......... .......... .......... 27% 10.6M 5s
 15650K .......... .......... .......... .......... .......... 27% 10.9M 5s
 15700K .......... .......... .......... .......... .......... 27% 10.8M 5s
 15750K .......... .......... .......... .......... .......... 27%  238K 6s
 15800K .......... .......... .......... .......... .......... 27% 11.0M 6s
 15850K .......... .......... .......... .......... .......... 27% 15.2M 6s
 15900K .......... .......... .......... .......... .......... 27% 24.4M 6s
 15950K .......... .......... .......... .......... .......... 27% 16.4M 6s
 16000K .......... .......... .......... .......... .......... 27% 14.3M 6s
 16050K .......... .......... .......... .......... .......... 27% 18.5M 6s
 16100K .......... .......... .......... .......... .......... 28% 17.1M 6s
 16150K .......... .......... .......... .......... .......... 28% 70.8M 6s
 16200K .......... .......... .......... .......... .......... 28% 9.38M 6s
 16250K .......... .......... .......... .......... .......... 28% 9.85M 6s
 16300K .......... .......... .......... .......... .......... 28% 21.4M 6s
 16350K .......... .......... .......... .......... .......... 28% 13.9M 6s
 16400K .......... .......... .......... .......... .......... 28% 17.7M 6s
 16450K .......... .......... .......... .......... .......... 28% 13.5M 6s
 16500K .......... .......... .......... .......... .......... 28% 20.0M 6s
 16550K .......... .......... .......... .......... .......... 28% 86.6M 6s
 16600K .......... .......... .......... .......... .......... 28% 7.41M 6s
 16650K .......... .......... .......... .......... .......... 29% 14.0M 6s
 16700K .......... .......... .......... .......... .......... 29% 87.5M 6s
 16750K .......... .......... .......... .......... .......... 29% 12.1M 6s
 16800K .......... .......... .......... .......... .......... 29% 14.4M 6s
 16850K .......... .......... .......... .......... .......... 29% 11.0M 6s
 16900K .......... .......... .......... .......... .......... 29% 64.4M 6s
 16950K .......... .......... .......... .......... .......... 29% 10.3M 6s
 17000K .......... .......... .......... .......... .......... 29% 61.7M 6s
 17050K .......... .......... .......... .......... .......... 29% 91.5M 6s
 17100K .......... .......... .......... .......... .......... 29% 92.0M 5s
 17150K .......... .......... .......... .......... .......... 29% 89.8M 5s
 17200K .......... .......... .......... .......... .......... 29% 81.1M 5s
 17250K .......... .......... .......... .......... .......... 30% 98.1M 5s
 17300K .......... .......... .......... .......... .......... 30%  104M 5s
 17350K .......... .......... .......... .......... .......... 30%  105M 5s
 17400K .......... .......... .......... .......... .......... 30% 89.6M 5s
 17450K .......... .......... .......... .......... .......... 30% 99.2M 5s
 17500K .......... .......... .......... .......... .......... 30%  106M 5s
 17550K .......... .......... .......... .......... .......... 30%  106M 5s
 17600K .......... .......... .......... .......... .......... 30% 91.1M 5s
 17650K .......... .......... .......... .......... .......... 30%  105M 5s
 17700K .......... .......... .......... .......... .......... 30%  105M 5s
 17750K .......... .......... .......... .......... .......... 30%  106M 5s
 17800K .......... .......... .......... .......... .......... 31% 27.1M 5s
 17850K .......... .......... .......... .......... .......... 31% 9.78M 5s
 17900K .......... .......... .......... .......... .......... 31% 8.49M 5s
 17950K .......... .......... .......... .......... .......... 31%  105M 5s
 18000K .......... .......... .......... .......... .......... 31% 8.32M 5s
 18050K .......... .......... .......... .......... .......... 31% 11.0M 5s
 18100K .......... .......... .......... .......... .......... 31%  105M 5s
 18150K .......... .......... .......... .......... .......... 31%  105M 5s
 18200K .......... .......... .......... .......... .......... 31% 89.1M 5s
 18250K .......... .......... .......... .......... .......... 31%  105M 5s
 18300K .......... .......... .......... .......... .......... 31%  106M 5s
 18350K .......... .......... .......... .......... .......... 31%  105M 5s
 18400K .......... .......... .......... .......... .......... 32% 91.3M 5s
 18450K .......... .......... .......... .......... .......... 32%  107M 5s
 18500K .......... .......... .......... .......... .......... 32%  105M 5s
 18550K .......... .......... .......... .......... .......... 32%  106M 5s
 18600K .......... .......... .......... .......... .......... 32% 90.5M 5s
 18650K .......... .......... .......... .......... .......... 32%  106M 5s
 18700K .......... .......... .......... .......... .......... 32%  106M 5s
 18750K .......... .......... .......... .......... .......... 32%  106M 5s
 18800K .......... .......... .......... .......... .......... 32%  745K 5s
 18850K .......... .......... .......... .......... .......... 32% 8.33M 5s
 18900K .......... .......... .......... .......... .......... 32% 11.1M 5s
 18950K .......... .......... .......... .......... .......... 33% 10.6M 5s
 19000K .......... .......... .......... .......... .......... 33% 8.28M 5s
 19050K .......... .......... .......... .......... .......... 33% 10.3M 5s
 19100K .......... .......... .......... .......... .......... 33% 11.0M 5s
 19150K .......... .......... .......... .......... .......... 33% 10.7M 5s
 19200K .......... .......... .......... .......... .......... 33% 10.9M 5s
 19250K .......... .......... .......... .......... .......... 33% 10.1M 5s
 19300K .......... .......... .......... .......... .......... 33% 11.5M 5s
 19350K .......... .......... .......... .......... .......... 33% 10.8M 5s
 19400K .......... .......... .......... .......... .......... 33% 8.28M 5s
 19450K .......... .......... .......... .......... .......... 33% 10.7M 5s
 19500K .......... .......... .......... .......... .......... 33% 11.0M 5s
 19550K .......... .......... .......... .......... .......... 34% 10.9M 5s
 19600K .......... .......... .......... .......... .......... 34% 10.6M 5s
 19650K .......... .......... .......... .......... .......... 34% 10.9M 5s
 19700K .......... .......... .......... .......... .......... 34% 10.6M 5s
 19750K .......... .......... .......... .......... .......... 34% 10.3M 5s
 19800K .......... .......... .......... .......... .......... 34% 8.29M 5s
 19850K .......... .......... .......... .......... .......... 34% 10.8M 5s
 19900K .......... .......... .......... .......... .......... 34% 10.8M 5s
 19950K .......... .......... .......... .......... .......... 34% 10.9M 5s
 20000K .......... .......... .......... .......... .......... 34%  382K 5s
 20050K .......... .......... .......... .......... .......... 34% 12.9M 5s
 20100K .......... .......... .......... .......... .......... 35% 11.5M 5s
 20150K .......... .......... .......... .......... .......... 35% 12.4M 5s
 20200K .......... .......... .......... .......... .......... 35% 9.06M 5s
 20250K .......... .......... .......... .......... .......... 35% 13.8M 5s
 20300K .......... .......... .......... .......... .......... 35% 17.7M 5s
 20350K .......... .......... .......... .......... .......... 35% 25.7M 5s
 20400K .......... .......... .......... .......... .......... 35% 46.6M 5s
 20450K .......... .......... .......... .......... .......... 35% 95.7M 5s
 20500K .......... .......... .......... .......... .......... 35% 96.7M 5s
 20550K .......... .......... .......... .......... .......... 35% 96.4M 5s
 20600K .......... .......... .......... .......... .......... 35% 81.4M 5s
 20650K .......... .......... .......... .......... .......... 35% 96.2M 5s
 20700K .......... .......... .......... .......... .......... 36% 95.7M 5s
 20750K .......... .......... .......... .......... .......... 36% 97.8M 5s
 20800K .......... .......... .......... .......... .......... 36% 84.3M 5s
 20850K .......... .......... .......... .......... .......... 36% 97.2M 5s
 20900K .......... .......... .......... .......... .......... 36% 96.5M 5s
 20950K .......... .......... .......... .......... .......... 36% 94.5M 5s
 21000K .......... .......... .......... .......... .......... 36% 80.1M 5s
 21050K .......... .......... .......... .......... .......... 36% 95.6M 5s
 21100K .......... .......... .......... .......... .......... 36% 97.2M 5s
 21150K .......... .......... .......... .......... .......... 36% 95.3M 5s
 21200K .......... .......... .......... .......... .......... 36% 87.9M 5s
 21250K .......... .......... .......... .......... .......... 37% 95.6M 5s
 21300K .......... .......... .......... .......... .......... 37% 92.9M 5s
 21350K .......... .......... .......... .......... .......... 37% 95.0M 5s
 21400K .......... .......... .......... .......... .......... 37% 81.5M 5s
 21450K .......... .......... .......... .......... .......... 37% 96.3M 5s
 21500K .......... .......... .......... .......... .......... 37% 99.2M 5s
 21550K .......... .......... .......... .......... .......... 37% 95.1M 5s
 21600K .......... .......... .......... .......... .......... 37% 85.4M 4s
 21650K .......... .......... .......... .......... .......... 37% 93.8M 4s
 21700K .......... .......... .......... .......... .......... 37% 99.7M 4s
 21750K .......... .......... .......... .......... .......... 37% 99.7M 4s
 21800K .......... .......... .......... .......... .......... 37% 15.5M 4s
 21850K .......... .......... .......... .......... .......... 38% 10.5M 4s
 21900K .......... .......... .......... .......... .......... 38% 11.2M 4s
 21950K .......... .......... .......... .......... .......... 38% 10.4M 4s
 22000K .......... .......... .......... .......... .......... 38% 10.7M 4s
 22050K .......... .......... .......... .......... .......... 38% 10.7M 4s
 22100K .......... .......... .......... .......... .......... 38% 10.9M 4s
 22150K .......... .......... .......... .......... .......... 38% 10.8M 4s
 22200K .......... .......... .......... .......... .......... 38% 7.93M 4s
 22250K .......... .......... .......... .......... .......... 38% 10.9M 4s
 22300K .......... .......... .......... .......... .......... 38% 10.7M 4s
 22350K .......... .......... .......... .......... .......... 38% 10.3M 4s
 22400K .......... .......... .......... .......... .......... 39% 9.65M 4s
 22450K .......... .......... .......... .......... .......... 39% 10.3M 4s
 22500K .......... .......... .......... .......... .......... 39% 10.8M 4s
 22550K .......... .......... .......... .......... .......... 39% 11.0M 4s
 22600K .......... .......... .......... .......... .......... 39% 8.21M 4s
 22650K .......... .......... .......... .......... .......... 39% 10.8M 4s
 22700K .......... .......... .......... .......... .......... 39% 10.9M 4s
 22750K .......... .......... .......... .......... .......... 39% 10.9M 4s
 22800K .......... .......... .......... .......... .......... 39% 10.7M 4s
 22850K .......... .......... .......... .......... .......... 39% 10.9M 4s
 22900K .......... .......... .......... .......... .......... 39% 10.2M 4s
 22950K .......... .......... .......... .......... .......... 39% 11.4M 4s
 23000K .......... .......... .......... .......... .......... 40% 8.27M 4s
 23050K .......... .......... .......... .......... .......... 40% 10.9M 4s
 23100K .......... .......... .......... .......... .......... 40% 10.2M 4s
 23150K .......... .......... .......... .......... .......... 40% 11.0M 4s
 23200K .......... .......... .......... .......... .......... 40% 10.0M 4s
 23250K .......... .......... .......... .......... .......... 40% 11.5M 4s
 23300K .......... .......... .......... .......... .......... 40% 10.9M 4s
 23350K .......... .......... .......... .......... .......... 40% 10.7M 4s
 23400K .......... .......... .......... .......... .......... 40% 8.25M 4s
 23450K .......... .......... .......... .......... .......... 40% 10.9M 4s
 23500K .......... .......... .......... .......... .......... 40% 10.9M 4s
 23550K .......... .......... .......... .......... .......... 41% 10.8M 4s
 23600K .......... .......... .......... .......... .......... 41% 10.6M 4s
 23650K .......... .......... .......... .......... .......... 41% 10.5M 4s
 23700K .......... .......... .......... .......... .......... 41% 11.3M 4s
 23750K .......... .......... .......... .......... .......... 41% 11.0M 4s
 23800K .......... .......... .......... .......... .......... 41% 7.88M 4s
 23850K .......... .......... .......... .......... .......... 41% 10.4M 4s
 23900K .......... .......... .......... .......... .......... 41% 10.7M 4s
 23950K .......... .......... .......... .......... .......... 41% 11.0M 4s
 24000K .......... .......... .......... .......... .......... 41% 10.5M 4s
 24050K .......... .......... .......... .......... .......... 41% 11.0M 4s
 24100K .......... .......... .......... .......... .......... 41% 10.8M 4s
 24150K .......... .......... .......... .......... .......... 42% 10.9M 4s
 24200K .......... .......... .......... .......... .......... 42% 8.24M 4s
 24250K .......... .......... .......... .......... .......... 42% 10.9M 4s
 24300K .......... .......... .......... .......... .......... 42% 10.9M 4s
 24350K .......... .......... .......... .......... .......... 42% 10.8M 4s
 24400K .......... .......... .......... .......... .......... 42% 10.6M 4s
 24450K .......... .......... .......... .......... .......... 42% 11.0M 4s
 24500K .......... .......... .......... .......... .......... 42% 10.3M 4s
 24550K .......... .......... .......... .......... .......... 42% 10.8M 4s
 24600K .......... .......... .......... .......... .......... 42% 8.23M 4s
 24650K .......... .......... .......... .......... .......... 42% 10.8M 4s
 24700K .......... .......... .......... .......... .......... 43% 10.9M 4s
 24750K .......... .......... .......... .......... .......... 43% 11.3M 4s
 24800K .......... .......... .......... .......... .......... 43% 10.3M 4s
 24850K .......... .......... .......... .......... .......... 43% 10.8M 4s
 24900K .......... .......... .......... .......... .......... 43% 10.9M 4s
 24950K .......... .......... .......... .......... .......... 43% 10.9M 4s
 25000K .......... .......... .......... .......... .......... 43% 8.16M 4s
 25050K .......... .......... .......... .......... .......... 43% 10.8M 4s
 25100K .......... .......... .......... .......... .......... 43% 10.9M 4s
 25150K .......... .......... .......... .......... .......... 43% 10.2M 4s
 25200K .......... .......... .......... .......... .......... 43% 10.9M 4s
 25250K .......... .......... .......... .......... .......... 43% 10.6M 4s
 25300K .......... .......... .......... .......... .......... 44% 10.9M 4s
 25350K .......... .......... .......... .......... .......... 44% 11.0M 4s
 25400K .......... .......... .......... .......... .......... 44% 8.28M 4s
 25450K .......... .......... .......... .......... .......... 44% 10.7M 4s
 25500K .......... .......... .......... .......... .......... 44% 11.0M 4s
 25550K .......... .......... .......... .......... .......... 44% 10.2M 4s
 25600K .......... .......... .......... .......... .......... 44% 11.2M 4s
 25650K .......... .......... .......... .......... .......... 44% 10.9M 4s
 25700K .......... .......... .......... .......... .......... 44% 10.8M 4s
 25750K .......... .......... .......... .......... .......... 44% 11.0M 4s
 25800K .......... .......... .......... .......... .......... 44% 8.21M 4s
 25850K .......... .......... .......... .......... .......... 45% 10.3M 4s
 25900K .......... .......... .......... .......... .......... 45% 10.7M 4s
 25950K .......... .......... .......... .......... .......... 45% 11.0M 4s
 26000K .......... .......... .......... .......... .......... 45% 10.6M 4s
 26050K .......... .......... .......... .......... .......... 45% 10.8M 4s
 26100K .......... .......... .......... .......... .......... 45% 10.8M 4s
 26150K .......... .......... .......... .......... .......... 45% 10.8M 4s
 26200K .......... .......... .......... .......... .......... 45% 8.27M 4s
 26250K .......... .......... .......... .......... .......... 45% 10.9M 4s
 26300K .......... .......... .......... .......... .......... 45% 10.8M 4s
 26350K .......... .......... .......... .......... .......... 45% 10.8M 4s
 26400K .......... .......... .......... .......... .......... 45% 10.7M 4s
 26450K .......... .......... .......... .......... .......... 46% 10.9M 4s
 26500K .......... .......... .......... .......... .......... 46% 10.7M 4s
 26550K .......... .......... .......... .......... .......... 46% 9.87M 4s
 26600K .......... .......... .......... .......... .......... 46% 8.28M 4s
 26650K .......... .......... .......... .......... .......... 46% 10.9M 4s
 26700K .......... .......... .......... .......... .......... 46% 10.8M 4s
 26750K .......... .......... .......... .......... .......... 46% 10.9M 4s
 26800K .......... .......... .......... .......... .......... 46% 10.7M 4s
 26850K .......... .......... .......... .......... .......... 46% 10.8M 4s
 26900K .......... .......... .......... .......... .......... 46% 11.4M 4s
 26950K .......... .......... .......... .......... .......... 46% 10.4M 4s
 27000K .......... .......... .......... .......... .......... 47% 8.24M 4s
 27050K .......... .......... .......... .......... .......... 47% 10.9M 4s
 27100K .......... .......... .......... .......... .......... 47% 11.3M 4s
 27150K .......... .......... .......... .......... .......... 47% 10.5M 4s
 27200K .......... .......... .......... .......... .......... 47% 10.5M 4s
 27250K .......... .......... .......... .......... .......... 47% 10.9M 4s
 27300K .......... .......... .......... .......... .......... 47% 10.3M 4s
 27350K .......... .......... .......... .......... .......... 47% 11.4M 4s
 27400K .......... .......... .......... .......... .......... 47% 8.27M 4s
 27450K .......... .......... .......... .......... .......... 47% 10.9M 4s
 27500K .......... .......... .......... .......... .......... 47% 10.8M 4s
 27550K .......... .......... .......... .......... .......... 47% 10.9M 4s
 27600K .......... .......... .......... .......... .......... 48% 10.1M 4s
 27650K .......... .......... .......... .......... .......... 48% 10.9M 4s
 27700K .......... .......... .......... .......... .......... 48% 11.0M 4s
 27750K .......... .......... .......... .......... .......... 48% 10.7M 4s
 27800K .......... .......... .......... .......... .......... 48% 8.24M 4s
 27850K .......... .......... .......... .......... .......... 48% 10.9M 3s
 27900K .......... .......... .......... .......... .......... 48% 10.3M 3s
 27950K .......... .......... .......... .......... .......... 48% 10.8M 3s
 28000K .......... .......... .......... .......... .......... 48% 10.6M 3s
 28050K .......... .......... .......... .......... .......... 48% 10.8M 3s
 28100K .......... .......... .......... .......... .......... 48% 11.0M 3s
 28150K .......... .......... .......... .......... .......... 49% 10.9M 3s
 28200K .......... .......... .......... .......... .......... 49% 8.22M 3s
 28250K .......... .......... .......... .......... .......... 49% 10.9M 3s
 28300K .......... .......... .......... .......... .......... 49% 10.8M 3s
 28350K .......... .......... .......... .......... .......... 49% 10.9M 3s
 28400K .......... .......... .......... .......... .......... 49% 10.6M 3s
 28450K .......... .......... .......... .......... .......... 49% 10.8M 3s
 28500K .......... .......... .......... .......... .......... 49% 11.3M 3s
 28550K .......... .......... .......... .......... .......... 49% 10.5M 3s
 28600K .......... .......... .......... .......... .......... 49% 7.88M 3s
 28650K .......... .......... .......... .......... .......... 49% 10.7M 3s
 28700K .......... .......... .......... .......... .......... 49% 10.9M 3s
 28750K .......... .......... .......... .......... .......... 50% 10.8M 3s
 28800K .......... .......... .......... .......... .......... 50% 10.6M 3s
 28850K .......... .......... .......... .......... .......... 50% 11.0M 3s
 28900K .......... .......... .......... .......... .......... 50% 10.8M 3s
 28950K .......... .......... .......... .......... .......... 50% 10.9M 3s
 29000K .......... .......... .......... .......... .......... 50% 8.23M 3s
 29050K .......... .......... .......... .......... .......... 50% 10.9M 3s
 29100K .......... .......... .......... .......... .......... 50% 10.8M 3s
 29150K .......... .......... .......... .......... .......... 50% 11.0M 3s
 29200K .......... .......... .......... .......... .......... 50% 10.1M 3s
 29250K .......... .......... .......... .......... .......... 50% 10.9M 3s
 29300K .......... .......... .......... .......... .......... 51% 10.3M 3s
 29350K .......... .......... .......... .......... .......... 51% 11.4M 3s
 29400K .......... .......... .......... .......... .......... 51% 8.25M 3s
 29450K .......... .......... .......... .......... .......... 51% 11.0M 3s
 29500K .......... .......... .......... .......... .......... 51% 10.3M 3s
 29550K .......... .......... .......... .......... .......... 51% 11.5M 3s
 29600K .......... .......... .......... .......... .......... 51% 10.7M 3s
 29650K .......... .......... .......... .......... .......... 51% 10.7M 3s
 29700K .......... .......... .......... .......... .......... 51% 11.0M 3s
 29750K .......... .......... .......... .......... .......... 51% 10.8M 3s
 29800K .......... .......... .......... .......... .......... 51% 8.26M 3s
 29850K .......... .......... .......... .......... .......... 51% 10.8M 3s
 29900K .......... .......... .......... .......... .......... 52% 10.8M 3s
 29950K .......... .......... .......... .......... .......... 52% 10.3M 3s
 30000K .......... .......... .......... .......... .......... 52% 10.6M 3s
 30050K .......... .......... .......... .......... .......... 52% 10.8M 3s
 30100K .......... .......... .......... .......... .......... 52% 11.0M 3s
 30150K .......... .......... .......... .......... .......... 52% 10.8M 3s
 30200K .......... .......... .......... .......... .......... 52% 8.24M 3s
 30250K .......... .......... .......... .......... .......... 52% 10.4M 3s
 30300K .......... .......... .......... .......... .......... 52% 10.8M 3s
 30350K .......... .......... .......... .......... .......... 52% 10.9M 3s
 30400K .......... .......... .......... .......... .......... 52% 11.1M 3s
 30450K .......... .......... .......... .......... .......... 53% 10.3M 3s
 30500K .......... .......... .......... .......... .......... 53% 11.5M 3s
 30550K .......... .......... .......... .......... .......... 53% 10.3M 3s
 30600K .......... .......... .......... .......... .......... 53% 8.28M 3s
 30650K .......... .......... .......... .......... .......... 53% 10.3M 3s
 30700K .......... .......... .......... .......... .......... 53% 11.4M 3s
 30750K .......... .......... .......... .......... .......... 53% 10.4M 3s
 30800K .......... .......... .......... .......... .......... 53% 11.1M 3s
 30850K .......... .......... .......... .......... .......... 53% 10.9M 3s
 30900K .......... .......... .......... .......... .......... 53% 10.4M 3s
 30950K .......... .......... .......... .......... .......... 53% 11.2M 3s
 31000K .......... .......... .......... .......... .......... 53% 8.28M 3s
 31050K .......... .......... .......... .......... .......... 54% 10.9M 3s
 31100K .......... .......... .......... .......... .......... 54% 10.8M 3s
 31150K .......... .......... .......... .......... .......... 54% 10.9M 3s
 31200K .......... .......... .......... .......... .......... 54% 10.7M 3s
 31250K .......... .......... .......... .......... .......... 54% 10.7M 3s
 31300K .......... .......... .......... .......... .......... 54% 9.84M 3s
 31350K .......... .......... .......... .......... .......... 54% 10.8M 3s
 31400K .......... .......... .......... .......... .......... 54% 8.35M 3s
 31450K .......... .......... .......... .......... .......... 54% 10.9M 3s
 31500K .......... .......... .......... .......... .......... 54% 10.8M 3s
 31550K .......... .......... .......... .......... .......... 54% 10.9M 3s
 31600K .......... .......... .......... .......... .......... 55% 10.7M 3s
 31650K .......... .......... .......... .......... .......... 55% 10.9M 3s
 31700K .......... .......... .......... .......... .......... 55% 10.7M 3s
 31750K .......... .......... .......... .......... .......... 55% 10.8M 3s
 31800K .......... .......... .......... .......... .......... 55% 8.29M 3s
 31850K .......... .......... .......... .......... .......... 55% 11.3M 3s
 31900K .......... .......... .......... .......... .......... 55% 10.4M 3s
 31950K .......... .......... .......... .......... .......... 55% 10.9M 3s
 32000K .......... .......... .......... .......... .......... 55% 10.2M 3s
 32050K .......... .......... .......... .......... .......... 55% 10.7M 3s
 32100K .......... .......... .......... .......... .......... 55% 11.5M 3s
 32150K .......... .......... .......... .......... .......... 55% 10.3M 3s
 32200K .......... .......... .......... .......... .......... 56% 8.27M 3s
 32250K .......... .......... .......... .......... .......... 56% 10.8M 3s
 32300K .......... .......... .......... .......... .......... 56% 11.4M 3s
 32350K .......... .......... .......... .......... .......... 56% 10.3M 3s
 32400K .......... .......... .......... .......... .......... 56% 10.6M 3s
 32450K .......... .......... .......... .......... .......... 56% 11.0M 3s
 32500K .......... .......... .......... .......... .......... 56% 10.7M 3s
 32550K .......... .......... .......... .......... .......... 56% 11.0M 3s
 32600K .......... .......... .......... .......... .......... 56% 8.28M 3s
 32650K .......... .......... .......... .......... .......... 56% 10.3M 3s
 32700K .......... .......... .......... .......... .......... 56% 10.7M 3s
 32750K .......... .......... .......... .......... .......... 57% 11.0M 3s
 32800K .......... .......... .......... .......... .......... 57% 10.5M 3s
 32850K .......... .......... .......... .......... .......... 57% 10.9M 3s
 32900K .......... .......... .......... .......... .......... 57% 10.9M 3s
 32950K .......... .......... .......... .......... .......... 57% 10.8M 3s
 33000K .......... .......... .......... .......... .......... 57% 8.19M 3s
 33050K .......... .......... .......... .......... .......... 57% 11.0M 3s
 33100K .......... .......... .......... .......... .......... 57% 10.9M 3s
 33150K .......... .......... .......... .......... .......... 57% 10.8M 3s
 33200K .......... .......... .......... .......... .......... 57% 10.6M 3s
 33250K .......... .......... .......... .......... .......... 57% 10.8M 3s
 33300K .......... .......... .......... .......... .......... 57% 10.9M 3s
 33350K .......... .......... .......... .......... .......... 58% 10.2M 3s
 33400K .......... .......... .......... .......... .......... 58% 8.24M 3s
 33450K .......... .......... .......... .......... .......... 58% 10.8M 3s
 33500K .......... .......... .......... .......... .......... 58% 10.9M 3s
 33550K .......... .......... .......... .......... .......... 58% 10.8M 3s
 33600K .......... .......... .......... .......... .......... 58% 10.7M 3s
 33650K .......... .......... .......... .......... .......... 58% 10.8M 3s
 33700K .......... .......... .......... .......... .......... 58% 11.0M 3s
 33750K .......... .......... .......... .......... .......... 58% 10.9M 3s
 33800K .......... .......... .......... .......... .......... 58% 8.22M 3s
 33850K .......... .......... .......... .......... .......... 58% 10.8M 3s
 33900K .......... .......... .......... .......... .......... 59% 11.0M 3s
 33950K .......... .......... .......... .......... .......... 59% 10.3M 3s
 34000K .......... .......... .......... .......... .......... 59% 11.3M 3s
 34050K .......... .......... .......... .......... .......... 59% 9.63M 3s
 34100K .......... .......... .......... .......... .......... 59% 11.5M 3s
 34150K .......... .......... .......... .......... .......... 59% 11.0M 3s
 34200K .......... .......... .......... .......... .......... 59% 8.22M 3s
 34250K .......... .......... .......... .......... .......... 59% 10.3M 3s
 34300K .......... .......... .......... .......... .......... 59% 11.5M 3s
 34350K .......... .......... .......... .......... .......... 59% 10.8M 3s
 34400K .......... .......... .......... .......... .......... 59% 10.5M 3s
 34450K .......... .......... .......... .......... .......... 59% 11.0M 3s
 34500K .......... .......... .......... .......... .......... 60% 10.3M 3s
 34550K .......... .......... .......... .......... .......... 60% 11.5M 3s
 34600K .......... .......... .......... .......... .......... 60% 8.21M 3s
 34650K .......... .......... .......... .......... .......... 60% 10.9M 3s
 34700K .......... .......... .......... .......... .......... 60% 10.3M 3s
 34750K .......... .......... .......... .......... .......... 60% 10.9M 3s
 34800K .......... .......... .......... .......... .......... 60% 10.7M 3s
 34850K .......... .......... .......... .......... .......... 60% 10.7M 3s
 34900K .......... .......... .......... .......... .......... 60% 11.0M 3s
 34950K .......... .......... .......... .......... .......... 60% 10.8M 3s
 35000K .......... .......... .......... .......... .......... 60% 7.96M 3s
 35050K .......... .......... .......... .......... .......... 61% 10.8M 3s
 35100K .......... .......... .......... .......... .......... 61% 10.9M 3s
 35150K .......... .......... .......... .......... .......... 61% 11.4M 3s
 35200K .......... .......... .......... .......... .......... 61% 10.1M 3s
 35250K .......... .......... .......... .......... .......... 61% 11.0M 3s
 35300K .......... .......... .......... .......... .......... 61% 10.7M 3s
 35350K .......... .......... .......... .......... .......... 61% 10.9M 2s
 35400K .......... .......... .......... .......... .......... 61% 7.92M 2s
 35450K .......... .......... .......... .......... .......... 61% 11.5M 2s
 35500K .......... .......... .......... .......... .......... 61% 10.3M 2s
 35550K .......... .......... .......... .......... .......... 61% 11.3M 2s
 35600K .......... .......... .......... .......... .......... 61% 10.6M 2s
 35650K .......... .......... .......... .......... .......... 62% 10.5M 2s
 35700K .......... .......... .......... .......... .......... 62% 11.4M 2s
 35750K .......... .......... .......... .......... .......... 62% 10.4M 2s
 35800K .......... .......... .......... .......... .......... 62% 8.49M 2s
 35850K .......... .......... .......... .......... .......... 62% 10.9M 2s
 35900K .......... .......... .......... .......... .......... 62% 10.8M 2s
 35950K .......... .......... .......... .......... .......... 62% 10.4M 2s
 36000K .......... .......... .......... .......... .......... 62% 11.2M 2s
 36050K .......... .......... .......... .......... .......... 62% 10.4M 2s
 36100K .......... .......... .......... .......... .......... 62% 10.1M 2s
 36150K .......... .......... .......... .......... .......... 62% 11.0M 2s
 36200K .......... .......... .......... .......... .......... 63% 8.31M 2s
 36250K .......... .......... .......... .......... .......... 63% 10.8M 2s
 36300K .......... .......... .......... .......... .......... 63% 10.8M 2s
 36350K .......... .......... .......... .......... .......... 63% 10.8M 2s
 36400K .......... .......... .......... .......... .......... 63% 10.7M 2s
 36450K .......... .......... .......... .......... .......... 63% 10.7M 2s
 36500K .......... .......... .......... .......... .......... 63% 11.0M 2s
 36550K .......... .......... .......... .......... .......... 63% 10.7M 2s
 36600K .......... .......... .......... .......... .......... 63% 8.59M 2s
 36650K .......... .......... .......... .......... .......... 63% 10.4M 2s
 36700K .......... .......... .......... .......... .......... 63% 10.8M 2s
 36750K .......... .......... .......... .......... .......... 63% 10.3M 2s
 36800K .......... .......... .......... .......... .......... 64% 10.6M 2s
 36850K .......... .......... .......... .......... .......... 64% 11.3M 2s
 36900K .......... .......... .......... .......... .......... 64% 10.5M 2s
 36950K .......... .......... .......... .......... .......... 64% 10.8M 2s
 37000K .......... .......... .......... .......... .......... 64% 8.30M 2s
 37050K .......... .......... .......... .......... .......... 64% 11.3M 2s
 37100K .......... .......... .......... .......... .......... 64% 10.4M 2s
 37150K .......... .......... .......... .......... .......... 64% 10.7M 2s
 37200K .......... .......... .......... .......... .......... 64% 10.6M 2s
 37250K .......... .......... .......... .......... .......... 64% 10.8M 2s
 37300K .......... .......... .......... .......... .......... 64% 11.0M 2s
 37350K .......... .......... .......... .......... .......... 65% 10.7M 2s
 37400K .......... .......... .......... .......... .......... 65% 8.27M 2s
 37450K .......... .......... .......... .......... .......... 65% 10.3M 2s
 37500K .......... .......... .......... .......... .......... 65% 10.9M 2s
 37550K .......... .......... .......... .......... .......... 65% 10.7M 2s
 37600K .......... .......... .......... .......... .......... 65% 10.8M 2s
 37650K .......... .......... .......... .......... .......... 65% 10.9M 2s
 37700K .......... .......... .......... .......... .......... 65% 10.8M 2s
 37750K .......... .......... .......... .......... .......... 65% 10.9M 2s
 37800K .......... .......... .......... .......... .......... 65% 8.26M 2s
 37850K .......... .......... .......... .......... .......... 65% 10.9M 2s
 37900K .......... .......... .......... .......... .......... 65% 10.9M 2s
 37950K .......... .......... .......... .......... .......... 66% 10.8M 2s
 38000K .......... .......... .......... .......... .......... 66% 10.5M 2s
 38050K .......... .......... .......... .......... .......... 66% 11.0M 2s
 38100K .......... .......... .......... .......... .......... 66% 10.8M 2s
 38150K .......... .......... .......... .......... .......... 66% 10.3M 2s
 38200K .......... .......... .......... .......... .......... 66% 8.20M 2s
 38250K .......... .......... .......... .......... .......... 66% 10.9M 2s
 38300K .......... .......... .......... .......... .......... 66% 10.8M 2s
 38350K .......... .......... .......... .......... .......... 66% 10.9M 2s
 38400K .......... .......... .......... .......... .......... 66% 10.6M 2s
 38450K .......... .......... .......... .......... .......... 66% 11.0M 2s
 38500K .......... .......... .......... .......... .......... 67% 10.8M 2s
 38550K .......... .......... .......... .......... .......... 67% 10.8M 2s
 38600K .......... .......... .......... .......... .......... 67% 8.27M 2s
 38650K .......... .......... .......... .......... .......... 67% 10.9M 2s
 38700K .......... .......... .......... .......... .......... 67% 10.4M 2s
 38750K .......... .......... .......... .......... .......... 67% 11.5M 2s
 38800K .......... .......... .......... .......... .......... 67% 9.49M 2s
 38850K .......... .......... .......... .......... .......... 67% 11.6M 2s
 38900K .......... .......... .......... .......... .......... 67% 10.9M 2s
 38950K .......... .......... .......... .......... .......... 67% 10.4M 2s
 39000K .......... .......... .......... .......... .......... 67% 8.24M 2s
 39050K .......... .......... .......... .......... .......... 67% 11.4M 2s
 39100K .......... .......... .......... .......... .......... 68% 10.6M 2s
 39150K .......... .......... .......... .......... .......... 68% 11.0M 2s
 39200K .......... .......... .......... .......... .......... 68% 10.8M 2s
 39250K .......... .......... .......... .......... .......... 68% 10.3M 2s
 39300K .......... .......... .......... .......... .......... 68% 11.4M 2s
 39350K .......... .......... .......... .......... .......... 68% 10.8M 2s
 39400K .......... .......... .......... .......... .......... 68% 8.22M 2s
 39450K .......... .......... .......... .......... .......... 68% 10.9M 2s
 39500K .......... .......... .......... .......... .......... 68% 10.2M 2s
 39550K .......... .......... .......... .......... .......... 68% 10.9M 2s
 39600K .......... .......... .......... .......... .......... 68% 10.6M 2s
 39650K .......... .......... .......... .......... .......... 69% 10.8M 2s
 39700K .......... .......... .......... .......... .......... 69% 10.7M 2s
 39750K .......... .......... .......... .......... .......... 69% 11.1M 2s
 39800K .......... .......... .......... .......... .......... 69% 7.98M 2s
 39850K .......... .......... .......... .......... .......... 69% 10.8M 2s
 39900K .......... .......... .......... .......... .......... 69% 10.8M 2s
 39950K .......... .......... .......... .......... .......... 69% 10.9M 2s
 40000K .......... .......... .......... .......... .......... 69% 10.7M 2s
 40050K .......... .......... .......... .......... .......... 69% 10.7M 2s
 40100K .......... .......... .......... .......... .......... 69% 11.0M 2s
 40150K .......... .......... .......... .......... .......... 69% 11.2M 2s
 40200K .......... .......... .......... .......... .......... 69% 7.94M 2s
 40250K .......... .......... .......... .......... .......... 70% 10.4M 2s
 40300K .......... .......... .......... .......... .......... 70% 10.8M 2s
 40350K .......... .......... .......... .......... .......... 70% 11.3M 2s
 40400K .......... .......... .......... .......... .......... 70% 10.2M 2s
 40450K .......... .......... .......... .......... .......... 70% 11.4M 2s
 40500K .......... .......... .......... .......... .......... 70% 10.3M 2s
 40550K .......... .......... .......... .......... .......... 70% 11.3M 2s
 40600K .......... .......... .......... .......... .......... 70% 8.02M 2s
 40650K .......... .......... .......... .......... .......... 70% 11.3M 2s
 40700K .......... .......... .......... .......... .......... 70% 10.4M 2s
 40750K .......... .......... .......... .......... .......... 70% 11.3M 2s
 40800K .......... .......... .......... .......... .......... 71% 10.8M 2s
 40850K .......... .......... .......... .......... .......... 71% 9.63M 2s
 40900K .......... .......... .......... .......... .......... 71% 11.0M 2s
 40950K .......... .......... .......... .......... .......... 71% 10.7M 2s
 41000K .......... .......... .......... .......... .......... 71% 8.37M 2s
 41050K .......... .......... .......... .......... .......... 71% 10.7M 2s
 41100K .......... .......... .......... .......... .......... 71% 11.0M 2s
 41150K .......... .......... .......... .......... .......... 71% 10.7M 2s
 41200K .......... .......... .......... .......... .......... 71% 10.8M 2s
 41250K .......... .......... .......... .......... .......... 71% 10.9M 2s
 41300K .......... .......... .......... .......... .......... 71% 10.8M 2s
 41350K .......... .......... .......... .......... .......... 71% 10.8M 2s
 41400K .......... .......... .......... .......... .......... 72% 8.27M 2s
 41450K .......... .......... .......... .......... .......... 72% 10.8M 2s
 41500K .......... .......... .......... .......... .......... 72% 10.8M 2s
 41550K .......... .......... .......... .......... .......... 72% 10.3M 2s
 41600K .......... .......... .......... .......... .......... 72% 11.2M 2s
 41650K .......... .......... .......... .......... .......... 72% 10.3M 2s
 41700K .......... .......... .......... .......... .......... 72% 10.8M 2s
 41750K .......... .......... .......... .......... .......... 72% 10.9M 2s
 41800K .......... .......... .......... .......... .......... 72% 8.54M 2s
 41850K .......... .......... .......... .......... .......... 72% 10.4M 2s
 41900K .......... .......... .......... .......... .......... 72% 10.8M 2s
 41950K .......... .......... .......... .......... .......... 73% 10.8M 2s
 42000K .......... .......... .......... .......... .......... 73% 10.6M 2s
 42050K .......... .......... .......... .......... .......... 73% 11.0M 2s
 42100K .......... .......... .......... .......... .......... 73% 10.7M 2s
 42150K .......... .......... .......... .......... .......... 73% 10.9M 2s
 42200K .......... .......... .......... .......... .......... 73% 7.95M 2s
 42250K .......... .......... .......... .......... .......... 73% 11.0M 2s
 42300K .......... .......... .......... .......... .......... 73% 10.7M 2s
 42350K .......... .......... .......... .......... .......... 73% 11.0M 2s
 42400K .......... .......... .......... .......... .......... 73% 10.1M 2s
 42450K .......... .......... .......... .......... .......... 73% 11.4M 2s
 42500K .......... .......... .......... .......... .......... 73% 11.0M 2s
 42550K .......... .......... .......... .......... .......... 74% 10.7M 2s
 42600K .......... .......... .......... .......... .......... 74% 8.00M 2s
 42650K .......... .......... .......... .......... .......... 74% 11.5M 2s
 42700K .......... .......... .......... .......... .......... 74% 10.8M 2s
 42750K .......... .......... .......... .......... .......... 74% 10.7M 2s
 42800K .......... .......... .......... .......... .......... 74% 10.7M 2s
 42850K .......... .......... .......... .......... .......... 74% 10.8M 2s
 42900K .......... .......... .......... .......... .......... 74% 10.3M 2s
 42950K .......... .......... .......... .......... .......... 74% 10.8M 2s
 43000K .......... .......... .......... .......... .......... 74% 8.28M 2s
 43050K .......... .......... .......... .......... .......... 74% 10.8M 2s
 43100K .......... .......... .......... .......... .......... 75% 10.9M 2s
 43150K .......... .......... .......... .......... .......... 75% 10.8M 2s
 43200K .......... .......... .......... .......... .......... 75% 10.6M 2s
 43250K .......... .......... .......... .......... .......... 75% 11.0M 2s
 43300K .......... .......... .......... .......... .......... 75% 10.8M 2s
 43350K .......... .......... .......... .......... .......... 75% 10.8M 2s
 43400K .......... .......... .......... .......... .......... 75% 8.30M 2s
 43450K .......... .......... .......... .......... .......... 75% 10.4M 2s
 43500K .......... .......... .......... .......... .......... 75% 11.4M 2s
 43550K .......... .......... .......... .......... .......... 75% 10.3M 2s
 43600K .......... .......... .......... .......... .......... 75% 10.7M 2s
 43650K .......... .......... .......... .......... .......... 75% 10.3M 2s
 43700K .......... .......... .......... .......... .......... 76% 10.9M 2s
 43750K .......... .......... .......... .......... .......... 76% 11.7M 1s
 43800K .......... .......... .......... .......... .......... 76% 8.15M 1s
 43850K .......... .......... .......... .......... .......... 76% 10.4M 1s
 43900K .......... .......... .......... .......... .......... 76% 11.3M 1s
 43950K .......... .......... .......... .......... .......... 76% 11.0M 1s
 44000K .......... .......... .......... .......... .......... 76% 10.0M 1s
 44050K .......... .......... .......... .......... .......... 76% 11.5M 1s
 44100K .......... .......... .......... .......... .......... 76% 10.9M 1s
 44150K .......... .......... .......... .......... .......... 76% 10.8M 1s
 44200K .......... .......... .......... .......... .......... 76% 8.21M 1s
 44250K .......... .......... .......... .......... .......... 77% 10.3M 1s
 44300K .......... .......... .......... .......... .......... 77% 10.9M 1s
 44350K .......... .......... .......... .......... .......... 77% 10.9M 1s
 44400K .......... .......... .......... .......... .......... 77% 10.6M 1s
 44450K .......... .......... .......... .......... .......... 77% 10.8M 1s
 44500K .......... .......... .......... .......... .......... 77% 11.0M 1s
 44550K .......... .......... .......... .......... .......... 77% 10.4M 1s
 44600K .......... .......... .......... .......... .......... 77% 8.23M 1s
 44650K .......... .......... .......... .......... .......... 77% 10.8M 1s
 44700K .......... .......... .......... .......... .......... 77% 10.8M 1s
 44750K .......... .......... .......... .......... .......... 77% 10.9M 1s
 44800K .......... .......... .......... .......... .......... 77% 10.6M 1s
 44850K .......... .......... .......... .......... .......... 78% 10.9M 1s
 44900K .......... .......... .......... .......... .......... 78% 10.8M 1s
 44950K .......... .......... .......... .......... .......... 78% 10.3M 1s
 45000K .......... .......... .......... .......... .......... 78% 8.14M 1s
 45050K .......... .......... .......... .......... .......... 78% 11.1M 1s
 45100K .......... .......... .......... .......... .......... 78% 10.9M 1s
 45150K .......... .......... .......... .......... .......... 78% 10.8M 1s
 45200K .......... .......... .......... .......... .......... 78% 11.2M 1s
 45250K .......... .......... .......... .......... .......... 78% 10.5M 1s
 45300K .......... .......... .......... .......... .......... 78% 11.3M 1s
 45350K .......... .......... .......... .......... .......... 78% 10.8M 1s
 45400K .......... .......... .......... .......... .......... 79% 8.26M 1s
 45450K .......... .......... .......... .......... .......... 79% 10.4M 1s
 45500K .......... .......... .......... .......... .......... 79% 11.2M 1s
 45550K .......... .......... .......... .......... .......... 79% 11.0M 1s
 45600K .......... .......... .......... .......... .......... 79% 10.2M 1s
 45650K .......... .......... .......... .......... .......... 79% 10.3M 1s
 45700K .......... .......... .......... .......... .......... 79% 10.7M 1s
 45750K .......... .......... .......... .......... .......... 79% 11.1M 1s
 45800K .......... .......... .......... .......... .......... 79% 8.14M 1s
 45850K .......... .......... .......... .......... .......... 79% 11.0M 1s
 45900K .......... .......... .......... .......... .......... 79% 10.7M 1s
 45950K .......... .......... .......... .......... .......... 79% 11.0M 1s
 46000K .......... .......... .......... .......... .......... 80% 10.7M 1s
 46050K .......... .......... .......... .......... .......... 80% 10.8M 1s
 46100K .......... .......... .......... .......... .......... 80% 10.8M 1s
 46150K .......... .......... .......... .......... .......... 80% 10.7M 1s
 46200K .......... .......... .......... .......... .......... 80% 8.34M 1s
 46250K .......... .......... .......... .......... .......... 80% 10.9M 1s
 46300K .......... .......... .......... .......... .......... 80% 10.3M 1s
 46350K .......... .......... .......... .......... .......... 80% 10.9M 1s
 46400K .......... .......... .......... .......... .......... 80% 10.7M 1s
 46450K .......... .......... .......... .......... .......... 80% 10.7M 1s
 46500K .......... .......... .......... .......... .......... 80% 10.9M 1s
 46550K .......... .......... .......... .......... .......... 81% 10.8M 1s
 46600K .......... .......... .......... .......... .......... 81% 8.29M 1s
 46650K .......... .......... .......... .......... .......... 81% 10.8M 1s
 46700K .......... .......... .......... .......... .......... 81% 10.8M 1s
 46750K .......... .......... .......... .......... .......... 81% 10.8M 1s
 46800K .......... .......... .......... .......... .......... 81% 10.8M 1s
 46850K .......... .......... .......... .......... .......... 81% 10.7M 1s
 46900K .......... .......... .......... .......... .......... 81% 10.9M 1s
 46950K .......... .......... .......... .......... .......... 81% 11.0M 1s
 47000K .......... .......... .......... .......... .......... 81% 7.94M 1s
 47050K .......... .......... .......... .......... .......... 81% 10.8M 1s
 47100K .......... .......... .......... .......... .......... 81% 11.0M 1s
 47150K .......... .......... .......... .......... .......... 82% 10.3M 1s
 47200K .......... .......... .......... .......... .......... 82% 11.1M 1s
 47250K .......... .......... .......... .......... .......... 82% 11.0M 1s
 47300K .......... .......... .......... .......... .......... 82% 10.8M 1s
 47350K .......... .......... .......... .......... .......... 82% 10.9M 1s
 47400K .......... .......... .......... .......... .......... 82% 8.26M 1s
 47450K .......... .......... .......... .......... .......... 82% 10.9M 1s
 47500K .......... .......... .......... .......... .......... 82% 10.7M 1s
 47550K .......... .......... .......... .......... .......... 82% 11.0M 1s
 47600K .......... .......... .......... .......... .......... 82% 10.5M 1s
 47650K .......... .......... .......... .......... .......... 82% 11.1M 1s
 47700K .......... .......... .......... .......... .......... 83% 10.1M 1s
 47750K .......... .......... .......... .......... .......... 83% 10.8M 1s
 47800K .......... .......... .......... .......... .......... 83% 8.29M 1s
 47850K .......... .......... .......... .......... .......... 83% 10.8M 1s
 47900K .......... .......... .......... .......... .......... 83% 10.8M 1s
 47950K .......... .......... .......... .......... .......... 83% 10.9M 1s
 48000K .......... .......... .......... .......... .......... 83% 10.7M 1s
 48050K .......... .......... .......... .......... .......... 83% 10.8M 1s
 48100K .......... .......... .......... .......... .......... 83% 10.9M 1s
 48150K .......... .......... .......... .......... .......... 83% 10.9M 1s
 48200K .......... .......... .......... .......... .......... 83% 7.99M 1s
 48250K .......... .......... .......... .......... .......... 83% 10.8M 1s
 48300K .......... .......... .......... .......... .......... 84% 10.8M 1s
 48350K .......... .......... .......... .......... .......... 84% 10.7M 1s
 48400K .......... .......... .......... .......... .......... 84% 10.2M 1s
 48450K .......... .......... .......... .......... .......... 84% 10.8M 1s
 48500K .......... .......... .......... .......... .......... 84% 11.5M 1s
 48550K .......... .......... .......... .......... .......... 84% 10.3M 1s
 48600K .......... .......... .......... .......... .......... 84% 8.24M 1s
 48650K .......... .......... .......... .......... .......... 84% 11.3M 1s
 48700K .......... .......... .......... .......... .......... 84% 11.0M 1s
 48750K .......... .......... .......... .......... .......... 84% 10.3M 1s
 48800K .......... .......... .......... .......... .......... 84% 11.1M 1s
 48850K .......... .......... .......... .......... .......... 85% 10.4M 1s
 48900K .......... .......... .......... .......... .......... 85% 11.3M 1s
 48950K .......... .......... .......... .......... .......... 85% 11.0M 1s
 49000K .......... .......... .......... .......... .......... 85% 8.20M 1s
 49050K .......... .......... .......... .......... .......... 85% 9.88M 1s
 49100K .......... .......... .......... .......... .......... 85% 11.3M 1s
 49150K .......... .......... .......... .......... .......... 85% 10.9M 1s
 49200K .......... .......... .......... .......... .......... 85% 10.6M 1s
 49250K .......... .......... .......... .......... .......... 85% 10.9M 1s
 49300K .......... .......... .......... .......... .......... 85% 10.3M 1s
 49350K .......... .......... .......... .......... .......... 85% 11.0M 1s
 49400K .......... .......... .......... .......... .......... 85% 8.17M 1s
 49450K .......... .......... .......... .......... .......... 86% 10.9M 1s
 49500K .......... .......... .......... .......... .......... 86% 10.7M 1s
 49550K .......... .......... .......... .......... .......... 86% 11.0M 1s
 49600K .......... .......... .......... .......... .......... 86% 10.6M 1s
 49650K .......... .......... .......... .......... .......... 86% 10.9M 1s
 49700K .......... .......... .......... .......... .......... 86% 10.7M 1s
 49750K .......... .......... .......... .......... .......... 86% 10.4M 1s
 49800K .......... .......... .......... .......... .......... 86% 8.22M 1s
 49850K .......... .......... .......... .......... .......... 86% 10.9M 1s
 49900K .......... .......... .......... .......... .......... 86% 10.9M 1s
 49950K .......... .......... .......... .......... .......... 86% 11.3M 1s
 50000K .......... .......... .......... .......... .......... 87% 10.3M 1s
 50050K .......... .......... .......... .......... .......... 87% 10.9M 1s
 50100K .......... .......... .......... .......... .......... 87% 10.9M 1s
 50150K .......... .......... .......... .......... .......... 87% 10.7M 1s
 50200K .......... .......... .......... .......... .......... 87% 8.29M 1s
 50250K .......... .......... .......... .......... .......... 87% 11.3M 1s
 50300K .......... .......... .......... .......... .......... 87% 10.4M 1s
 50350K .......... .......... .......... .......... .......... 87% 10.7M 1s
 50400K .......... .......... .......... .......... .......... 87% 10.2M 1s
 50450K .......... .......... .......... .......... .......... 87% 10.8M 1s
 50500K .......... .......... .......... .......... .......... 87% 10.7M 1s
 50550K .......... .......... .......... .......... .......... 87% 10.9M 1s
 50600K .......... .......... .......... .......... .......... 88% 8.34M 1s
 50650K .......... .......... .......... .......... .......... 88% 10.7M 1s
 50700K .......... .......... .......... .......... .......... 88% 11.0M 1s
 50750K .......... .......... .......... .......... .......... 88% 10.7M 1s
 50800K .......... .......... .......... .......... .......... 88% 10.6M 1s
 50850K .......... .......... .......... .......... .......... 88% 11.0M 1s
 50900K .......... .......... .......... .......... .......... 88% 10.7M 1s
 50950K .......... .......... .......... .......... .......... 88% 11.0M 1s
 51000K .......... .......... .......... .......... .......... 88% 8.27M 1s
 51050K .......... .......... .......... .......... .......... 88% 10.9M 1s
 51100K .......... .......... .......... .......... .......... 88% 10.2M 1s
 51150K .......... .......... .......... .......... .......... 89% 11.0M 1s
 51200K .......... .......... .......... .......... .......... 89% 10.7M 1s
 51250K .......... .......... .......... .......... .......... 89% 10.8M 1s
 51300K .......... .......... .......... .......... .......... 89% 10.8M 1s
 51350K .......... .......... .......... .......... .......... 89% 10.9M 1s
 51400K .......... .......... .......... .......... .......... 89% 8.17M 1s
 51450K .......... .......... .......... .......... .......... 89% 10.9M 1s
 51500K .......... .......... .......... .......... .......... 89% 10.9M 1s
 51550K .......... .......... .......... .......... .......... 89% 10.9M 1s
 51600K .......... .......... .......... .......... .......... 89% 10.6M 1s
 51650K .......... .......... .......... .......... .......... 89% 10.8M 1s
 51700K .......... .......... .......... .......... .......... 89% 10.9M 1s
 51750K .......... .......... .......... .......... .......... 90% 10.8M 1s
 51800K .......... .......... .......... .......... .......... 90% 7.93M 1s
 51850K .......... .......... .......... .......... .......... 90% 10.9M 1s
 51900K .......... .......... .......... .......... .......... 90% 10.4M 1s
 51950K .......... .......... .......... .......... .......... 90% 11.4M 1s
 52000K .......... .......... .......... .......... .......... 90% 10.2M 1s
 52050K .......... .......... .......... .......... .......... 90% 11.0M 1s
 52100K .......... .......... .......... .......... .......... 90% 11.4M 1s
 52150K .......... .......... .......... .......... .......... 90% 10.2M 1s
 52200K .......... .......... .......... .......... .......... 90% 8.28M 1s
 52250K .......... .......... .......... .......... .......... 90% 11.4M 1s
 52300K .......... .......... .......... .......... .......... 91% 11.0M 1s
 52350K .......... .......... .......... .......... .......... 91% 10.2M 1s
 52400K .......... .......... .......... .......... .......... 91% 11.3M 1s
 52450K .......... .......... .......... .......... .......... 91% 10.1M 1s
 52500K .......... .......... .......... .......... .......... 91% 10.9M 1s
 52550K .......... .......... .......... .......... .......... 91% 10.9M 1s
 52600K .......... .......... .......... .......... .......... 91% 8.28M 1s
 52650K .......... .......... .......... .......... .......... 91% 10.8M 1s
 52700K .......... .......... .......... .......... .......... 91% 10.8M 1s
 52750K .......... .......... .......... .......... .......... 91% 10.9M 1s
 52800K .......... .......... .......... .......... .......... 91% 10.6M 0s
 52850K .......... .......... .......... .......... .......... 91% 10.9M 0s
 52900K .......... .......... .......... .......... .......... 92% 10.8M 0s
 52950K .......... .......... .......... .......... .......... 92% 10.9M 0s
 53000K .......... .......... .......... .......... .......... 92% 8.00M 0s
 53050K .......... .......... .......... .......... .......... 92% 10.8M 0s
 53100K .......... .......... .......... .......... .......... 92% 10.8M 0s
 53150K .......... .......... .......... .......... .......... 92% 10.3M 0s
 53200K .......... .......... .......... .......... .......... 92% 10.6M 0s
 53250K .......... .......... .......... .......... .......... 92% 10.8M 0s
 53300K .......... .......... .......... .......... .......... 92% 10.8M 0s
 53350K .......... .......... .......... .......... .......... 92% 11.5M 0s
 53400K .......... .......... .......... .......... .......... 92% 7.96M 0s
 53450K .......... .......... .......... .......... .......... 93% 10.8M 0s
 53500K .......... .......... .......... .......... .......... 93% 10.9M 0s
 53550K .......... .......... .......... .......... .......... 93% 11.3M 0s
 53600K .......... .......... .......... .......... .......... 93% 10.3M 0s
 53650K .......... .......... .......... .......... .......... 93% 11.2M 0s
 53700K .......... .......... .......... .......... .......... 93% 10.9M 0s
 53750K .......... .......... .......... .......... .......... 93% 10.5M 0s
 53800K .......... .......... .......... .......... .......... 93% 7.89M 0s
 53850K .......... .......... .......... .......... .......... 93% 11.3M 0s
 53900K .......... .......... .......... .......... .......... 93% 10.9M 0s
 53950K .......... .......... .......... .......... .......... 93% 10.8M 0s
 54000K .......... .......... .......... .......... .......... 93% 10.7M 0s
 54050K .......... .......... .......... .......... .......... 94% 10.3M 0s
 54100K .......... .......... .......... .......... .......... 94% 11.1M 0s
 54150K .......... .......... .......... .......... .......... 94% 10.7M 0s
 54200K .......... .......... .......... .......... .......... 94% 8.28M 0s
 54250K .......... .......... .......... .......... .......... 94% 10.7M 0s
 54300K .......... .......... .......... .......... .......... 94% 11.0M 0s
 54350K .......... .......... .......... .......... .......... 94% 10.9M 0s
 54400K .......... .......... .......... .......... .......... 94% 10.6M 0s
 54450K .......... .......... .......... .......... .......... 94% 10.8M 0s
 54500K .......... .......... .......... .......... .......... 94% 10.3M 0s
 54550K .......... .......... .......... .......... .......... 94% 11.0M 0s
 54600K .......... .......... .......... .......... .......... 95% 8.22M 0s
 54650K .......... .......... .......... .......... .......... 95% 10.9M 0s
 54700K .......... .......... .......... .......... .......... 95% 10.9M 0s
 54750K .......... .......... .......... .......... .......... 95% 10.8M 0s
 54800K .......... .......... .......... .......... .......... 95% 10.7M 0s
 54850K .......... .......... .......... .......... .......... 95% 10.8M 0s
 54900K .......... .......... .......... .......... .......... 95% 10.9M 0s
 54950K .......... .......... .......... .......... .......... 95% 11.3M 0s
 55000K .......... .......... .......... .......... .......... 95% 8.22M 0s
 55050K .......... .......... .......... .......... .......... 95% 10.4M 0s
 55100K .......... .......... .......... .......... .......... 95% 10.7M 0s
 55150K .......... .......... .......... .......... .......... 95% 11.0M 0s
 55200K .......... .......... .......... .......... .......... 96% 10.1M 0s
 55250K .......... .......... .......... .......... .......... 96% 10.8M 0s
 55300K .......... .......... .......... .......... .......... 96% 10.9M 0s
 55350K .......... .......... .......... .......... .......... 96% 10.8M 0s
 55400K .......... .......... .......... .......... .......... 96% 8.27M 0s
 55450K .......... .......... .......... .......... .......... 96% 11.0M 0s
 55500K .......... .......... .......... .......... .......... 96% 10.7M 0s
 55550K .......... .......... .......... .......... .......... 96% 10.9M 0s
 55600K .......... .......... .......... .......... .......... 96% 10.9M 0s
 55650K .......... .......... .......... .......... .......... 96% 10.3M 0s
 55700K .......... .......... .......... .......... .......... 96% 11.5M 0s
 55750K .......... .......... .......... .......... .......... 97% 10.1M 0s
 55800K .......... .......... .......... .......... .......... 97% 8.66M 0s
 55850K .......... .......... .......... .......... .......... 97% 10.2M 0s
 55900K .......... .......... .......... .......... .......... 97% 11.0M 0s
 55950K .......... .......... .......... .......... .......... 97% 10.3M 0s
 56000K .......... .......... .......... .......... .......... 97% 11.3M 0s
 56050K .......... .......... .......... .......... .......... 97% 10.8M 0s
 56100K .......... .......... .......... .......... .......... 97% 10.6M 0s
 56150K .......... .......... .......... .......... .......... 97% 10.8M 0s
 56200K .......... .......... .......... .......... .......... 97% 8.27M 0s
 56250K .......... .......... .......... .......... .......... 97% 10.9M 0s
 56300K .......... .......... .......... .......... .......... 97% 10.8M 0s
 56350K .......... .......... .......... .......... .......... 98% 10.9M 0s
 56400K .......... .......... .......... .......... .......... 98% 10.7M 0s
 56450K .......... .......... .......... .......... .......... 98% 10.8M 0s
 56500K .......... .......... .......... .......... .......... 98% 10.9M 0s
 56550K .......... .......... .......... .......... .......... 98% 10.3M 0s
 56600K .......... .......... .......... .......... .......... 98% 8.27M 0s
 56650K .......... .......... .......... .......... .......... 98% 10.4M 0s
 56700K .......... .......... .......... .......... .......... 98% 10.9M 0s
 56750K .......... .......... .......... .......... .......... 98% 10.8M 0s
 56800K .......... .......... .......... .......... .......... 98% 10.7M 0s
 56850K .......... .......... .......... .......... .......... 98% 11.3M 0s
 56900K .......... .......... .......... .......... .......... 99% 10.4M 0s
 56950K .......... .......... .......... .......... .......... 99% 10.9M 0s
 57000K .......... .......... .......... .......... .......... 99% 8.28M 0s
 57050K .......... .......... .......... .......... .......... 99% 11.5M 0s
 57100K .......... .......... .......... .......... .......... 99% 10.3M 0s
 57150K .......... .......... .......... .......... .......... 99% 11.4M 0s
 57200K .......... .......... .......... .......... .......... 99% 10.2M 0s
 57250K .......... .......... .......... .......... .......... 99% 10.3M 0s
 57300K .......... .......... .......... .......... .......... 99% 11.4M 0s
 57350K .......... .......... .......... .......... .......... 99% 10.4M 0s
 57400K .......... .......... .......... .......... .......... 99% 8.29M 0s
 57450K .......... .......... .......... .......... .......... 99% 12.8M 0s
 57500K .........                                             100% 99.9M=6.1s

2021-01-04 13:41:55 (9.24 MB/s) - âvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5.1â saved [58889256/58889256]

current dir is
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code

##################################################################################
Step2a: FCN8 TRAINING
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UPSCALE =  False
EPOCHS =  200
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]
__________________________________________________________________________________________________
block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]
__________________________________________________________________________________________________
conv6 (Conv2D)                  (None, 7, 7, 512)    12845568    block5_pool[0][0]
__________________________________________________________________________________________________
pool4_11 (Conv2D)               (None, 14, 14, 12)   6156        block4_pool[0][0]
__________________________________________________________________________________________________
conv7 (Conv2D)                  (None, 7, 7, 512)    262656      conv6[0][0]
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 12)   576         pool4_11[0][0]
__________________________________________________________________________________________________
pool3_11 (Conv2D)               (None, 28, 28, 12)   3084        block3_pool[0][0]
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 28, 28, 12)   98304       conv7[0][0]
__________________________________________________________________________________________________
add_layer (Add)                 (None, 28, 28, 12)   0           conv2d_transpose_1[0][0]
                                                                 pool3_11[0][0]
                                                                 conv2d_transpose[0][0]
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 224, 224, 12) 9216        add_layer[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 224, 224, 12) 0           conv2d_transpose_2[0][0]
==================================================================================================
Total params: 27,940,248
Trainable params: 27,940,248
Non-trainable params: 0
__________________________________________________________________________________________________
(311, 224, 224, 3) (311, 224, 224, 12)
(56, 224, 224, 3) (56, 224, 224, 12)
Train on 311 samples, validate on 56 samples
Epoch 1/200
311/311 - 10s - loss: 2.5676 - acc: 0.0900 - val_loss: 2.4788 - val_acc: 0.0995
Epoch 2/200
311/311 - 5s - loss: 2.4624 - acc: 0.1218 - val_loss: 2.4278 - val_acc: 0.1576
Epoch 3/200
311/311 - 5s - loss: 2.3612 - acc: 0.2061 - val_loss: 2.2030 - val_acc: 0.2896
Epoch 4/200
311/311 - 5s - loss: 2.1103 - acc: 0.3332 - val_loss: 1.9800 - val_acc: 0.3577
Epoch 5/200
311/311 - 5s - loss: 1.9384 - acc: 0.3676 - val_loss: 1.9230 - val_acc: 0.3668
Epoch 6/200
311/311 - 5s - loss: 1.8404 - acc: 0.3806 - val_loss: 1.8282 - val_acc: 0.3823
Epoch 7/200
311/311 - 5s - loss: 1.8258 - acc: 0.3917 - val_loss: 1.7845 - val_acc: 0.4004
Epoch 8/200
311/311 - 5s - loss: 1.7247 - acc: 0.4176 - val_loss: 1.6918 - val_acc: 0.4397
Epoch 9/200
311/311 - 5s - loss: 1.6171 - acc: 0.4661 - val_loss: 1.5455 - val_acc: 0.5003
Epoch 10/200
311/311 - 5s - loss: 1.4907 - acc: 0.5095 - val_loss: 1.7066 - val_acc: 0.4641
Epoch 11/200
311/311 - 5s - loss: 1.3865 - acc: 0.5221 - val_loss: 1.2952 - val_acc: 0.5303
Epoch 12/200
311/311 - 5s - loss: 1.2789 - acc: 0.5207 - val_loss: 1.2270 - val_acc: 0.5192
Epoch 13/200
311/311 - 5s - loss: 1.2317 - acc: 0.5112 - val_loss: 1.1902 - val_acc: 0.5556
Epoch 14/200
311/311 - 5s - loss: 1.2294 - acc: 0.5421 - val_loss: 1.1528 - val_acc: 0.5954
Epoch 15/200
311/311 - 5s - loss: 1.1547 - acc: 0.6173 - val_loss: 1.0953 - val_acc: 0.6579
Epoch 16/200
311/311 - 5s - loss: 1.1016 - acc: 0.6577 - val_loss: 1.0380 - val_acc: 0.6858
Epoch 17/200
311/311 - 5s - loss: 1.0584 - acc: 0.6698 - val_loss: 1.0656 - val_acc: 0.6770
Epoch 18/200
311/311 - 5s - loss: 1.0406 - acc: 0.6713 - val_loss: 0.9753 - val_acc: 0.6967
Epoch 19/200
311/311 - 5s - loss: 0.9988 - acc: 0.6781 - val_loss: 0.9558 - val_acc: 0.6967
Epoch 20/200
311/311 - 5s - loss: 0.9854 - acc: 0.6791 - val_loss: 0.9427 - val_acc: 0.6986
Epoch 21/200
311/311 - 5s - loss: 0.9796 - acc: 0.6791 - val_loss: 0.9453 - val_acc: 0.6982
Epoch 22/200
311/311 - 5s - loss: 0.9588 - acc: 0.6821 - val_loss: 0.9202 - val_acc: 0.7026
Epoch 23/200
311/311 - 5s - loss: 0.9466 - acc: 0.6835 - val_loss: 0.9079 - val_acc: 0.7042
Epoch 24/200
311/311 - 5s - loss: 0.9752 - acc: 0.6779 - val_loss: 0.9090 - val_acc: 0.7040
Epoch 25/200
311/311 - 5s - loss: 0.9337 - acc: 0.6849 - val_loss: 0.8924 - val_acc: 0.7051
Epoch 26/200
311/311 - 5s - loss: 0.9264 - acc: 0.6847 - val_loss: 0.8912 - val_acc: 0.7052
Epoch 27/200
311/311 - 5s - loss: 0.9195 - acc: 0.6859 - val_loss: 0.8771 - val_acc: 0.7071
Epoch 28/200
311/311 - 5s - loss: 0.9122 - acc: 0.6860 - val_loss: 0.8812 - val_acc: 0.7060
Epoch 29/200
311/311 - 5s - loss: 0.9117 - acc: 0.6859 - val_loss: 0.8687 - val_acc: 0.7073
Epoch 30/200
311/311 - 5s - loss: 0.8978 - acc: 0.6874 - val_loss: 0.8590 - val_acc: 0.7093
Epoch 31/200
311/311 - 5s - loss: 0.8919 - acc: 0.6882 - val_loss: 0.9292 - val_acc: 0.6971
Epoch 32/200
311/311 - 5s - loss: 0.9372 - acc: 0.6817 - val_loss: 0.8681 - val_acc: 0.7075
Epoch 33/200
311/311 - 5s - loss: 0.8888 - acc: 0.6885 - val_loss: 0.8494 - val_acc: 0.7095
Epoch 34/200
311/311 - 5s - loss: 0.8839 - acc: 0.6894 - val_loss: 0.8490 - val_acc: 0.7112
Epoch 35/200
311/311 - 5s - loss: 0.8760 - acc: 0.6911 - val_loss: 0.8432 - val_acc: 0.7123
Epoch 36/200
311/311 - 5s - loss: 0.8719 - acc: 0.6938 - val_loss: 0.8328 - val_acc: 0.7156
Epoch 37/200
311/311 - 5s - loss: 0.8666 - acc: 0.6944 - val_loss: 0.8366 - val_acc: 0.7146
Epoch 38/200
311/311 - 5s - loss: 0.8781 - acc: 0.6931 - val_loss: 0.8318 - val_acc: 0.7160
Epoch 39/200
311/311 - 5s - loss: 0.8578 - acc: 0.6976 - val_loss: 0.8215 - val_acc: 0.7182
Epoch 40/200
311/311 - 5s - loss: 0.8546 - acc: 0.6988 - val_loss: 0.8166 - val_acc: 0.7194
Epoch 41/200
311/311 - 5s - loss: 0.8461 - acc: 0.7016 - val_loss: 0.8432 - val_acc: 0.7190
Epoch 42/200
311/311 - 5s - loss: 0.8489 - acc: 0.7035 - val_loss: 0.8154 - val_acc: 0.7224
Epoch 43/200
311/311 - 5s - loss: 0.8430 - acc: 0.7057 - val_loss: 0.8941 - val_acc: 0.7070
Epoch 44/200
311/311 - 5s - loss: 0.8524 - acc: 0.7032 - val_loss: 0.8073 - val_acc: 0.7255
Epoch 45/200
311/311 - 5s - loss: 0.8416 - acc: 0.7078 - val_loss: 0.8072 - val_acc: 0.7280
Epoch 46/200
311/311 - 5s - loss: 0.8328 - acc: 0.7126 - val_loss: 0.8035 - val_acc: 0.7261
Epoch 47/200
311/311 - 5s - loss: 0.8266 - acc: 0.7149 - val_loss: 0.7942 - val_acc: 0.7325
Epoch 48/200
311/311 - 5s - loss: 0.8258 - acc: 0.7190 - val_loss: 0.8258 - val_acc: 0.7319
Epoch 49/200
311/311 - 5s - loss: 0.8607 - acc: 0.7087 - val_loss: 0.7993 - val_acc: 0.7331
Epoch 50/200
311/311 - 5s - loss: 0.8193 - acc: 0.7220 - val_loss: 0.7878 - val_acc: 0.7404
Epoch 51/200
311/311 - 5s - loss: 0.8115 - acc: 0.7276 - val_loss: 0.7899 - val_acc: 0.7361
Epoch 52/200
311/311 - 5s - loss: 0.8114 - acc: 0.7312 - val_loss: 0.8141 - val_acc: 0.7399
Epoch 53/200
311/311 - 5s - loss: 0.8205 - acc: 0.7300 - val_loss: 0.7786 - val_acc: 0.7484
Epoch 54/200
311/311 - 5s - loss: 0.7982 - acc: 0.7420 - val_loss: 0.7751 - val_acc: 0.7491
Epoch 55/200
311/311 - 5s - loss: 0.7956 - acc: 0.7450 - val_loss: 0.7761 - val_acc: 0.7505
Epoch 56/200
311/311 - 5s - loss: 0.7961 - acc: 0.7476 - val_loss: 0.7670 - val_acc: 0.7624
Epoch 57/200
311/311 - 5s - loss: 0.8024 - acc: 0.7461 - val_loss: 0.7639 - val_acc: 0.7571
Epoch 58/200
311/311 - 5s - loss: 0.7879 - acc: 0.7541 - val_loss: 0.7693 - val_acc: 0.7589
Epoch 59/200
311/311 - 5s - loss: 0.7822 - acc: 0.7590 - val_loss: 0.7490 - val_acc: 0.7683
Epoch 60/200
311/311 - 5s - loss: 0.7756 - acc: 0.7639 - val_loss: 0.7623 - val_acc: 0.7691
Epoch 61/200
311/311 - 5s - loss: 0.7876 - acc: 0.7600 - val_loss: 0.7470 - val_acc: 0.7712
Epoch 62/200
311/311 - 5s - loss: 0.7687 - acc: 0.7675 - val_loss: 0.7488 - val_acc: 0.7734
Epoch 63/200
311/311 - 5s - loss: 0.7673 - acc: 0.7689 - val_loss: 0.7678 - val_acc: 0.7551
Epoch 64/200
311/311 - 5s - loss: 0.7657 - acc: 0.7695 - val_loss: 0.7369 - val_acc: 0.7800
Epoch 65/200
311/311 - 5s - loss: 0.7560 - acc: 0.7765 - val_loss: 0.7295 - val_acc: 0.7823
Epoch 66/200
311/311 - 5s - loss: 0.7500 - acc: 0.7797 - val_loss: 0.7230 - val_acc: 0.7846
Epoch 67/200
311/311 - 5s - loss: 0.7584 - acc: 0.7777 - val_loss: 0.7223 - val_acc: 0.7846
Epoch 68/200
311/311 - 5s - loss: 0.7528 - acc: 0.7773 - val_loss: 0.7203 - val_acc: 0.7840
Epoch 69/200
311/311 - 5s - loss: 0.7361 - acc: 0.7858 - val_loss: 0.7275 - val_acc: 0.7840
Epoch 70/200
311/311 - 5s - loss: 0.7486 - acc: 0.7828 - val_loss: 0.7124 - val_acc: 0.7911
Epoch 71/200
311/311 - 5s - loss: 0.7381 - acc: 0.7854 - val_loss: 0.7335 - val_acc: 0.7846
Epoch 72/200
311/311 - 5s - loss: 0.7328 - acc: 0.7881 - val_loss: 0.7578 - val_acc: 0.7707
Epoch 73/200
311/311 - 5s - loss: 0.7312 - acc: 0.7878 - val_loss: 0.7157 - val_acc: 0.7897
Epoch 74/200
311/311 - 5s - loss: 0.7377 - acc: 0.7887 - val_loss: 0.7039 - val_acc: 0.7924
Epoch 75/200
311/311 - 5s - loss: 0.7175 - acc: 0.7958 - val_loss: 0.7095 - val_acc: 0.7926
Epoch 76/200
311/311 - 5s - loss: 0.7105 - acc: 0.7976 - val_loss: 0.7328 - val_acc: 0.7829
Epoch 77/200
311/311 - 5s - loss: 0.7108 - acc: 0.7975 - val_loss: 0.6890 - val_acc: 0.8000
Epoch 78/200
311/311 - 5s - loss: 0.7126 - acc: 0.7975 - val_loss: 0.7051 - val_acc: 0.7943
Epoch 79/200
311/311 - 5s - loss: 0.7143 - acc: 0.7975 - val_loss: 0.6838 - val_acc: 0.8024
Epoch 80/200
311/311 - 5s - loss: 0.6931 - acc: 0.8041 - val_loss: 0.6801 - val_acc: 0.8038
Epoch 81/200
311/311 - 5s - loss: 0.6997 - acc: 0.8005 - val_loss: 0.6730 - val_acc: 0.8062
Epoch 82/200
311/311 - 5s - loss: 0.7118 - acc: 0.7982 - val_loss: 0.6772 - val_acc: 0.8069
Epoch 83/200
311/311 - 5s - loss: 0.6894 - acc: 0.8071 - val_loss: 0.6760 - val_acc: 0.8039
Epoch 84/200
311/311 - 5s - loss: 0.6781 - acc: 0.8093 - val_loss: 0.6634 - val_acc: 0.8094
Epoch 85/200
311/311 - 5s - loss: 0.6901 - acc: 0.8046 - val_loss: 0.6643 - val_acc: 0.8109
Epoch 86/200
311/311 - 5s - loss: 0.6737 - acc: 0.8109 - val_loss: 0.6700 - val_acc: 0.8054
Epoch 87/200
311/311 - 5s - loss: 0.6719 - acc: 0.8112 - val_loss: 0.6619 - val_acc: 0.8103
Epoch 88/200
311/311 - 5s - loss: 0.6689 - acc: 0.8128 - val_loss: 0.6548 - val_acc: 0.8126
Epoch 89/200
311/311 - 5s - loss: 0.6701 - acc: 0.8122 - val_loss: 0.6556 - val_acc: 0.8114
Epoch 90/200
311/311 - 5s - loss: 0.6579 - acc: 0.8158 - val_loss: 0.6446 - val_acc: 0.8149
Epoch 91/200
311/311 - 5s - loss: 0.6792 - acc: 0.8084 - val_loss: 0.6525 - val_acc: 0.8126
Epoch 92/200
311/311 - 5s - loss: 0.6524 - acc: 0.8177 - val_loss: 0.6557 - val_acc: 0.8110
Epoch 93/200
311/311 - 5s - loss: 0.6576 - acc: 0.8162 - val_loss: 0.6375 - val_acc: 0.8173
Epoch 94/200
311/311 - 5s - loss: 0.6456 - acc: 0.8192 - val_loss: 0.6381 - val_acc: 0.8175
Epoch 95/200
311/311 - 5s - loss: 0.6431 - acc: 0.8205 - val_loss: 0.6415 - val_acc: 0.8148
Epoch 96/200
311/311 - 5s - loss: 0.6514 - acc: 0.8180 - val_loss: 0.6316 - val_acc: 0.8198
Epoch 97/200
311/311 - 5s - loss: 0.6349 - acc: 0.8228 - val_loss: 0.6447 - val_acc: 0.8140
Epoch 98/200
311/311 - 5s - loss: 0.6393 - acc: 0.8206 - val_loss: 0.6323 - val_acc: 0.8203
Epoch 99/200
311/311 - 5s - loss: 0.6299 - acc: 0.8245 - val_loss: 0.6294 - val_acc: 0.8191
Epoch 100/200
311/311 - 5s - loss: 0.6492 - acc: 0.8194 - val_loss: 0.6308 - val_acc: 0.8197
Epoch 101/200
311/311 - 5s - loss: 0.6246 - acc: 0.8258 - val_loss: 0.6229 - val_acc: 0.8213
Epoch 102/200
311/311 - 5s - loss: 0.6217 - acc: 0.8263 - val_loss: 0.6230 - val_acc: 0.8212
Epoch 103/200
311/311 - 5s - loss: 0.6182 - acc: 0.8275 - val_loss: 0.6285 - val_acc: 0.8180
Epoch 104/200
311/311 - 5s - loss: 0.6174 - acc: 0.8273 - val_loss: 0.6180 - val_acc: 0.8228
Epoch 105/200
311/311 - 5s - loss: 0.6367 - acc: 0.8216 - val_loss: 0.6199 - val_acc: 0.8243
Epoch 106/200
311/311 - 5s - loss: 0.6123 - acc: 0.8294 - val_loss: 0.6126 - val_acc: 0.8255
Epoch 107/200
311/311 - 5s - loss: 0.6079 - acc: 0.8302 - val_loss: 0.6097 - val_acc: 0.8244
Epoch 108/200
311/311 - 5s - loss: 0.6042 - acc: 0.8314 - val_loss: 0.6021 - val_acc: 0.8279
Epoch 109/200
311/311 - 5s - loss: 0.6084 - acc: 0.8301 - val_loss: 0.6238 - val_acc: 0.8238
Epoch 110/200
311/311 - 5s - loss: 0.6208 - acc: 0.8262 - val_loss: 0.6096 - val_acc: 0.8257
Epoch 111/200
311/311 - 5s - loss: 0.6006 - acc: 0.8321 - val_loss: 0.5977 - val_acc: 0.8306
Epoch 112/200
311/311 - 5s - loss: 0.5980 - acc: 0.8331 - val_loss: 0.5977 - val_acc: 0.8300
Epoch 113/200
311/311 - 5s - loss: 0.5998 - acc: 0.8317 - val_loss: 0.5992 - val_acc: 0.8283
Epoch 114/200
311/311 - 5s - loss: 0.5924 - acc: 0.8348 - val_loss: 0.6551 - val_acc: 0.8165
Epoch 115/200
311/311 - 5s - loss: 0.6129 - acc: 0.8283 - val_loss: 0.5957 - val_acc: 0.8295
Epoch 116/200
311/311 - 5s - loss: 0.5879 - acc: 0.8357 - val_loss: 0.5891 - val_acc: 0.8315
Epoch 117/200
311/311 - 5s - loss: 0.5935 - acc: 0.8337 - val_loss: 0.5945 - val_acc: 0.8312
Epoch 118/200
311/311 - 5s - loss: 0.5831 - acc: 0.8371 - val_loss: 0.5944 - val_acc: 0.8285
Epoch 119/200
311/311 - 5s - loss: 0.5848 - acc: 0.8363 - val_loss: 0.6009 - val_acc: 0.8259
Epoch 120/200
311/311 - 5s - loss: 0.5791 - acc: 0.8380 - val_loss: 0.5832 - val_acc: 0.8341
Epoch 121/200
311/311 - 5s - loss: 0.5806 - acc: 0.8372 - val_loss: 0.5877 - val_acc: 0.8329
Epoch 122/200
311/311 - 5s - loss: 0.5762 - acc: 0.8387 - val_loss: 0.5881 - val_acc: 0.8353
Epoch 123/200
311/311 - 5s - loss: 0.5861 - acc: 0.8366 - val_loss: 0.5850 - val_acc: 0.8348
Epoch 124/200
311/311 - 5s - loss: 0.5705 - acc: 0.8405 - val_loss: 0.5777 - val_acc: 0.8353
Epoch 125/200
311/311 - 5s - loss: 0.6118 - acc: 0.8270 - val_loss: 0.5890 - val_acc: 0.8324
Epoch 126/200
311/311 - 5s - loss: 0.5763 - acc: 0.8393 - val_loss: 0.5766 - val_acc: 0.8359
Epoch 127/200
311/311 - 5s - loss: 0.5693 - acc: 0.8406 - val_loss: 0.5755 - val_acc: 0.8349
Epoch 128/200
311/311 - 5s - loss: 0.5646 - acc: 0.8416 - val_loss: 0.5739 - val_acc: 0.8378
Epoch 129/200
311/311 - 5s - loss: 0.5632 - acc: 0.8423 - val_loss: 0.5763 - val_acc: 0.8351
Epoch 130/200
311/311 - 5s - loss: 0.5630 - acc: 0.8423 - val_loss: 0.5721 - val_acc: 0.8380
Epoch 131/200
311/311 - 5s - loss: 0.5695 - acc: 0.8403 - val_loss: 0.5772 - val_acc: 0.8373
Epoch 132/200
311/311 - 5s - loss: 0.5598 - acc: 0.8432 - val_loss: 0.5707 - val_acc: 0.8371
Epoch 133/200
311/311 - 5s - loss: 0.5596 - acc: 0.8431 - val_loss: 0.5660 - val_acc: 0.8400
Epoch 134/200
311/311 - 5s - loss: 0.5557 - acc: 0.8441 - val_loss: 0.5645 - val_acc: 0.8401
Epoch 135/200
311/311 - 5s - loss: 0.5568 - acc: 0.8437 - val_loss: 0.5726 - val_acc: 0.8384
Epoch 136/200
311/311 - 5s - loss: 0.5534 - acc: 0.8448 - val_loss: 0.5622 - val_acc: 0.8401
Epoch 137/200
311/311 - 5s - loss: 0.5568 - acc: 0.8440 - val_loss: 0.5671 - val_acc: 0.8379
Epoch 138/200
311/311 - 5s - loss: 0.5493 - acc: 0.8459 - val_loss: 0.5634 - val_acc: 0.8392
Epoch 139/200
311/311 - 5s - loss: 0.5475 - acc: 0.8462 - val_loss: 0.5592 - val_acc: 0.8419
Epoch 140/200
311/311 - 5s - loss: 0.5518 - acc: 0.8448 - val_loss: 0.6343 - val_acc: 0.8191
Epoch 141/200
311/311 - 5s - loss: 0.5619 - acc: 0.8427 - val_loss: 0.5598 - val_acc: 0.8402
Epoch 142/200
311/311 - 5s - loss: 0.5445 - acc: 0.8471 - val_loss: 0.5592 - val_acc: 0.8402
Epoch 143/200
311/311 - 5s - loss: 0.5457 - acc: 0.8466 - val_loss: 0.5585 - val_acc: 0.8401
Epoch 144/200
311/311 - 5s - loss: 0.5443 - acc: 0.8471 - val_loss: 0.5724 - val_acc: 0.8365
Epoch 145/200
311/311 - 5s - loss: 0.5476 - acc: 0.8461 - val_loss: 0.5659 - val_acc: 0.8385
Epoch 146/200
311/311 - 5s - loss: 0.5437 - acc: 0.8470 - val_loss: 0.5717 - val_acc: 0.8370
Epoch 147/200
311/311 - 5s - loss: 0.5420 - acc: 0.8476 - val_loss: 0.5734 - val_acc: 0.8361
Epoch 148/200
311/311 - 5s - loss: 0.5404 - acc: 0.8481 - val_loss: 0.5572 - val_acc: 0.8435
Epoch 149/200
311/311 - 5s - loss: 0.5471 - acc: 0.8465 - val_loss: 0.5544 - val_acc: 0.8427
Epoch 150/200
311/311 - 5s - loss: 0.5360 - acc: 0.8493 - val_loss: 0.5535 - val_acc: 0.8429
Epoch 151/200
311/311 - 5s - loss: 0.5350 - acc: 0.8495 - val_loss: 0.5618 - val_acc: 0.8425
Epoch 152/200
311/311 - 5s - loss: 0.5451 - acc: 0.8470 - val_loss: 0.5583 - val_acc: 0.8408
Epoch 153/200
311/311 - 5s - loss: 0.5333 - acc: 0.8500 - val_loss: 0.5515 - val_acc: 0.8430
Epoch 154/200
311/311 - 5s - loss: 0.5314 - acc: 0.8503 - val_loss: 0.5506 - val_acc: 0.8429
Epoch 155/200
311/311 - 5s - loss: 0.5335 - acc: 0.8499 - val_loss: 0.5478 - val_acc: 0.8435
Epoch 156/200
311/311 - 5s - loss: 0.5335 - acc: 0.8498 - val_loss: 0.5571 - val_acc: 0.8416
Epoch 157/200
311/311 - 5s - loss: 0.5298 - acc: 0.8508 - val_loss: 0.5531 - val_acc: 0.8429
Epoch 158/200
311/311 - 5s - loss: 0.5314 - acc: 0.8507 - val_loss: 0.5559 - val_acc: 0.8411
Epoch 159/200
311/311 - 5s - loss: 0.5336 - acc: 0.8500 - val_loss: 0.5465 - val_acc: 0.8455
Epoch 160/200
311/311 - 5s - loss: 0.5269 - acc: 0.8519 - val_loss: 0.5490 - val_acc: 0.8437
Epoch 161/200
311/311 - 5s - loss: 0.5253 - acc: 0.8521 - val_loss: 0.5474 - val_acc: 0.8437
Epoch 162/200
311/311 - 5s - loss: 0.5239 - acc: 0.8525 - val_loss: 0.5430 - val_acc: 0.8462
Epoch 163/200
311/311 - 5s - loss: 0.5249 - acc: 0.8524 - val_loss: 0.5419 - val_acc: 0.8453
Epoch 164/200
311/311 - 5s - loss: 0.5225 - acc: 0.8529 - val_loss: 0.5432 - val_acc: 0.8463
Epoch 165/200
311/311 - 5s - loss: 0.5280 - acc: 0.8516 - val_loss: 0.5492 - val_acc: 0.8450
Epoch 166/200
311/311 - 5s - loss: 0.5328 - acc: 0.8493 - val_loss: 0.5417 - val_acc: 0.8465
Epoch 167/200
311/311 - 5s - loss: 0.5222 - acc: 0.8528 - val_loss: 0.5408 - val_acc: 0.8465
Epoch 168/200
311/311 - 5s - loss: 0.5200 - acc: 0.8535 - val_loss: 0.5397 - val_acc: 0.8462
Epoch 169/200
311/311 - 5s - loss: 0.5204 - acc: 0.8535 - val_loss: 0.5403 - val_acc: 0.8469
Epoch 170/200
311/311 - 5s - loss: 0.5204 - acc: 0.8535 - val_loss: 0.5513 - val_acc: 0.8439
Epoch 171/200
311/311 - 5s - loss: 0.5222 - acc: 0.8532 - val_loss: 0.5392 - val_acc: 0.8465
Epoch 172/200
311/311 - 5s - loss: 0.5208 - acc: 0.8536 - val_loss: 0.5358 - val_acc: 0.8477
Epoch 173/200
311/311 - 5s - loss: 0.5202 - acc: 0.8536 - val_loss: 0.5473 - val_acc: 0.8456
Epoch 174/200
311/311 - 5s - loss: 0.5177 - acc: 0.8542 - val_loss: 0.5360 - val_acc: 0.8470
Epoch 175/200
311/311 - 5s - loss: 0.5143 - acc: 0.8552 - val_loss: 0.5339 - val_acc: 0.8482
Epoch 176/200
311/311 - 5s - loss: 0.5141 - acc: 0.8552 - val_loss: 0.5419 - val_acc: 0.8473
Epoch 177/200
311/311 - 5s - loss: 0.5299 - acc: 0.8506 - val_loss: 0.5362 - val_acc: 0.8473
Epoch 178/200
311/311 - 5s - loss: 0.5159 - acc: 0.8548 - val_loss: 0.5344 - val_acc: 0.8477
Epoch 179/200
311/311 - 5s - loss: 0.5113 - acc: 0.8560 - val_loss: 0.5348 - val_acc: 0.8481
Epoch 180/200
311/311 - 5s - loss: 0.5118 - acc: 0.8560 - val_loss: 0.5348 - val_acc: 0.8476
Epoch 181/200
311/311 - 5s - loss: 0.5101 - acc: 0.8565 - val_loss: 0.5344 - val_acc: 0.8476
Epoch 182/200
311/311 - 5s - loss: 0.5121 - acc: 0.8556 - val_loss: 0.5364 - val_acc: 0.8480
Epoch 183/200
311/311 - 5s - loss: 0.5098 - acc: 0.8565 - val_loss: 0.5320 - val_acc: 0.8484
Epoch 184/200
311/311 - 5s - loss: 0.5080 - acc: 0.8572 - val_loss: 0.5318 - val_acc: 0.8491
Epoch 185/200
311/311 - 5s - loss: 0.5081 - acc: 0.8571 - val_loss: 0.5328 - val_acc: 0.8481
Epoch 186/200
311/311 - 5s - loss: 0.5117 - acc: 0.8559 - val_loss: 0.5671 - val_acc: 0.8410
Epoch 187/200
311/311 - 5s - loss: 0.5124 - acc: 0.8558 - val_loss: 0.5306 - val_acc: 0.8496
Epoch 188/200
311/311 - 5s - loss: 0.5061 - acc: 0.8577 - val_loss: 0.5349 - val_acc: 0.8470
Epoch 189/200
311/311 - 5s - loss: 0.5065 - acc: 0.8573 - val_loss: 0.5308 - val_acc: 0.8500
Epoch 190/200
311/311 - 5s - loss: 0.5067 - acc: 0.8574 - val_loss: 0.5310 - val_acc: 0.8496
Epoch 191/200
311/311 - 5s - loss: 0.5082 - acc: 0.8569 - val_loss: 0.5377 - val_acc: 0.8466
Epoch 192/200
311/311 - 5s - loss: 0.5049 - acc: 0.8580 - val_loss: 0.5301 - val_acc: 0.8493
Epoch 193/200
311/311 - 5s - loss: 0.5086 - acc: 0.8569 - val_loss: 0.5620 - val_acc: 0.8408
Epoch 194/200
311/311 - 5s - loss: 0.5109 - acc: 0.8560 - val_loss: 0.5331 - val_acc: 0.8486
Epoch 195/200
311/311 - 5s - loss: 0.5029 - acc: 0.8586 - val_loss: 0.5276 - val_acc: 0.8497
Epoch 196/200
311/311 - 5s - loss: 0.5020 - acc: 0.8587 - val_loss: 0.5279 - val_acc: 0.8501
Epoch 197/200
311/311 - 5s - loss: 0.5029 - acc: 0.8586 - val_loss: 0.5344 - val_acc: 0.8480
Epoch 198/200
311/311 - 5s - loss: 0.5017 - acc: 0.8588 - val_loss: 0.5310 - val_acc: 0.8490
Epoch 199/200
311/311 - 5s - loss: 0.5039 - acc: 0.8582 - val_loss: 0.5364 - val_acc: 0.8472
Epoch 200/200
311/311 - 5s - loss: 0.5007 - acc: 0.8591 - val_loss: 0.5261 - val_acc: 0.8509


Elapsed time for Keras training (s):  973.861318



End of FCN8 training


##################################################################################
Step2b: FCN8 MAKING PREDICTIONS
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code


validation data (X) (Y) shapes: (56, 224, 224, 3) (56, 224, 224, 12)
testing    data (X) (Y) shapes (101, 224, 224, 3) (101, 224, 224, 12)



now computing IoU over testing data set:
class ( 0)          Sky: #TP= 430352, #FP=  32532, #FN=  25621, IoU=0.881
class ( 1)         Wall: #TP=1111843, #FP= 138516, #FN= 192103, IoU=0.771
class ( 2)         Pole: #TP=      0, #FP=     30, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1407345, #FP=  83809, #FN=  67674, IoU=0.903
class ( 4)     Sidewalk: #TP= 389025, #FP=  94622, #FN=  59408, IoU=0.716
class ( 5)   Vegetation: #TP= 799549, #FP= 163077, #FN=  26996, IoU=0.808
class ( 6)         Sign: #TP=      2, #FP=    107, #FN=  53390, IoU=0.000
class ( 7)        Fence: #TP=     15, #FP=    324, #FN= 156388, IoU=0.000
class ( 8)      vehicle: #TP=  81678, #FP= 183356, #FN=  12382, IoU=0.294
class ( 9)   Pedestrian: #TP=     29, #FP=     44, #FN=  36836, IoU=0.001
class (10)    Bicyclist: #TP=      6, #FP=      3, #FN= 110966, IoU=0.000
class (11)  miscellanea: #TP=  25520, #FP= 125992, #FN=  44228, IoU=0.130
_________________
Mean IoU: 0.375

now computing IoU over validation data set:
class ( 0)          Sky: #TP= 466346, #FP=  40005, #FN=  21429, IoU=0.884
class ( 1)         Wall: #TP= 634521, #FP= 133035, #FN=  56045, IoU=0.770
class ( 2)         Pole: #TP=      2, #FP=     33, #FN=  30993, IoU=0.000
class ( 3)         Road: #TP= 849230, #FP=  42835, #FN=  37765, IoU=0.913
class ( 4)     Sidewalk: #TP=  99908, #FP=  39967, #FN=  40819, IoU=0.553
class ( 5)   Vegetation: #TP= 182048, #FP=  73399, #FN=  40836, IoU=0.614
class ( 6)         Sign: #TP=     10, #FP=     89, #FN=  30106, IoU=0.000
class ( 7)        Fence: #TP=    145, #FP=    245, #FN=  44512, IoU=0.003
class ( 8)      vehicle: #TP= 126489, #FP=  62060, #FN=  21910, IoU=0.601
class ( 9)   Pedestrian: #TP=      3, #FP=     93, #FN=  20028, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=     48, #FN=  10125, IoU=0.000
class (11)  miscellanea: #TP=  32108, #FP=  27237, #FN=  64478, IoU=0.259
_________________
Mean IoU: 0.383

#######################################################################################
Step3: FCN8 KERAS to TENSORFLOW GRAPH CONVERSION
#######################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From Keras2TF.py:96: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
model name =  fcn8

 TF input node name:
[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation/truediv:0' shape=(?, 224, 224, 12) dtype=float32>]

FINISHED CREATING TF FILES


##############################################################################
Step4a: FCN8 FREEZE TF GRAPHS
##############################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0104 13:58:37.995445 140078468704064 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2021-01-04 13:58:37.998925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-01-04 13:58:38.023773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 13:58:38.023979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 13:58:38.024989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 13:58:38.025959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 13:58:38.026207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 13:58:38.027437: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 13:58:38.028417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 13:58:38.031021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 13:58:38.032738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 13:58:38.033039: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-01-04 13:58:38.041337: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496220000 Hz
2021-01-04 13:58:38.042380: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c466f3c940 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-04 13:58:38.042391: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-04 13:58:38.110748: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c465971760 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-04 13:58:38.110779: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2021-01-04 13:58:38.112671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 13:58:38.112725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 13:58:38.112741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 13:58:38.112754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 13:58:38.112768: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 13:58:38.112781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 13:58:38.112794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 13:58:38.112808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 13:58:38.116093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 13:58:38.116144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 13:58:38.118633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-04 13:58:38.118648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2021-01-04 13:58:38.118655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2021-01-04 13:58:38.123708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22362 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fcn8/float_model.ckpt
I0104 13:58:38.347500 140078468704064 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fcn8/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0104 13:58:39.544028 140078468704064 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0104 13:58:39.544346 140078468704064 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 37 variables.
I0104 13:58:39.619871 140078468704064 graph_util_impl.py:334] Froze 37 variables.
INFO:tensorflow:Converted 37 variables to const ops.
I0104 13:58:39.832677 140078468704064 graph_util_impl.py:394] Converted 37 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fcn8/float_model.ckpt.meta

##############################################################################
Step4a: FCN8 INSPECT FROZEN GRAPH
##############################################################################

Op types used: 81 Const, 37 Identity, 17 BiasAdd, 17 Conv2D, 17 Relu, 9 StridedSlice, 8 AddV2, 6 Mul, 5 MaxPool, 3 Pack, 3 Shape, 3 Conv2DBackpropInput, 1 Max, 1 Placeholder, 1 RealDiv, 1 Exp, 1 Sub, 1 Sum

Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3])
Found 1 possible outputs: (name=activation/truediv, op=RealDiv)

##############################################################################
Step4b: FCN8 EVALUATING THE ORIGINAL GRAPH
##############################################################################

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 430352, #FP=  32532, #FN=  25621, IoU=0.881
class ( 1)         Wall: #TP=1111843, #FP= 138516, #FN= 192103, IoU=0.771
class ( 2)         Pole: #TP=      0, #FP=     30, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1407345, #FP=  83809, #FN=  67674, IoU=0.903
class ( 4)     Sidewalk: #TP= 389025, #FP=  94622, #FN=  59408, IoU=0.716
class ( 5)   Vegetation: #TP= 799549, #FP= 163077, #FN=  26996, IoU=0.808
class ( 6)         Sign: #TP=      2, #FP=    107, #FN=  53390, IoU=0.000
class ( 7)        Fence: #TP=     15, #FP=    324, #FN= 156388, IoU=0.000
class ( 8)      vehicle: #TP=  81678, #FP= 183356, #FN=  12382, IoU=0.294
class ( 9)   Pedestrian: #TP=     29, #FP=     44, #FN=  36836, IoU=0.001
class (10)    Bicyclist: #TP=      6, #FP=      3, #FN= 110966, IoU=0.000
class (11)  miscellanea: #TP=  25520, #FP= 125992, #FN=  44228, IoU=0.130
_________________
Mean IoU: 0.375
FINISHED!

##########################################################################
Step5a: FCN8 QUANTIZATION
##########################################################################


Vai_q_tensorflow v1.2.0 build for Tensorflow 1.15.2 git version
heads/1.3-0-gc680f744

N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--                                                                               10 20% (2 of 10) |#####                    | Elapsed Time: 0:00:02 ETA:   0:00:    30% (3 of 10) |#######                  | Elapsed Time: 0:00:03 ETA:   0:08                                                                            00:07                                                                                 60% (6 of 10) |###############          | Elapsed Time: 0:00:07 ETA:   0:00:04                                                                         03                                                                                80% (8 of 10) |####################     | Elapsed Time: 0:00:09 ETA:            90%                                                                   e:  0:00:11
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
script running on folder  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code/../build/dataset1/img_calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 10 iterations...
INFO: Calibration Done.
INFO: Generating Deploy Model...
INFO: Deploy Model Generated.
********************* Quantization Summary *********************
INFO: Output:
  quantize_eval_model: .././build/quantize_results/fcn8/quantize_eval_model.pb
  deploy_model: .././build/quantize_results/fcn8/deploy_model.pb

##############################################################################
Step5b: FCN8 EVALUATE QUANTIZED GRAPH
##############################################################################

Using TensorFlow backend.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 426680, #FP=  29757, #FN=  29293, IoU=0.878
class ( 1)         Wall: #TP=1122961, #FP= 161867, #FN= 180985, IoU=0.766
class ( 2)         Pole: #TP=      5, #FP=     99, #FN=  36415, IoU=0.000
class ( 3)         Road: #TP=1406929, #FP=  85268, #FN=  68090, IoU=0.902
class ( 4)     Sidewalk: #TP= 385062, #FP=  95436, #FN=  63371, IoU=0.708
class ( 5)   Vegetation: #TP= 796767, #FP= 154826, #FN=  29778, IoU=0.812
class ( 6)         Sign: #TP=      8, #FP=    367, #FN=  53384, IoU=0.000
class ( 7)        Fence: #TP=     74, #FP=    528, #FN= 156329, IoU=0.000
class ( 8)      vehicle: #TP=  80654, #FP= 178728, #FN=  13406, IoU=0.296
class ( 9)   Pedestrian: #TP=     22, #FP=     49, #FN=  36843, IoU=0.001
class (10)    Bicyclist: #TP=     11, #FP=     16, #FN= 110961, IoU=0.000
class (11)  miscellanea: #TP=  23176, #FP= 118486, #FN=  46572, IoU=0.123
_________________
Mean IoU: 0.374
FINISHED!

##########################################################################
COMPILE FCN8 XMODEL FILE WITH Vitis AI for VCK190 TARGET
##########################################################################

[INFO] parse raw model     :  0%|          | 0/105 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 105/105 [00:00<00:00, 20585.30it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/120 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 120/120 [00:00<00:00, 10428.62it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/81 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 81/81 [00:00<00:00, 4928.68it/s]
[INFO] generate xmodel     :  0%|          | 0/78 [00:00<?, ?it/s][INFO] generate xmodel     : 58%|ââââââ    | 45/78 [00:00<00:00, 393.59it/s][INFO] ge : 79%|ââââââââ  | 62/78 [00:00<00:00, 164.81it/s]                 xmodel    | 78/78 [00:00<00:00, 216.49it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8/fcn8_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8/fcn8_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-135931-3460"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 152
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/fcn8.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is faacc2f0aa8f9ed0f996bf7ea7ceb8f9, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE FCN8 MODEL FILE WITH Vitis AI for ZCU104
##########################################################################

[INFO] parse raw model     :  0%|          | 0/105 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 105/105 [00:00<00:00, 20676.15it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/120 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 120/120 [00:00<00:00, 10280.16it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/81 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 81/81 [00:00<00:00, 4916.41it/s]
[INFO] generate xmodel     :  0%|          | 0/78 [00:00<?, ?it/s][INFO] generate xmodel     : 65%|âââââââ   | 51/78 [00:00<00:00, 474.47it/s][INFO] ge : 85%|âââââââââ | 66/78 [00:00<00:00, 168.21it/s]                 xmodel    | 78/78 [00:00<00:00, 234.76it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8/fcn8_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8/fcn8_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-135937-3498"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 152
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/fcn8.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 29368e71750a3139c069e52c4c0e82a3, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE FCN8 XMODEL FILE WITH Vitis AI for ZCU102
##########################################################################

[INFO] parse raw model     :  0%|          | 0/105 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 105/105 [00:00<00:00, 20646.10it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/120 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 120/120 [00:00<00:00, 10480.08it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/81 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 81/81 [00:00<00:00, 4894.52it/s]
[INFO] generate xmodel     :  0%|          | 0/78 [00:00<?, ?it/s][INFO] generate xmodel     : 65%|âââââââ   | 51/78 [00:00<00:00, 485.44it/s][INFO] ge : 85%|âââââââââ | 66/78 [00:00<00:00, 170.20it/s]                 xmodel    | 78/78 [00:00<00:00, 237.80it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8/fcn8_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8/fcn8_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-135948-3536"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 152
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/fcn8.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 8fb6bf47d49a9488ee1f7bd89a99db81, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
#####################################
MAIN FCN8 FLOW COMPLETED
#####################################


##################################################################################
A) CLEAN PREVIOUS DIRECTORIES
##################################################################################

current dir is
/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code

##################################################################################
Step2a: FCN8UPS TRAINING
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UPSCALE =  True
EPOCHS =  200
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_1[0][0]
__________________________________________________________________________________________________
block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]
__________________________________________________________________________________________________
block1_pool (MaxPooling2D)      (None, 112, 112, 64) 0           block1_conv2[0][0]
__________________________________________________________________________________________________
block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool[0][0]
__________________________________________________________________________________________________
block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]
__________________________________________________________________________________________________
block2_pool (MaxPooling2D)      (None, 56, 56, 128)  0           block2_conv2[0][0]
__________________________________________________________________________________________________
block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool[0][0]
__________________________________________________________________________________________________
block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]
__________________________________________________________________________________________________
block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]
__________________________________________________________________________________________________
block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_conv3[0][0]
__________________________________________________________________________________________________
block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool[0][0]
__________________________________________________________________________________________________
block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]
__________________________________________________________________________________________________
block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]
__________________________________________________________________________________________________
block4_pool (MaxPooling2D)      (None, 14, 14, 512)  0           block4_conv3[0][0]
__________________________________________________________________________________________________
block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool[0][0]
__________________________________________________________________________________________________
block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]
__________________________________________________________________________________________________
block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]
__________________________________________________________________________________________________
block5_pool (MaxPooling2D)      (None, 7, 7, 512)    0           block5_conv3[0][0]
__________________________________________________________________________________________________
pool4_11 (Conv2D)               (None, 14, 14, 12)   6156        block4_pool[0][0]
__________________________________________________________________________________________________
conv7_4a (Conv2D)               (None, 7, 7, 12)     301068      block5_pool[0][0]
__________________________________________________________________________________________________
pool411_b (Conv2D)              (None, 14, 14, 12)   156         pool4_11[0][0]
__________________________________________________________________________________________________
conv7_4b (Conv2D)               (None, 7, 7, 12)     156         conv7_4a[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 28, 28, 12)   0           pool411_b[0][0]
__________________________________________________________________________________________________
pool3_11 (Conv2D)               (None, 28, 28, 12)   3084        block3_pool[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 28, 28, 12)   0           conv7_4b[0][0]
__________________________________________________________________________________________________
add_layer (Add)                 (None, 28, 28, 12)   0           up_sampling2d_1[0][0]
                                                                 pool3_11[0][0]
                                                                 up_sampling2d[0][0]
__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 224, 224, 12) 9216        add_layer[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 224, 224, 12) 0           conv2d_transpose[0][0]
==================================================================================================
Total params: 15,034,524
Trainable params: 15,034,524
Non-trainable params: 0
__________________________________________________________________________________________________
(311, 224, 224, 3) (311, 224, 224, 12)
(56, 224, 224, 3) (56, 224, 224, 12)
Train on 311 samples, validate on 56 samples
Epoch 1/200
311/311 - 10s - loss: 2.5786 - acc: 0.0899 - val_loss: 2.4748 - val_acc: 0.1031
Epoch 2/200
311/311 - 4s - loss: 2.4556 - acc: 0.1228 - val_loss: 2.4084 - val_acc: 0.1573
Epoch 3/200
311/311 - 4s - loss: 2.3460 - acc: 0.1952 - val_loss: 2.1216 - val_acc: 0.3120
Epoch 4/200
311/311 - 4s - loss: 2.1470 - acc: 0.3117 - val_loss: 1.9993 - val_acc: 0.3262
Epoch 5/200
311/311 - 4s - loss: 2.2921 - acc: 0.2736 - val_loss: 2.0211 - val_acc: 0.3422
Epoch 6/200
311/311 - 4s - loss: 1.9134 - acc: 0.3986 - val_loss: 1.8635 - val_acc: 0.4859
Epoch 7/200
311/311 - 4s - loss: 1.5830 - acc: 0.5326 - val_loss: 1.3785 - val_acc: 0.5790
Epoch 8/200
311/311 - 4s - loss: 1.3529 - acc: 0.5806 - val_loss: 1.2531 - val_acc: 0.6264
Epoch 9/200
311/311 - 4s - loss: 1.2294 - acc: 0.6314 - val_loss: 1.1544 - val_acc: 0.6742
Epoch 10/200
311/311 - 4s - loss: 1.1429 - acc: 0.6609 - val_loss: 1.0647 - val_acc: 0.6884
Epoch 11/200
311/311 - 4s - loss: 1.1339 - acc: 0.6581 - val_loss: 1.0318 - val_acc: 0.6906
Epoch 12/200
311/311 - 4s - loss: 1.0408 - acc: 0.6748 - val_loss: 0.9844 - val_acc: 0.7013
Epoch 13/200
311/311 - 4s - loss: 0.9978 - acc: 0.6809 - val_loss: 0.9661 - val_acc: 0.7034
Epoch 14/200
311/311 - 4s - loss: 0.9773 - acc: 0.6867 - val_loss: 0.9084 - val_acc: 0.7137
Epoch 15/200
311/311 - 4s - loss: 0.9537 - acc: 0.6917 - val_loss: 0.8977 - val_acc: 0.7205
Epoch 16/200
311/311 - 5s - loss: 0.9266 - acc: 0.6995 - val_loss: 0.8879 - val_acc: 0.7228
Epoch 17/200
311/311 - 5s - loss: 0.9121 - acc: 0.7065 - val_loss: 0.8527 - val_acc: 0.7327
Epoch 18/200
311/311 - 5s - loss: 0.8993 - acc: 0.7131 - val_loss: 0.8378 - val_acc: 0.7364
Epoch 19/200
311/311 - 5s - loss: 0.8653 - acc: 0.7255 - val_loss: 0.8202 - val_acc: 0.7402
Epoch 20/200
311/311 - 5s - loss: 0.8710 - acc: 0.7283 - val_loss: 0.8032 - val_acc: 0.7520
Epoch 21/200
311/311 - 5s - loss: 0.8367 - acc: 0.7443 - val_loss: 0.7931 - val_acc: 0.7615
Epoch 22/200
311/311 - 5s - loss: 0.8287 - acc: 0.7514 - val_loss: 0.7741 - val_acc: 0.7656
Epoch 23/200
311/311 - 5s - loss: 0.8046 - acc: 0.7604 - val_loss: 0.7735 - val_acc: 0.7642
Epoch 24/200
311/311 - 5s - loss: 0.8113 - acc: 0.7566 - val_loss: 0.8581 - val_acc: 0.7441
Epoch 25/200
311/311 - 5s - loss: 0.8210 - acc: 0.7474 - val_loss: 0.7564 - val_acc: 0.7689
Epoch 26/200
311/311 - 5s - loss: 0.7733 - acc: 0.7691 - val_loss: 0.7549 - val_acc: 0.7716
Epoch 27/200
311/311 - 5s - loss: 0.7656 - acc: 0.7717 - val_loss: 0.7289 - val_acc: 0.7795
Epoch 28/200
311/311 - 5s - loss: 0.7548 - acc: 0.7747 - val_loss: 0.7249 - val_acc: 0.7818
Epoch 29/200
311/311 - 4s - loss: 0.7417 - acc: 0.7785 - val_loss: 0.7161 - val_acc: 0.7806
Epoch 30/200
311/311 - 5s - loss: 0.7374 - acc: 0.7797 - val_loss: 0.7325 - val_acc: 0.7758
Epoch 31/200
311/311 - 5s - loss: 0.7759 - acc: 0.7665 - val_loss: 0.7087 - val_acc: 0.7835
Epoch 32/200
311/311 - 5s - loss: 0.7242 - acc: 0.7831 - val_loss: 0.6969 - val_acc: 0.7884
Epoch 33/200
311/311 - 5s - loss: 0.7143 - acc: 0.7871 - val_loss: 0.6987 - val_acc: 0.7889
Epoch 34/200
311/311 - 5s - loss: 0.7123 - acc: 0.7875 - val_loss: 0.7053 - val_acc: 0.7841
Epoch 35/200
311/311 - 5s - loss: 0.7051 - acc: 0.7900 - val_loss: 0.6845 - val_acc: 0.7951
Epoch 36/200
311/311 - 5s - loss: 0.6893 - acc: 0.7951 - val_loss: 0.6742 - val_acc: 0.7966
Epoch 37/200
311/311 - 5s - loss: 0.6915 - acc: 0.7953 - val_loss: 0.6726 - val_acc: 0.7966
Epoch 38/200
311/311 - 5s - loss: 0.6853 - acc: 0.7979 - val_loss: 0.6572 - val_acc: 0.8031
Epoch 39/200
311/311 - 5s - loss: 0.6725 - acc: 0.8019 - val_loss: 0.6562 - val_acc: 0.8033
Epoch 40/200
311/311 - 5s - loss: 0.6766 - acc: 0.8010 - val_loss: 0.8262 - val_acc: 0.7333
Epoch 41/200
311/311 - 5s - loss: 0.6963 - acc: 0.7927 - val_loss: 0.6469 - val_acc: 0.8069
Epoch 42/200
311/311 - 5s - loss: 0.6567 - acc: 0.8090 - val_loss: 0.6397 - val_acc: 0.8112
Epoch 43/200
311/311 - 5s - loss: 0.6501 - acc: 0.8110 - val_loss: 0.6341 - val_acc: 0.8124
Epoch 44/200
311/311 - 5s - loss: 0.6415 - acc: 0.8153 - val_loss: 0.6470 - val_acc: 0.8055
Epoch 45/200
311/311 - 5s - loss: 0.6434 - acc: 0.8141 - val_loss: 0.6237 - val_acc: 0.8148
Epoch 46/200
311/311 - 5s - loss: 0.6306 - acc: 0.8193 - val_loss: 0.6210 - val_acc: 0.8179
Epoch 47/200
311/311 - 5s - loss: 0.6301 - acc: 0.8188 - val_loss: 0.6241 - val_acc: 0.8158
Epoch 48/200
311/311 - 5s - loss: 0.6279 - acc: 0.8196 - val_loss: 0.6133 - val_acc: 0.8180
Epoch 49/200
311/311 - 5s - loss: 0.6435 - acc: 0.8141 - val_loss: 0.6093 - val_acc: 0.8195
Epoch 50/200
311/311 - 5s - loss: 0.6114 - acc: 0.8251 - val_loss: 0.5998 - val_acc: 0.8237
Epoch 51/200
311/311 - 5s - loss: 0.6070 - acc: 0.8261 - val_loss: 0.6028 - val_acc: 0.8224
Epoch 52/200
311/311 - 5s - loss: 0.6032 - acc: 0.8284 - val_loss: 0.6017 - val_acc: 0.8235
Epoch 53/200
311/311 - 5s - loss: 0.6020 - acc: 0.8280 - val_loss: 0.5939 - val_acc: 0.8262
Epoch 54/200
311/311 - 5s - loss: 0.5946 - acc: 0.8310 - val_loss: 0.5930 - val_acc: 0.8264
Epoch 55/200
311/311 - 5s - loss: 0.5888 - acc: 0.8321 - val_loss: 0.5851 - val_acc: 0.8271
Epoch 56/200
311/311 - 5s - loss: 0.5847 - acc: 0.8337 - val_loss: 0.5832 - val_acc: 0.8299
Epoch 57/200
311/311 - 5s - loss: 0.5942 - acc: 0.8296 - val_loss: 0.5979 - val_acc: 0.8210
Epoch 58/200
311/311 - 5s - loss: 0.6083 - acc: 0.8246 - val_loss: 0.5791 - val_acc: 0.8303
Epoch 59/200
311/311 - 5s - loss: 0.5771 - acc: 0.8360 - val_loss: 0.5748 - val_acc: 0.8311
Epoch 60/200
311/311 - 5s - loss: 0.5732 - acc: 0.8370 - val_loss: 0.5801 - val_acc: 0.8280
Epoch 61/200
311/311 - 5s - loss: 0.5672 - acc: 0.8394 - val_loss: 0.5686 - val_acc: 0.8338
Epoch 62/200
311/311 - 5s - loss: 0.5743 - acc: 0.8373 - val_loss: 0.5660 - val_acc: 0.8347
Epoch 63/200
311/311 - 5s - loss: 0.5616 - acc: 0.8411 - val_loss: 0.5612 - val_acc: 0.8361
Epoch 64/200
311/311 - 5s - loss: 0.5625 - acc: 0.8406 - val_loss: 0.5927 - val_acc: 0.8227
Epoch 65/200
311/311 - 5s - loss: 0.5656 - acc: 0.8390 - val_loss: 0.5667 - val_acc: 0.8337
Epoch 66/200
311/311 - 5s - loss: 0.5544 - acc: 0.8433 - val_loss: 0.5571 - val_acc: 0.8369
Epoch 67/200
311/311 - 5s - loss: 0.5608 - acc: 0.8407 - val_loss: 0.5590 - val_acc: 0.8346
Epoch 68/200
311/311 - 5s - loss: 0.5506 - acc: 0.8439 - val_loss: 0.5617 - val_acc: 0.8361
Epoch 69/200
311/311 - 5s - loss: 0.5461 - acc: 0.8458 - val_loss: 0.5620 - val_acc: 0.8346
Epoch 70/200
311/311 - 5s - loss: 0.5511 - acc: 0.8441 - val_loss: 0.5482 - val_acc: 0.8392
Epoch 71/200
311/311 - 5s - loss: 0.5394 - acc: 0.8476 - val_loss: 0.5532 - val_acc: 0.8387
Epoch 72/200
311/311 - 5s - loss: 0.5430 - acc: 0.8464 - val_loss: 0.5519 - val_acc: 0.8393
Epoch 73/200
311/311 - 5s - loss: 0.5384 - acc: 0.8474 - val_loss: 0.5464 - val_acc: 0.8390
Epoch 74/200
311/311 - 5s - loss: 0.5355 - acc: 0.8480 - val_loss: 0.5475 - val_acc: 0.8407
Epoch 75/200
311/311 - 5s - loss: 0.5407 - acc: 0.8472 - val_loss: 0.5567 - val_acc: 0.8361
Epoch 76/200
311/311 - 5s - loss: 0.5327 - acc: 0.8491 - val_loss: 0.5386 - val_acc: 0.8419
Epoch 77/200
311/311 - 5s - loss: 0.5291 - acc: 0.8502 - val_loss: 0.5377 - val_acc: 0.8418
Epoch 78/200
311/311 - 5s - loss: 0.5244 - acc: 0.8516 - val_loss: 0.5441 - val_acc: 0.8386
Epoch 79/200
311/311 - 5s - loss: 0.5704 - acc: 0.8378 - val_loss: 0.5446 - val_acc: 0.8398
Epoch 80/200
311/311 - 5s - loss: 0.5286 - acc: 0.8501 - val_loss: 0.5392 - val_acc: 0.8410
Epoch 81/200
311/311 - 5s - loss: 0.5244 - acc: 0.8514 - val_loss: 0.5333 - val_acc: 0.8431
Epoch 82/200
311/311 - 5s - loss: 0.5194 - acc: 0.8528 - val_loss: 0.5421 - val_acc: 0.8415
Epoch 83/200
311/311 - 5s - loss: 0.5182 - acc: 0.8531 - val_loss: 0.5321 - val_acc: 0.8425
Epoch 84/200
311/311 - 5s - loss: 0.5205 - acc: 0.8519 - val_loss: 0.5325 - val_acc: 0.8440
Epoch 85/200
311/311 - 5s - loss: 0.5146 - acc: 0.8538 - val_loss: 0.5365 - val_acc: 0.8421
Epoch 86/200
311/311 - 5s - loss: 0.5121 - acc: 0.8551 - val_loss: 0.5281 - val_acc: 0.8440
Epoch 87/200
311/311 - 5s - loss: 0.5099 - acc: 0.8556 - val_loss: 0.5314 - val_acc: 0.8438
Epoch 88/200
311/311 - 5s - loss: 0.5093 - acc: 0.8555 - val_loss: 0.5244 - val_acc: 0.8460
Epoch 89/200
311/311 - 5s - loss: 0.5063 - acc: 0.8564 - val_loss: 0.5232 - val_acc: 0.8462
Epoch 90/200
311/311 - 5s - loss: 0.5091 - acc: 0.8556 - val_loss: 0.5240 - val_acc: 0.8462
Epoch 91/200
311/311 - 5s - loss: 0.5058 - acc: 0.8565 - val_loss: 0.5325 - val_acc: 0.8417
Epoch 92/200
311/311 - 5s - loss: 0.5079 - acc: 0.8554 - val_loss: 0.5304 - val_acc: 0.8442
Epoch 93/200
311/311 - 5s - loss: 0.5041 - acc: 0.8568 - val_loss: 0.5317 - val_acc: 0.8445
Epoch 94/200
311/311 - 5s - loss: 0.5036 - acc: 0.8573 - val_loss: 0.5384 - val_acc: 0.8422
Epoch 95/200
311/311 - 5s - loss: 0.5025 - acc: 0.8572 - val_loss: 0.5164 - val_acc: 0.8483
Epoch 96/200
311/311 - 5s - loss: 0.4975 - acc: 0.8589 - val_loss: 0.5225 - val_acc: 0.8452
Epoch 97/200
311/311 - 5s - loss: 0.4953 - acc: 0.8592 - val_loss: 0.5153 - val_acc: 0.8486
Epoch 98/200
311/311 - 5s - loss: 0.4943 - acc: 0.8599 - val_loss: 0.5314 - val_acc: 0.8422
Epoch 99/200
311/311 - 5s - loss: 0.5042 - acc: 0.8565 - val_loss: 0.5153 - val_acc: 0.8482
Epoch 100/200
311/311 - 5s - loss: 0.4949 - acc: 0.8593 - val_loss: 0.5348 - val_acc: 0.8441
Epoch 101/200
311/311 - 5s - loss: 0.4937 - acc: 0.8599 - val_loss: 0.5166 - val_acc: 0.8479
Epoch 102/200
311/311 - 5s - loss: 0.4911 - acc: 0.8606 - val_loss: 0.5134 - val_acc: 0.8492
Epoch 103/200
311/311 - 5s - loss: 0.4897 - acc: 0.8609 - val_loss: 0.5161 - val_acc: 0.8483
Epoch 104/200
311/311 - 5s - loss: 0.4872 - acc: 0.8616 - val_loss: 0.5136 - val_acc: 0.8483
Epoch 105/200
311/311 - 5s - loss: 0.4922 - acc: 0.8598 - val_loss: 0.5151 - val_acc: 0.8480
Epoch 106/200
311/311 - 5s - loss: 0.4857 - acc: 0.8620 - val_loss: 0.5139 - val_acc: 0.8489
Epoch 107/200
311/311 - 5s - loss: 0.4863 - acc: 0.8617 - val_loss: 0.5074 - val_acc: 0.8508
Epoch 108/200
311/311 - 5s - loss: 0.4816 - acc: 0.8632 - val_loss: 0.5104 - val_acc: 0.8493
Epoch 109/200
311/311 - 5s - loss: 0.4851 - acc: 0.8619 - val_loss: 0.5100 - val_acc: 0.8496
Epoch 110/200
311/311 - 5s - loss: 0.4883 - acc: 0.8609 - val_loss: 0.5078 - val_acc: 0.8502
Epoch 111/200
311/311 - 5s - loss: 0.4834 - acc: 0.8624 - val_loss: 0.5089 - val_acc: 0.8504
Epoch 112/200
311/311 - 5s - loss: 0.4789 - acc: 0.8640 - val_loss: 0.5087 - val_acc: 0.8497
Epoch 113/200
311/311 - 5s - loss: 0.4829 - acc: 0.8626 - val_loss: 0.5044 - val_acc: 0.8513
Epoch 114/200
311/311 - 5s - loss: 0.4764 - acc: 0.8646 - val_loss: 0.5071 - val_acc: 0.8494
Epoch 115/200
311/311 - 5s - loss: 0.4765 - acc: 0.8644 - val_loss: 0.5014 - val_acc: 0.8521
Epoch 116/200
311/311 - 5s - loss: 0.4756 - acc: 0.8646 - val_loss: 0.5092 - val_acc: 0.8499
Epoch 117/200
311/311 - 5s - loss: 0.4746 - acc: 0.8648 - val_loss: 0.5067 - val_acc: 0.8499
Epoch 118/200
311/311 - 5s - loss: 0.4744 - acc: 0.8649 - val_loss: 0.5000 - val_acc: 0.8527
Epoch 119/200
311/311 - 5s - loss: 0.4708 - acc: 0.8660 - val_loss: 0.5038 - val_acc: 0.8508
Epoch 120/200
311/311 - 5s - loss: 0.4735 - acc: 0.8655 - val_loss: 0.5010 - val_acc: 0.8526
Epoch 121/200
311/311 - 5s - loss: 0.4705 - acc: 0.8661 - val_loss: 0.4994 - val_acc: 0.8525
Epoch 122/200
311/311 - 5s - loss: 0.4686 - acc: 0.8664 - val_loss: 0.5038 - val_acc: 0.8511
Epoch 123/200
311/311 - 5s - loss: 0.4768 - acc: 0.8638 - val_loss: 0.5191 - val_acc: 0.8480
Epoch 124/200
311/311 - 5s - loss: 0.4737 - acc: 0.8650 - val_loss: 0.4979 - val_acc: 0.8532
Epoch 125/200
311/311 - 5s - loss: 0.4662 - acc: 0.8672 - val_loss: 0.4968 - val_acc: 0.8530
Epoch 126/200
311/311 - 5s - loss: 0.4676 - acc: 0.8666 - val_loss: 0.4955 - val_acc: 0.8537
Epoch 127/200
311/311 - 5s - loss: 0.4643 - acc: 0.8677 - val_loss: 0.4951 - val_acc: 0.8537
Epoch 128/200
311/311 - 5s - loss: 0.4645 - acc: 0.8674 - val_loss: 0.5068 - val_acc: 0.8509
Epoch 129/200
311/311 - 5s - loss: 0.4673 - acc: 0.8669 - val_loss: 0.4944 - val_acc: 0.8537
Epoch 130/200
311/311 - 5s - loss: 0.4629 - acc: 0.8678 - val_loss: 0.5007 - val_acc: 0.8523
Epoch 131/200
311/311 - 5s - loss: 0.4662 - acc: 0.8673 - val_loss: 0.4977 - val_acc: 0.8536
Epoch 132/200
311/311 - 5s - loss: 0.4672 - acc: 0.8660 - val_loss: 0.4928 - val_acc: 0.8549
Epoch 133/200
311/311 - 5s - loss: 0.4606 - acc: 0.8685 - val_loss: 0.4937 - val_acc: 0.8541
Epoch 134/200
311/311 - 5s - loss: 0.4589 - acc: 0.8690 - val_loss: 0.4916 - val_acc: 0.8544
Epoch 135/200
311/311 - 5s - loss: 0.4626 - acc: 0.8680 - val_loss: 0.5106 - val_acc: 0.8511
Epoch 136/200
311/311 - 5s - loss: 0.4642 - acc: 0.8674 - val_loss: 0.4914 - val_acc: 0.8546
Epoch 137/200
311/311 - 5s - loss: 0.4592 - acc: 0.8688 - val_loss: 0.4899 - val_acc: 0.8552
Epoch 138/200
311/311 - 5s - loss: 0.4572 - acc: 0.8694 - val_loss: 0.4924 - val_acc: 0.8545
Epoch 139/200
311/311 - 5s - loss: 0.4561 - acc: 0.8696 - val_loss: 0.4892 - val_acc: 0.8552
Epoch 140/200
311/311 - 5s - loss: 0.4559 - acc: 0.8695 - val_loss: 0.4969 - val_acc: 0.8525
Epoch 141/200
311/311 - 5s - loss: 0.4561 - acc: 0.8695 - val_loss: 0.4903 - val_acc: 0.8545
Epoch 142/200
311/311 - 5s - loss: 0.4550 - acc: 0.8699 - val_loss: 0.4882 - val_acc: 0.8555
Epoch 143/200
311/311 - 5s - loss: 0.4527 - acc: 0.8706 - val_loss: 0.4887 - val_acc: 0.8554
Epoch 144/200
311/311 - 5s - loss: 0.4559 - acc: 0.8694 - val_loss: 0.5027 - val_acc: 0.8513
Epoch 145/200
311/311 - 5s - loss: 0.4554 - acc: 0.8697 - val_loss: 0.4912 - val_acc: 0.8544
Epoch 146/200
311/311 - 5s - loss: 0.4515 - acc: 0.8710 - val_loss: 0.4887 - val_acc: 0.8547
Epoch 147/200
311/311 - 5s - loss: 0.4518 - acc: 0.8707 - val_loss: 0.4900 - val_acc: 0.8553
Epoch 148/200
311/311 - 5s - loss: 0.4520 - acc: 0.8706 - val_loss: 0.4864 - val_acc: 0.8559
Epoch 149/200
311/311 - 5s - loss: 0.4517 - acc: 0.8708 - val_loss: 0.4872 - val_acc: 0.8558
Epoch 150/200
311/311 - 5s - loss: 0.4488 - acc: 0.8715 - val_loss: 0.4851 - val_acc: 0.8561
Epoch 151/200
311/311 - 5s - loss: 0.4505 - acc: 0.8711 - val_loss: 0.4860 - val_acc: 0.8556
Epoch 152/200
311/311 - 5s - loss: 0.4490 - acc: 0.8716 - val_loss: 0.4850 - val_acc: 0.8558
Epoch 153/200
311/311 - 5s - loss: 0.4482 - acc: 0.8719 - val_loss: 0.4833 - val_acc: 0.8569
Epoch 154/200
311/311 - 5s - loss: 0.4475 - acc: 0.8717 - val_loss: 0.4884 - val_acc: 0.8547
Epoch 155/200
311/311 - 5s - loss: 0.4530 - acc: 0.8697 - val_loss: 0.4857 - val_acc: 0.8554
Epoch 156/200
311/311 - 5s - loss: 0.4457 - acc: 0.8723 - val_loss: 0.4848 - val_acc: 0.8556
Epoch 157/200
311/311 - 5s - loss: 0.4451 - acc: 0.8724 - val_loss: 0.4839 - val_acc: 0.8568
Epoch 158/200
311/311 - 5s - loss: 0.4443 - acc: 0.8727 - val_loss: 0.4817 - val_acc: 0.8574
Epoch 159/200
311/311 - 5s - loss: 0.4428 - acc: 0.8732 - val_loss: 0.4809 - val_acc: 0.8573
Epoch 160/200
311/311 - 5s - loss: 0.4434 - acc: 0.8731 - val_loss: 0.4862 - val_acc: 0.8566
Epoch 161/200
311/311 - 5s - loss: 0.4479 - acc: 0.8716 - val_loss: 0.4822 - val_acc: 0.8568
Epoch 162/200
311/311 - 5s - loss: 0.4442 - acc: 0.8728 - val_loss: 0.5035 - val_acc: 0.8511
Epoch 163/200
311/311 - 5s - loss: 0.4439 - acc: 0.8728 - val_loss: 0.4816 - val_acc: 0.8573
Epoch 164/200
311/311 - 5s - loss: 0.4404 - acc: 0.8739 - val_loss: 0.4810 - val_acc: 0.8577
Epoch 165/200
311/311 - 5s - loss: 0.4413 - acc: 0.8737 - val_loss: 0.4835 - val_acc: 0.8562
Epoch 166/200
311/311 - 5s - loss: 0.4416 - acc: 0.8732 - val_loss: 0.5006 - val_acc: 0.8514
Epoch 167/200
311/311 - 5s - loss: 0.4442 - acc: 0.8724 - val_loss: 0.4830 - val_acc: 0.8564
Epoch 168/200
311/311 - 5s - loss: 0.4401 - acc: 0.8740 - val_loss: 0.4820 - val_acc: 0.8569
Epoch 169/200
311/311 - 5s - loss: 0.4384 - acc: 0.8744 - val_loss: 0.4824 - val_acc: 0.8569
Epoch 170/200
311/311 - 5s - loss: 0.4375 - acc: 0.8747 - val_loss: 0.4792 - val_acc: 0.8579
Epoch 171/200
311/311 - 5s - loss: 0.4378 - acc: 0.8746 - val_loss: 0.4875 - val_acc: 0.8560
Epoch 172/200
311/311 - 5s - loss: 0.4401 - acc: 0.8740 - val_loss: 0.4833 - val_acc: 0.8570
Epoch 173/200
311/311 - 5s - loss: 0.4361 - acc: 0.8751 - val_loss: 0.4795 - val_acc: 0.8577
Epoch 174/200
311/311 - 5s - loss: 0.4370 - acc: 0.8748 - val_loss: 0.4801 - val_acc: 0.8573
Epoch 175/200
311/311 - 5s - loss: 0.4347 - acc: 0.8755 - val_loss: 0.4794 - val_acc: 0.8578
Epoch 176/200
311/311 - 5s - loss: 0.4347 - acc: 0.8756 - val_loss: 0.4779 - val_acc: 0.8584
Epoch 177/200
311/311 - 5s - loss: 0.4342 - acc: 0.8756 - val_loss: 0.4796 - val_acc: 0.8573
Epoch 178/200
311/311 - 5s - loss: 0.4357 - acc: 0.8751 - val_loss: 0.4759 - val_acc: 0.8586
Epoch 179/200
311/311 - 5s - loss: 0.4325 - acc: 0.8761 - val_loss: 0.4811 - val_acc: 0.8579
Epoch 180/200
311/311 - 5s - loss: 0.4360 - acc: 0.8749 - val_loss: 0.4804 - val_acc: 0.8569
Epoch 181/200
311/311 - 5s - loss: 0.4326 - acc: 0.8762 - val_loss: 0.4754 - val_acc: 0.8589
Epoch 182/200
311/311 - 5s - loss: 0.4315 - acc: 0.8764 - val_loss: 0.4751 - val_acc: 0.8595
Epoch 183/200
311/311 - 5s - loss: 0.4324 - acc: 0.8763 - val_loss: 0.4751 - val_acc: 0.8591
Epoch 184/200
311/311 - 5s - loss: 0.4296 - acc: 0.8772 - val_loss: 0.4777 - val_acc: 0.8580
Epoch 185/200
311/311 - 5s - loss: 0.4308 - acc: 0.8767 - val_loss: 0.4774 - val_acc: 0.8583
Epoch 186/200
311/311 - 5s - loss: 0.4310 - acc: 0.8767 - val_loss: 0.4778 - val_acc: 0.8580
Epoch 187/200
311/311 - 5s - loss: 0.4314 - acc: 0.8762 - val_loss: 0.4741 - val_acc: 0.8595
Epoch 188/200
311/311 - 5s - loss: 0.4312 - acc: 0.8765 - val_loss: 0.4734 - val_acc: 0.8598
Epoch 189/200
311/311 - 5s - loss: 0.4272 - acc: 0.8778 - val_loss: 0.4771 - val_acc: 0.8589
Epoch 190/200
311/311 - 5s - loss: 0.4311 - acc: 0.8767 - val_loss: 0.4756 - val_acc: 0.8593
Epoch 191/200
311/311 - 5s - loss: 0.4275 - acc: 0.8779 - val_loss: 0.4730 - val_acc: 0.8598
Epoch 192/200
311/311 - 5s - loss: 0.4276 - acc: 0.8779 - val_loss: 0.4762 - val_acc: 0.8595
Epoch 193/200
311/311 - 5s - loss: 0.4264 - acc: 0.8782 - val_loss: 0.4716 - val_acc: 0.8602
Epoch 194/200
311/311 - 5s - loss: 0.4245 - acc: 0.8788 - val_loss: 0.4727 - val_acc: 0.8598
Epoch 195/200
311/311 - 5s - loss: 0.4241 - acc: 0.8790 - val_loss: 0.4740 - val_acc: 0.8600
Epoch 196/200
311/311 - 5s - loss: 0.4332 - acc: 0.8762 - val_loss: 0.4742 - val_acc: 0.8594
Epoch 197/200
311/311 - 5s - loss: 0.4243 - acc: 0.8787 - val_loss: 0.4726 - val_acc: 0.8602
Epoch 198/200
311/311 - 5s - loss: 0.4227 - acc: 0.8795 - val_loss: 0.4728 - val_acc: 0.8597
Epoch 199/200
311/311 - 5s - loss: 0.4256 - acc: 0.8783 - val_loss: 0.4733 - val_acc: 0.8598
Epoch 200/200
311/311 - 5s - loss: 0.4245 - acc: 0.8788 - val_loss: 0.4766 - val_acc: 0.8588


Elapsed time for Keras training (s):  919.007176



End of FCN8 training


##################################################################################
Step2b: FCN8UPS MAKING PREDICTIONS
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code


validation data (X) (Y) shapes: (56, 224, 224, 3) (56, 224, 224, 12)
testing    data (X) (Y) shapes (101, 224, 224, 3) (101, 224, 224, 12)



now computing IoU over testing data set:
class ( 0)          Sky: #TP= 438351, #FP=  42491, #FN=  17622, IoU=0.879
class ( 1)         Wall: #TP=1080394, #FP= 114452, #FN= 223552, IoU=0.762
class ( 2)         Pole: #TP=      6, #FP=    703, #FN=  36414, IoU=0.000
class ( 3)         Road: #TP=1433167, #FP= 119724, #FN=  41852, IoU=0.899
class ( 4)     Sidewalk: #TP= 384811, #FP= 115599, #FN=  63622, IoU=0.682
class ( 5)   Vegetation: #TP= 779304, #FP= 114175, #FN=  47241, IoU=0.828
class ( 6)         Sign: #TP=    632, #FP=   1344, #FN=  52760, IoU=0.012
class ( 7)        Fence: #TP=  26567, #FP=  29748, #FN= 129836, IoU=0.143
class ( 8)      vehicle: #TP=  77367, #FP= 159298, #FN=  16693, IoU=0.305
class ( 9)   Pedestrian: #TP=    282, #FP=    991, #FN=  36583, IoU=0.007
class (10)    Bicyclist: #TP=    172, #FP=    145, #FN= 110800, IoU=0.002
class (11)  miscellanea: #TP=  23399, #FP= 124654, #FN=  46349, IoU=0.120
_________________
Mean IoU: 0.387

now computing IoU over validation data set:
class ( 0)          Sky: #TP= 472825, #FP=  49419, #FN=  14950, IoU=0.880
class ( 1)         Wall: #TP= 618974, #FP=  87962, #FN=  71592, IoU=0.795
class ( 2)         Pole: #TP=     11, #FP=    135, #FN=  30984, IoU=0.000
class ( 3)         Road: #TP= 857741, #FP=  52094, #FN=  29254, IoU=0.913
class ( 4)     Sidewalk: #TP=  96338, #FP=  37356, #FN=  44389, IoU=0.541
class ( 5)   Vegetation: #TP= 184461, #FP=  71037, #FN=  38423, IoU=0.628
class ( 6)         Sign: #TP=    371, #FP=    713, #FN=  29745, IoU=0.012
class ( 7)        Fence: #TP=   6943, #FP=   3624, #FN=  37714, IoU=0.144
class ( 8)      vehicle: #TP= 120783, #FP=  41131, #FN=  27616, IoU=0.637
class ( 9)   Pedestrian: #TP=     34, #FP=    152, #FN=  19997, IoU=0.002
class (10)    Bicyclist: #TP=      0, #FP=     40, #FN=  10125, IoU=0.000
class (11)  miscellanea: #TP=  54525, #FP=  53187, #FN=  42061, IoU=0.364
_________________
Mean IoU: 0.410

#######################################################################################
Step3: FCN8UPS KERAS to TENSORFLOW GRAPH CONVERSION
#######################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From Keras2TF.py:96: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
model name =  fcn8ups

 TF input node name:
[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation/truediv:0' shape=(?, 224, 224, 12) dtype=float32>]

FINISHED CREATING TF FILES


##############################################################################
Step4a: FCN8UPS FREEZE TF GRAPHS
##############################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0104 16:59:06.286590 140097355933504 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2021-01-04 16:59:06.289420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-01-04 16:59:06.310765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:59:06.310955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:59:06.311872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:59:06.312835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:59:06.313076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:59:06.314308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:59:06.315274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:59:06.317976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:59:06.319657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:59:06.319979: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-01-04 16:59:06.328165: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496220000 Hz
2021-01-04 16:59:06.329190: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cf1bf71320 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:59:06.329202: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-04 16:59:06.398697: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cf1a9698c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:59:06.398727: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2021-01-04 16:59:06.400580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:59:06.400634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:59:06.400650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:59:06.400663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:59:06.400676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:59:06.400689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:59:06.400702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:59:06.400716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:59:06.403908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:59:06.403969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:59:06.406469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-04 16:59:06.406483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2021-01-04 16:59:06.406490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2021-01-04 16:59:06.410405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22351 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fcn8ups/float_model.ckpt
I0104 16:59:06.629742 140097355933504 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fcn8ups/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0104 16:59:07.819087 140097355933504 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0104 16:59:07.819419 140097355933504 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 37 variables.
I0104 16:59:07.889413 140097355933504 graph_util_impl.py:334] Froze 37 variables.
INFO:tensorflow:Converted 37 variables to const ops.
I0104 16:59:07.956829 140097355933504 graph_util_impl.py:394] Converted 37 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fcn8ups/float_model.ckpt.meta

##############################################################################
Step4a: FCN8UPS INSPECT FROZEN GRAPH
##############################################################################

Op types used: 61 Const, 37 Identity, 18 BiasAdd, 18 Conv2D, 18 Relu, 5 StridedSlice, 5 MaxPool, 4 Mul, 4 AddV2, 3 Shape, 2 ResizeBilinear, 1 Max, 1 Pack, 1 Placeholder, 1 RealDiv, 1 Exp, 1 Conv2DBackpropInput, 1 Sub, 1 Sum

Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3])
Found 1 possible outputs: (name=activation/truediv, op=RealDiv)

##############################################################################
Step4b: FCN8UPS EVALUATING THE ORIGINAL GRAPH
##############################################################################

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 438351, #FP=  42491, #FN=  17622, IoU=0.879
class ( 1)         Wall: #TP=1080394, #FP= 114452, #FN= 223552, IoU=0.762
class ( 2)         Pole: #TP=      6, #FP=    703, #FN=  36414, IoU=0.000
class ( 3)         Road: #TP=1433167, #FP= 119724, #FN=  41852, IoU=0.899
class ( 4)     Sidewalk: #TP= 384811, #FP= 115599, #FN=  63622, IoU=0.682
class ( 5)   Vegetation: #TP= 779304, #FP= 114175, #FN=  47241, IoU=0.828
class ( 6)         Sign: #TP=    632, #FP=   1344, #FN=  52760, IoU=0.012
class ( 7)        Fence: #TP=  26567, #FP=  29748, #FN= 129836, IoU=0.143
class ( 8)      vehicle: #TP=  77367, #FP= 159298, #FN=  16693, IoU=0.305
class ( 9)   Pedestrian: #TP=    282, #FP=    991, #FN=  36583, IoU=0.007
class (10)    Bicyclist: #TP=    172, #FP=    145, #FN= 110800, IoU=0.002
class (11)  miscellanea: #TP=  23399, #FP= 124654, #FN=  46349, IoU=0.120
_________________
Mean IoU: 0.387
FINISHED!

##########################################################################
Step5a: FCN8UPS QUANTIZATION
##########################################################################


Vai_q_tensorflow v1.2.0 build for Tensorflow 1.15.2 git version
heads/1.3-0-gc680f744




N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
                                                                               10


 20% (2 of 10) |#####                    | Elapsed Time: 0:00:02 ETA:   0:00:

 30% (3 of 10) |#######                  | Elapsed Time: 0:00:03 ETA:   0:00:07
                                                                            00:06


 50% (5 of 10) |############             | Elapsed Time: 0:00:05 ETA:   0:

 60% (6 of 10) |###############          | Elapsed Time: 0:00:06 ETA:   0:00:04
                                                                          0:00:03


 80% (8 of 10) |####################     | Elapsed Time: 0:00:08 ETA:

 90% (9 of 10) |######################   | Elapsed Time: 0:00:09 ETA:   0:00:01
                                                                      e:  0:00:10
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
script running on folder  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code/../build/dataset1/img_calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 10 iterations...
INFO: Calibration Done.
INFO: Generating Deploy Model...
INFO: Deploy Model Generated.
********************* Quantization Summary *********************
INFO: Output:
  quantize_eval_model: .././build/quantize_results/fcn8ups/quantize_eval_model.pb
  deploy_model: .././build/quantize_results/fcn8ups/deploy_model.pb

##############################################################################
Step5b: FCN8UPS EVALUATE QUANTIZED GRAPH
##############################################################################

Using TensorFlow backend.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 438445, #FP=  49673, #FN=  17528, IoU=0.867
class ( 1)         Wall: #TP=1042936, #FP=  97044, #FN= 261010, IoU=0.744
class ( 2)         Pole: #TP=      5, #FP=    857, #FN=  36415, IoU=0.000
class ( 3)         Road: #TP=1439467, #FP= 136133, #FN=  35552, IoU=0.893
class ( 4)     Sidewalk: #TP= 379442, #FP= 122952, #FN=  68991, IoU=0.664
class ( 5)   Vegetation: #TP= 787524, #FP= 138732, #FN=  39021, IoU=0.816
class ( 6)         Sign: #TP=    610, #FP=   1306, #FN=  52782, IoU=0.011
class ( 7)        Fence: #TP=  21387, #FP=  30571, #FN= 135016, IoU=0.114
class ( 8)      vehicle: #TP=  76627, #FP= 154571, #FN=  17433, IoU=0.308
class ( 9)   Pedestrian: #TP=    230, #FP=    795, #FN=  36635, IoU=0.006
class (10)    Bicyclist: #TP=    136, #FP=    122, #FN= 110836, IoU=0.001
class (11)  miscellanea: #TP=  21223, #FP= 126988, #FN=  48525, IoU=0.108
_________________
Mean IoU: 0.378
FINISHED!

##########################################################################
COMPILE FCN8UPS XMODEL FILE WITH Vitis AI for VCK190 TARGET
##########################################################################


[INFO] parse raw model     :  0%|          | 0/96 [00:00<?, ?it/s]
[INFO] parse raw model     :100%|ââââââââââ| 96/96 [00:00<00:00, 21364.31it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/103 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 103/103 [00:00<00:00, 9293.81it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/84 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 84/84 [00:00<00:00, 5110.92it/s]

[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s]
[INFO] generate xmodel     : 71%|âââââââ   | 59/83 [00:00<00:00, 501.14it/s]
[INFO] ge :100%|ââââââââââ| 83/83 [00:00<00:00, 689.52it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8ups/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8ups/fcn8ups_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8ups/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8ups/fcn8ups_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-165954-7918"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 157
[UNILOG][INFO] Begin to compile...
[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_1/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[UNILOG][INFO] Total device subgraph number 8, DPU subgraph number 2
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/fcn8ups.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 3e1059aee994405e8064f0ed70f31baf, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE FCN8UPS XMODEL FILE WITH Vitis AI for ZCU102
##########################################################################


[INFO] parse raw model     :  0%|          | 0/96 [00:00<?, ?it/s]
[INFO] parse raw model     :100%|ââââââââââ| 96/96 [00:00<00:00, 21200.08it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/103 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 103/103 [00:00<00:00, 9131.93it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/84 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 84/84 [00:00<00:00, 5099.09it/s]

[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s]
[INFO] generate xmodel     : 64%|âââââââ   | 53/83 [00:00<00:00, 499.73it/s]
[INFO] ge :100%|ââââââââââ| 83/83 [00:00<00:00, 653.23it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8ups/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8ups/fcn8ups_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8ups/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8ups/fcn8ups_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-165959-7956"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 157
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/fcn8ups.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is f87d30bce5d4e9e7887ef0e1b28b4952, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE FCN8UPS MODEL FILE WITH Vitis AI for ZCU104
##########################################################################


[INFO] parse raw model     :  0%|          | 0/96 [00:00<?, ?it/s]
[INFO] parse raw model     :100%|ââââââââââ| 96/96 [00:00<00:00, 21393.83it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/103 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 103/103 [00:00<00:00, 9065.44it/s]

[INFO] infer shape (NHWC)  :  0%|          | 0/84 [00:00<?, ?it/s]
[INFO] infer shape (NHWC)  :100%|ââââââââââ| 84/84 [00:00<00:00, 4955.85it/s]

[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s]
[INFO] generate xmodel     : 57%|ââââââ    | 47/83 [00:00<00:00, 332.85it/s]
[INFO] ge : 71%|âââââââ   | 59/83 [00:00<00:00, 197.93it/s]
[INFO] generate xmodel    | 83/83 [00:00<00:00, 317.02it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/fcn8ups/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/fcn8ups/fcn8ups_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/fcn8ups/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/fcn8ups/fcn8ups_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-170009-7994"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 157
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/fcn8ups.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is e501a5cf95cac014e93db91ff88bca86, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/fcn8ups/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

#####################################
MAIN FCN8UPS FLOW COMPLETED
#####################################


##################################################################################
Step2a: TRAINING
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  1
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 224, 224, 64) 1792        input_1[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 224, 224, 64) 256         conv2d[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 224, 224, 64) 0           batch_normalization[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 224, 224, 64) 36928       activation[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 224, 224, 64) 256         conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 224, 224, 64) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 112, 112, 64) 0           activation_1[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 112, 112, 64) 0           max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 112, 128 73856       dropout[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 112, 128 512         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 112, 128 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 112, 112, 128 147584      activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 112, 112, 128 512         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 112, 112, 128 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 128)  0           activation_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 56, 128)  0           max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 56, 256)  295168      dropout_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 56, 256)  1024        conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 56, 256)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 56, 56, 256)  590080      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 56, 56, 256)  1024        conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 56, 56, 256)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)  0           activation_5[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 28, 256)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 512)  1180160     dropout_2[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 512)  2048        conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 512)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 512)  2359808     activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 512)  2048        conv2d_7[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 512)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)  0           activation_7[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 512)  0           max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 14, 14, 1024) 4719616     dropout_3[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 14, 14, 1024) 4096        conv2d_8[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 14, 14, 1024) 9438208     activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 14, 14, 1024) 4096        conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 14, 14, 1024) 0           activation_9[0][0]               WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 28, 28, 1024) 0           dropout_4[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 28, 28, 1536) 0           up_sampling2d[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 28, 28, 1536) 0           concatenate[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 512)  7078400     dropout_5[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 512)  2048        conv2d_10[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 512)  0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 512)  2359808     activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 512)  2048        conv2d_11[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 512)  0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 56, 56, 512)  0           activation_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 768)  0           up_sampling2d_1[0][0]
                                                                 activation_5[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 56, 56, 768)  0           concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 56, 56, 256)  1769728     dropout_6[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 56, 56, 256)  1024        conv2d_12[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 56, 56, 256)  0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 56, 56, 256)  590080      activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 56, 56, 256)  1024        conv2d_13[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 56, 56, 256)  0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 112, 112, 256 0           activation_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 112, 112, 384 0           up_sampling2d_2[0][0]
                                                                 activation_3[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 112, 112, 384 0           concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 112, 112, 128 442496      dropout_7[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 112, 112, 128 512         conv2d_14[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 112, 112, 128 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 112, 112, 128 147584      activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 112, 112, 128 512         conv2d_15[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 112, 112, 128 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           activation_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 224, 224, 192 0           up_sampling2d_3[0][0]
                                                                 activation_1[0][0]
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 224, 224, 192 0           concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 224, 224, 64) 110656      dropout_8[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 224, 224, 64) 256         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 224, 224, 64) 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 224, 224, 64) 36928       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 224, 224, 64) 256         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 224, 224, 64) 0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 224, 224, 12) 6924        activation_17[0][0]
==================================================================================================
Total params: 31,409,356
Trainable params: 31,397,580
Non-trainable params: 11,776
__________________________________________________________________________________________________
(311, 224, 224, 3) (311, 224, 224, 12)
(56, 224, 224, 3) (56, 224, 224, 12)
Train on 311 samples, validate on 56 samples
Epoch 1/200
311/311 - 25s - loss: 2.5918 - acc: 0.3847 - val_loss: 1.9746 - val_acc: 0.2458
Epoch 2/200
311/311 - 12s - loss: 1.5891 - acc: 0.4612 - val_loss: 1.9843 - val_acc: 0.2458
Epoch 3/200
311/311 - 12s - loss: 1.4767 - acc: 0.5557 - val_loss: 1.9718 - val_acc: 0.2473
Epoch 4/200
311/311 - 12s - loss: 1.3729 - acc: 0.6187 - val_loss: 1.9293 - val_acc: 0.2567
Epoch 5/200
311/311 - 12s - loss: 1.2733 - acc: 0.6469 - val_loss: 1.8978 - val_acc: 0.2793
Epoch 6/200
311/311 - 12s - loss: 1.1860 - acc: 0.6618 - val_loss: 1.8569 - val_acc: 0.3221
Epoch 7/200
311/311 - 12s - loss: 1.1120 - acc: 0.6718 - val_loss: 1.7787 - val_acc: 0.3895
Epoch 8/200
311/311 - 12s - loss: 1.0753 - acc: 0.6760 - val_loss: 1.6968 - val_acc: 0.4681
Epoch 9/200
311/311 - 12s - loss: 1.0477 - acc: 0.6795 - val_loss: 1.6300 - val_acc: 0.5165
Epoch 10/200
311/311 - 12s - loss: 1.0209 - acc: 0.6809 - val_loss: 1.6615 - val_acc: 0.5284
Epoch 11/200
311/311 - 12s - loss: 1.0043 - acc: 0.6816 - val_loss: 1.7779 - val_acc: 0.5689
Epoch 12/200
311/311 - 12s - loss: 1.0116 - acc: 0.6776 - val_loss: 1.3814 - val_acc: 0.6409
Epoch 13/200
311/311 - 12s - loss: 0.9942 - acc: 0.6782 - val_loss: 1.5045 - val_acc: 0.6346
Epoch 14/200
311/311 - 12s - loss: 0.9496 - acc: 0.6823 - val_loss: 1.5223 - val_acc: 0.6396
Epoch 15/200
311/311 - 12s - loss: 0.9135 - acc: 0.6854 - val_loss: 1.5314 - val_acc: 0.6261
Epoch 16/200
311/311 - 12s - loss: 0.8993 - acc: 0.6872 - val_loss: 1.5610 - val_acc: 0.6201
Epoch 17/200
311/311 - 12s - loss: 0.8982 - acc: 0.6892 - val_loss: 1.6218 - val_acc: 0.6149
Epoch 18/200
311/311 - 12s - loss: 0.8952 - acc: 0.6965 - val_loss: 1.6094 - val_acc: 0.6306
Epoch 19/200
311/311 - 12s - loss: 0.8781 - acc: 0.7037 - val_loss: 1.7546 - val_acc: 0.6287
Epoch 20/200
311/311 - 12s - loss: 0.8620 - acc: 0.7146 - val_loss: 2.4103 - val_acc: 0.5970
Epoch 21/200
311/311 - 12s - loss: 0.8949 - acc: 0.7132 - val_loss: 2.8837 - val_acc: 0.6037
Epoch 22/200
311/311 - 12s - loss: 0.8712 - acc: 0.7178 - val_loss: 2.2849 - val_acc: 0.6169
Epoch 23/200
311/311 - 12s - loss: 0.8388 - acc: 0.7222 - val_loss: 1.8785 - val_acc: 0.6380
Epoch 24/200
311/311 - 12s - loss: 0.8165 - acc: 0.7253 - val_loss: 1.4564 - val_acc: 0.6722
Epoch 25/200
311/311 - 12s - loss: 0.8071 - acc: 0.7275 - val_loss: 1.7565 - val_acc: 0.6674
Epoch 26/200
311/311 - 12s - loss: 0.7981 - acc: 0.7327 - val_loss: 1.9772 - val_acc: 0.6534
Epoch 27/200
311/311 - 12s - loss: 0.8011 - acc: 0.7338 - val_loss: 2.0446 - val_acc: 0.6544
Epoch 28/200
311/311 - 12s - loss: 0.7823 - acc: 0.7380 - val_loss: 1.3587 - val_acc: 0.6955
Epoch 29/200
311/311 - 12s - loss: 0.7669 - acc: 0.7457 - val_loss: 1.3425 - val_acc: 0.7096
Epoch 30/200
311/311 - 12s - loss: 0.7693 - acc: 0.7442 - val_loss: 1.1868 - val_acc: 0.7017
Epoch 31/200
311/311 - 12s - loss: 0.7469 - acc: 0.7501 - val_loss: 1.6344 - val_acc: 0.6776
Epoch 32/200
311/311 - 12s - loss: 0.7470 - acc: 0.7527 - val_loss: 1.3556 - val_acc: 0.6966
Epoch 33/200
311/311 - 12s - loss: 0.7348 - acc: 0.7560 - val_loss: 1.1404 - val_acc: 0.7199
Epoch 34/200
311/311 - 12s - loss: 0.7230 - acc: 0.7591 - val_loss: 1.4077 - val_acc: 0.7005
Epoch 35/200
311/311 - 12s - loss: 0.6971 - acc: 0.7668 - val_loss: 1.1580 - val_acc: 0.7188
Epoch 36/200
311/311 - 12s - loss: 0.6943 - acc: 0.7732 - val_loss: 1.1131 - val_acc: 0.7308
Epoch 37/200
311/311 - 12s - loss: 0.6836 - acc: 0.7786 - val_loss: 1.0666 - val_acc: 0.7337
Epoch 38/200
311/311 - 12s - loss: 0.6734 - acc: 0.7890 - val_loss: 1.1359 - val_acc: 0.7306
Epoch 39/200
311/311 - 12s - loss: 0.6644 - acc: 0.7971 - val_loss: 1.3113 - val_acc: 0.7225
Epoch 40/200
311/311 - 12s - loss: 0.6494 - acc: 0.8042 - val_loss: 0.9757 - val_acc: 0.7611
Epoch 41/200
311/311 - 12s - loss: 0.6310 - acc: 0.8118 - val_loss: 0.8817 - val_acc: 0.7768
Epoch 42/200
311/311 - 12s - loss: 0.6411 - acc: 0.8133 - val_loss: 0.8854 - val_acc: 0.7782
Epoch 43/200
311/311 - 12s - loss: 0.6587 - acc: 0.8125 - val_loss: 0.9662 - val_acc: 0.7651
Epoch 44/200
311/311 - 12s - loss: 0.6375 - acc: 0.8189 - val_loss: 0.9027 - val_acc: 0.7790
Epoch 45/200
311/311 - 12s - loss: 0.6241 - acc: 0.8230 - val_loss: 0.9787 - val_acc: 0.7776
Epoch 46/200
311/311 - 12s - loss: 0.6249 - acc: 0.8252 - val_loss: 0.9561 - val_acc: 0.7717
Epoch 47/200
311/311 - 12s - loss: 0.6164 - acc: 0.8278 - val_loss: 0.9222 - val_acc: 0.7780
Epoch 48/200
311/311 - 12s - loss: 0.5992 - acc: 0.8342 - val_loss: 0.8433 - val_acc: 0.7830
Epoch 49/200
311/311 - 12s - loss: 0.6355 - acc: 0.8222 - val_loss: 1.1283 - val_acc: 0.7290
Epoch 50/200
311/311 - 12s - loss: 0.6711 - acc: 0.8150 - val_loss: 1.8947 - val_acc: 0.7059
Epoch 51/200
311/311 - 12s - loss: 0.6405 - acc: 0.8235 - val_loss: 1.1846 - val_acc: 0.7463
Epoch 52/200
311/311 - 12s - loss: 0.6080 - acc: 0.8288 - val_loss: 1.0388 - val_acc: 0.7476
Epoch 53/200
311/311 - 12s - loss: 0.5867 - acc: 0.8340 - val_loss: 0.8424 - val_acc: 0.7857
Epoch 54/200
311/311 - 12s - loss: 0.5779 - acc: 0.8396 - val_loss: 1.1891 - val_acc: 0.7475
Epoch 55/200
311/311 - 12s - loss: 0.7318 - acc: 0.7924 - val_loss: 1.5371 - val_acc: 0.5692
Epoch 56/200
311/311 - 12s - loss: 0.6990 - acc: 0.8068 - val_loss: 1.4299 - val_acc: 0.6029
Epoch 57/200
311/311 - 12s - loss: 0.6574 - acc: 0.8175 - val_loss: 1.3859 - val_acc: 0.6365
Epoch 58/200
311/311 - 12s - loss: 0.6263 - acc: 0.8247 - val_loss: 1.5892 - val_acc: 0.6634
Epoch 59/200
311/311 - 12s - loss: 0.5956 - acc: 0.8301 - val_loss: 1.3775 - val_acc: 0.7002
Epoch 60/200
311/311 - 12s - loss: 0.6019 - acc: 0.8334 - val_loss: 1.1614 - val_acc: 0.7373
Epoch 61/200
311/311 - 12s - loss: 0.5936 - acc: 0.8374 - val_loss: 1.0720 - val_acc: 0.7559
Epoch 62/200
311/311 - 12s - loss: 0.5659 - acc: 0.8410 - val_loss: 1.0202 - val_acc: 0.7543
Epoch 63/200
311/311 - 12s - loss: 0.5718 - acc: 0.8365 - val_loss: 0.9026 - val_acc: 0.7559
Epoch 64/200
311/311 - 12s - loss: 0.6005 - acc: 0.8295 - val_loss: 0.8537 - val_acc: 0.7819
Epoch 65/200
311/311 - 12s - loss: 0.5653 - acc: 0.8395 - val_loss: 0.8507 - val_acc: 0.7860
Epoch 66/200
311/311 - 12s - loss: 0.5521 - acc: 0.8435 - val_loss: 0.8894 - val_acc: 0.7893
Epoch 67/200
311/311 - 12s - loss: 0.5441 - acc: 0.8466 - val_loss: 0.9458 - val_acc: 0.7859
Epoch 68/200
311/311 - 12s - loss: 0.5391 - acc: 0.8497 - val_loss: 0.9197 - val_acc: 0.7954
Epoch 69/200
311/311 - 12s - loss: 0.5276 - acc: 0.8513 - val_loss: 0.9442 - val_acc: 0.7922
Epoch 70/200
311/311 - 12s - loss: 0.5229 - acc: 0.8527 - val_loss: 0.9577 - val_acc: 0.7912
Epoch 71/200
311/311 - 12s - loss: 0.5796 - acc: 0.8370 - val_loss: 1.2475 - val_acc: 0.6441
Epoch 72/200
311/311 - 12s - loss: 0.5899 - acc: 0.8359 - val_loss: 1.1478 - val_acc: 0.6801
Epoch 73/200
311/311 - 12s - loss: 0.5624 - acc: 0.8424 - val_loss: 1.0834 - val_acc: 0.7012
Epoch 74/200
311/311 - 12s - loss: 0.5404 - acc: 0.8469 - val_loss: 1.0210 - val_acc: 0.7399
Epoch 75/200
311/311 - 12s - loss: 0.5384 - acc: 0.8483 - val_loss: 0.9495 - val_acc: 0.7833
Epoch 76/200
311/311 - 12s - loss: 0.5572 - acc: 0.8481 - val_loss: 0.9126 - val_acc: 0.7778
Epoch 77/200
311/311 - 12s - loss: 0.5342 - acc: 0.8531 - val_loss: 0.9221 - val_acc: 0.7578
Epoch 78/200
311/311 - 12s - loss: 0.5252 - acc: 0.8549 - val_loss: 0.9172 - val_acc: 0.7880
Epoch 79/200
311/311 - 12s - loss: 0.5257 - acc: 0.8565 - val_loss: 0.7800 - val_acc: 0.8011
Epoch 80/200
311/311 - 12s - loss: 0.5199 - acc: 0.8593 - val_loss: 0.8084 - val_acc: 0.8070
Epoch 81/200
311/311 - 12s - loss: 0.5174 - acc: 0.8585 - val_loss: 0.7651 - val_acc: 0.8100
Epoch 82/200
311/311 - 12s - loss: 0.5129 - acc: 0.8609 - val_loss: 0.7839 - val_acc: 0.8064
Epoch 83/200
311/311 - 12s - loss: 0.5119 - acc: 0.8595 - val_loss: 0.8353 - val_acc: 0.8042
Epoch 84/200
311/311 - 12s - loss: 0.5011 - acc: 0.8620 - val_loss: 0.8171 - val_acc: 0.7929
Epoch 85/200
311/311 - 12s - loss: 0.5074 - acc: 0.8613 - val_loss: 0.7926 - val_acc: 0.8117
Epoch 86/200
311/311 - 12s - loss: 0.5008 - acc: 0.8620 - val_loss: 0.8111 - val_acc: 0.8057
Epoch 87/200
311/311 - 12s - loss: 0.4956 - acc: 0.8626 - val_loss: 0.8967 - val_acc: 0.7877
Epoch 88/200
311/311 - 12s - loss: 0.5016 - acc: 0.8638 - val_loss: 0.8029 - val_acc: 0.7969
Epoch 89/200
311/311 - 12s - loss: 0.6678 - acc: 0.7934 - val_loss: 1.2281 - val_acc: 0.6687
Epoch 90/200
311/311 - 12s - loss: 0.6346 - acc: 0.8229 - val_loss: 1.4808 - val_acc: 0.6992
Epoch 91/200
311/311 - 12s - loss: 0.5762 - acc: 0.8377 - val_loss: 1.3061 - val_acc: 0.7267
Epoch 92/200
311/311 - 12s - loss: 0.5437 - acc: 0.8440 - val_loss: 1.2432 - val_acc: 0.7413
Epoch 93/200
311/311 - 12s - loss: 0.5303 - acc: 0.8477 - val_loss: 1.0998 - val_acc: 0.7656
Epoch 94/200
311/311 - 12s - loss: 0.5242 - acc: 0.8496 - val_loss: 0.9604 - val_acc: 0.7667
Epoch 95/200
311/311 - 12s - loss: 0.5224 - acc: 0.8520 - val_loss: 0.8215 - val_acc: 0.7941
Epoch 96/200
311/311 - 12s - loss: 0.5163 - acc: 0.8539 - val_loss: 0.9669 - val_acc: 0.7769
Epoch 97/200
311/311 - 12s - loss: 0.5626 - acc: 0.8395 - val_loss: 1.3183 - val_acc: 0.6042
Epoch 98/200
311/311 - 12s - loss: 0.6782 - acc: 0.8068 - val_loss: 1.5339 - val_acc: 0.6260
Epoch 99/200
311/311 - 12s - loss: 0.6080 - acc: 0.8334 - val_loss: 1.7542 - val_acc: 0.6485
Epoch 100/200
311/311 - 12s - loss: 0.5758 - acc: 0.8416 - val_loss: 1.9418 - val_acc: 0.6546
Epoch 101/200
311/311 - 12s - loss: 0.5593 - acc: 0.8477 - val_loss: 2.0227 - val_acc: 0.6720
Epoch 102/200
311/311 - 12s - loss: 0.5456 - acc: 0.8507 - val_loss: 1.7624 - val_acc: 0.7033
Epoch 103/200
311/311 - 12s - loss: 0.5216 - acc: 0.8549 - val_loss: 1.6396 - val_acc: 0.7320
Epoch 104/200
311/311 - 12s - loss: 0.5159 - acc: 0.8547 - val_loss: 1.4273 - val_acc: 0.7408
Epoch 105/200
311/311 - 12s - loss: 0.5102 - acc: 0.8563 - val_loss: 1.4112 - val_acc: 0.7418
Epoch 106/200
311/311 - 12s - loss: 0.5021 - acc: 0.8583 - val_loss: 1.4017 - val_acc: 0.7309
Epoch 107/200
311/311 - 12s - loss: 0.5080 - acc: 0.8583 - val_loss: 1.2524 - val_acc: 0.7498
Epoch 108/200
311/311 - 12s - loss: 0.5026 - acc: 0.8598 - val_loss: 1.0811 - val_acc: 0.7576
Epoch 109/200
311/311 - 12s - loss: 0.4939 - acc: 0.8621 - val_loss: 1.1661 - val_acc: 0.7599
Epoch 110/200
311/311 - 12s - loss: 0.4874 - acc: 0.8635 - val_loss: 1.1805 - val_acc: 0.7654
Epoch 111/200
311/311 - 12s - loss: 0.4818 - acc: 0.8644 - val_loss: 1.1672 - val_acc: 0.7643
Epoch 112/200
311/311 - 12s - loss: 0.4822 - acc: 0.8660 - val_loss: 1.0124 - val_acc: 0.7802
Epoch 113/200
311/311 - 12s - loss: 0.4859 - acc: 0.8651 - val_loss: 1.1351 - val_acc: 0.7652
Epoch 114/200
311/311 - 12s - loss: 0.4729 - acc: 0.8674 - val_loss: 1.1462 - val_acc: 0.7741
Epoch 115/200
311/311 - 12s - loss: 0.4690 - acc: 0.8686 - val_loss: 1.0235 - val_acc: 0.7854
Epoch 116/200
311/311 - 12s - loss: 0.4804 - acc: 0.8668 - val_loss: 1.0409 - val_acc: 0.7861
Epoch 117/200
311/311 - 12s - loss: 0.4704 - acc: 0.8680 - val_loss: 0.9876 - val_acc: 0.7848
Epoch 118/200
311/311 - 12s - loss: 0.4692 - acc: 0.8682 - val_loss: 0.9338 - val_acc: 0.7938
Epoch 119/200
311/311 - 12s - loss: 0.4848 - acc: 0.8629 - val_loss: 1.4512 - val_acc: 0.7475
Epoch 120/200
311/311 - 12s - loss: 0.4995 - acc: 0.8575 - val_loss: 0.9101 - val_acc: 0.7997
Epoch 121/200
311/311 - 12s - loss: 0.4810 - acc: 0.8660 - val_loss: 0.8245 - val_acc: 0.8135
Epoch 122/200
311/311 - 12s - loss: 0.4863 - acc: 0.8662 - val_loss: 0.8653 - val_acc: 0.7800
Epoch 123/200
311/311 - 12s - loss: 0.5427 - acc: 0.8466 - val_loss: 0.9887 - val_acc: 0.7545
Epoch 124/200
311/311 - 12s - loss: 0.5188 - acc: 0.8548 - val_loss: 0.8284 - val_acc: 0.7922
Epoch 125/200
311/311 - 12s - loss: 0.4964 - acc: 0.8608 - val_loss: 0.8984 - val_acc: 0.8016
Epoch 126/200
311/311 - 12s - loss: 0.5140 - acc: 0.8556 - val_loss: 0.9960 - val_acc: 0.7650
Epoch 127/200
311/311 - 12s - loss: 0.5061 - acc: 0.8599 - val_loss: 0.8930 - val_acc: 0.7912
Epoch 128/200
311/311 - 12s - loss: 0.4958 - acc: 0.8627 - val_loss: 0.8891 - val_acc: 0.8104
Epoch 129/200
311/311 - 12s - loss: 0.4778 - acc: 0.8661 - val_loss: 0.9294 - val_acc: 0.7985
Epoch 130/200
311/311 - 12s - loss: 0.4686 - acc: 0.8684 - val_loss: 0.9542 - val_acc: 0.8089
Epoch 131/200
311/311 - 13s - loss: 0.4724 - acc: 0.8664 - val_loss: 0.9576 - val_acc: 0.8026
Epoch 132/200
311/311 - 13s - loss: 0.4760 - acc: 0.8655 - val_loss: 0.8740 - val_acc: 0.8117
Epoch 133/200
311/311 - 13s - loss: 0.4650 - acc: 0.8701 - val_loss: 0.8671 - val_acc: 0.8138
Epoch 134/200
311/311 - 13s - loss: 0.4543 - acc: 0.8721 - val_loss: 0.8695 - val_acc: 0.8094
Epoch 135/200
311/311 - 13s - loss: 0.4586 - acc: 0.8710 - val_loss: 0.7686 - val_acc: 0.8096
Epoch 136/200
311/311 - 13s - loss: 0.4626 - acc: 0.8700 - val_loss: 0.7904 - val_acc: 0.8084
Epoch 137/200
311/311 - 13s - loss: 0.4525 - acc: 0.8718 - val_loss: 0.7782 - val_acc: 0.8124
Epoch 138/200
311/311 - 13s - loss: 0.4507 - acc: 0.8729 - val_loss: 0.7963 - val_acc: 0.8136
Epoch 139/200
311/311 - 13s - loss: 0.4481 - acc: 0.8729 - val_loss: 0.8192 - val_acc: 0.8087
Epoch 140/200
311/311 - 13s - loss: 0.4432 - acc: 0.8747 - val_loss: 0.8471 - val_acc: 0.8035
Epoch 141/200
311/311 - 13s - loss: 0.4429 - acc: 0.8750 - val_loss: 0.7703 - val_acc: 0.8138
Epoch 142/200
311/311 - 13s - loss: 0.4390 - acc: 0.8762 - val_loss: 0.7682 - val_acc: 0.8166
Epoch 143/200
311/311 - 13s - loss: 0.4450 - acc: 0.8734 - val_loss: 0.8289 - val_acc: 0.7992
Epoch 144/200
311/311 - 13s - loss: 0.4628 - acc: 0.8685 - val_loss: 0.7305 - val_acc: 0.8052
Epoch 145/200
311/311 - 12s - loss: 0.4632 - acc: 0.8696 - val_loss: 0.7899 - val_acc: 0.8044
Epoch 146/200
311/311 - 13s - loss: 0.4459 - acc: 0.8745 - val_loss: 0.7461 - val_acc: 0.8112
Epoch 147/200
311/311 - 13s - loss: 0.4392 - acc: 0.8757 - val_loss: 0.8103 - val_acc: 0.8163
Epoch 148/200
311/311 - 13s - loss: 0.4507 - acc: 0.8742 - val_loss: 0.8512 - val_acc: 0.8091
Epoch 149/200
311/311 - 13s - loss: 0.4500 - acc: 0.8749 - val_loss: 0.6918 - val_acc: 0.8228
Epoch 150/200
311/311 - 12s - loss: 0.4575 - acc: 0.8720 - val_loss: 0.7234 - val_acc: 0.8027
Epoch 151/200
311/311 - 13s - loss: 0.4631 - acc: 0.8708 - val_loss: 0.6761 - val_acc: 0.8155
Epoch 152/200
311/311 - 12s - loss: 0.4533 - acc: 0.8733 - val_loss: 0.6854 - val_acc: 0.8123
Epoch 153/200
311/311 - 13s - loss: 0.4412 - acc: 0.8763 - val_loss: 0.7077 - val_acc: 0.8089
Epoch 154/200
311/311 - 12s - loss: 0.4373 - acc: 0.8764 - val_loss: 0.7649 - val_acc: 0.8004
Epoch 155/200
311/311 - 13s - loss: 0.4372 - acc: 0.8765 - val_loss: 0.7546 - val_acc: 0.8035
Epoch 156/200
311/311 - 13s - loss: 0.4795 - acc: 0.8668 - val_loss: 1.0587 - val_acc: 0.7542
Epoch 157/200
311/311 - 13s - loss: 0.4707 - acc: 0.8684 - val_loss: 0.7649 - val_acc: 0.7982
Epoch 158/200
311/311 - 13s - loss: 0.4577 - acc: 0.8721 - val_loss: 0.6977 - val_acc: 0.8136
Epoch 159/200
311/311 - 13s - loss: 0.4557 - acc: 0.8734 - val_loss: 0.6612 - val_acc: 0.8228
Epoch 160/200
311/311 - 13s - loss: 0.4600 - acc: 0.8715 - val_loss: 0.6580 - val_acc: 0.8240
Epoch 161/200
311/311 - 13s - loss: 0.4568 - acc: 0.8736 - val_loss: 0.7120 - val_acc: 0.8144
Epoch 162/200
311/311 - 13s - loss: 0.4455 - acc: 0.8762 - val_loss: 0.6687 - val_acc: 0.8247
Epoch 163/200
311/311 - 13s - loss: 0.4436 - acc: 0.8764 - val_loss: 0.6490 - val_acc: 0.8296
Epoch 164/200
311/311 - 13s - loss: 0.4372 - acc: 0.8781 - val_loss: 0.6666 - val_acc: 0.8297
Epoch 165/200
311/311 - 13s - loss: 0.4347 - acc: 0.8777 - val_loss: 0.6942 - val_acc: 0.8287
Epoch 166/200
311/311 - 13s - loss: 0.4297 - acc: 0.8792 - val_loss: 0.7052 - val_acc: 0.8297
Epoch 167/200
311/311 - 13s - loss: 0.4392 - acc: 0.8761 - val_loss: 0.7122 - val_acc: 0.8273
Epoch 168/200
311/311 - 13s - loss: 0.4334 - acc: 0.8777 - val_loss: 0.7264 - val_acc: 0.8258
Epoch 169/200
311/311 - 13s - loss: 0.4336 - acc: 0.8773 - val_loss: 0.6834 - val_acc: 0.8251
Epoch 170/200
311/311 - 13s - loss: 0.4341 - acc: 0.8767 - val_loss: 0.6646 - val_acc: 0.8320
Epoch 171/200
311/311 - 13s - loss: 0.4437 - acc: 0.8752 - val_loss: 0.7299 - val_acc: 0.8254
Epoch 172/200
311/311 - 13s - loss: 0.4350 - acc: 0.8772 - val_loss: 0.6966 - val_acc: 0.8294
Epoch 173/200
311/311 - 12s - loss: 0.4242 - acc: 0.8793 - val_loss: 0.7274 - val_acc: 0.8239
Epoch 174/200
311/311 - 13s - loss: 0.5246 - acc: 0.8594 - val_loss: 1.0025 - val_acc: 0.7674
Epoch 175/200
311/311 - 13s - loss: 0.5060 - acc: 0.8580 - val_loss: 0.7225 - val_acc: 0.8088
Epoch 176/200
311/311 - 13s - loss: 0.4760 - acc: 0.8652 - val_loss: 0.7022 - val_acc: 0.8174
Epoch 177/200
311/311 - 13s - loss: 0.4608 - acc: 0.8690 - val_loss: 0.7145 - val_acc: 0.8111
Epoch 178/200
311/311 - 13s - loss: 0.4462 - acc: 0.8730 - val_loss: 0.7341 - val_acc: 0.8112
Epoch 179/200
311/311 - 13s - loss: 0.4407 - acc: 0.8752 - val_loss: 0.7737 - val_acc: 0.8039
Epoch 180/200
311/311 - 13s - loss: 0.4459 - acc: 0.8733 - val_loss: 0.7544 - val_acc: 0.8055
Epoch 181/200
311/311 - 13s - loss: 0.4365 - acc: 0.8763 - val_loss: 0.7264 - val_acc: 0.8123
Epoch 182/200
311/311 - 13s - loss: 0.4374 - acc: 0.8761 - val_loss: 0.7110 - val_acc: 0.8180
Epoch 183/200
311/311 - 13s - loss: 0.4353 - acc: 0.8768 - val_loss: 0.7231 - val_acc: 0.8176
Epoch 184/200
311/311 - 13s - loss: 0.4239 - acc: 0.8801 - val_loss: 0.7156 - val_acc: 0.8204
Epoch 185/200
311/311 - 13s - loss: 0.4263 - acc: 0.8794 - val_loss: 0.7273 - val_acc: 0.8164
Epoch 186/200
311/311 - 13s - loss: 0.4531 - acc: 0.8678 - val_loss: 0.7251 - val_acc: 0.8198
Epoch 187/200
311/311 - 13s - loss: 0.4394 - acc: 0.8741 - val_loss: 0.7521 - val_acc: 0.8080
Epoch 188/200
311/311 - 12s - loss: 0.4346 - acc: 0.8761 - val_loss: 0.7729 - val_acc: 0.8045
Epoch 189/200
311/311 - 13s - loss: 0.4342 - acc: 0.8761 - val_loss: 0.7257 - val_acc: 0.8129
Epoch 190/200
311/311 - 12s - loss: 0.4289 - acc: 0.8782 - val_loss: 0.6888 - val_acc: 0.8171
Epoch 191/200
311/311 - 13s - loss: 0.4258 - acc: 0.8787 - val_loss: 0.6839 - val_acc: 0.8218
Epoch 192/200
311/311 - 13s - loss: 0.4248 - acc: 0.8792 - val_loss: 0.7349 - val_acc: 0.8105
Epoch 193/200
311/311 - 12s - loss: 0.4216 - acc: 0.8802 - val_loss: 0.7278 - val_acc: 0.8166
Epoch 194/200
311/311 - 13s - loss: 0.4263 - acc: 0.8796 - val_loss: 0.7296 - val_acc: 0.8121
Epoch 195/200
311/311 - 13s - loss: 0.4197 - acc: 0.8810 - val_loss: 0.7413 - val_acc: 0.8127
Epoch 196/200
311/311 - 12s - loss: 0.4171 - acc: 0.8820 - val_loss: 0.7451 - val_acc: 0.8136
Epoch 197/200
311/311 - 13s - loss: 0.4154 - acc: 0.8823 - val_loss: 0.7728 - val_acc: 0.8175
Epoch 198/200
311/311 - 13s - loss: 0.4240 - acc: 0.8813 - val_loss: 0.8141 - val_acc: 0.8087
Epoch 199/200
311/311 - 13s - loss: 0.4237 - acc: 0.8816 - val_loss: 0.7701 - val_acc: 0.8181
Epoch 200/200
311/311 - 13s - loss: 0.4156 - acc: 0.8831 - val_loss: 0.7651 - val_acc: 0.8180


Elapsed time for Keras training (s):  2502.492676



End of UNET training

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  2
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 224, 224, 64) 1792        input_1[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 224, 224, 64) 256         conv2d[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 224, 224, 64) 0           batch_normalization[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 224, 224, 64) 36928       activation[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 224, 224, 64) 256         conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 224, 224, 64) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 112, 112, 64) 0           activation_1[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 112, 112, 64) 0           max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 112, 128 73856       dropout[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 112, 128 512         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 112, 128 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 112, 112, 128 147584      activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 112, 112, 128 512         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 112, 112, 128 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 128)  0           activation_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 56, 128)  0           max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 56, 256)  295168      dropout_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 56, 256)  1024        conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 56, 256)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 56, 56, 256)  590080      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 56, 56, 256)  1024        conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 56, 56, 256)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)  0           activation_5[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 28, 256)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 512)  1180160     dropout_2[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 512)  2048        conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 512)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 512)  2359808     activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 512)  2048        conv2d_7[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 512)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)  0           activation_7[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 512)  0           max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 14, 14, 1024) 4719616     dropout_3[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 14, 14, 1024) 4096        conv2d_8[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 14, 14, 1024) 9438208     activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 14, 14, 1024) 4096        conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 14, 14, 1024) 0           activation_9[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 28, 28, 1024) 0           dropout_4[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 512)  2097664     up_sampling2d[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 28, 28, 1024) 0           conv2d_10[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 28, 28, 1024) 0           concatenate[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 512)  4719104     dropout_5[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 512)  2048        conv2d_11[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 512)  0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 28, 28, 512)  2359808     activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 512)  2048        conv2d_12[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 512)  0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 56, 56, 512)  0           activation_11[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 56, 56, 256)  524544      up_sampling2d_1[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 512)  0           conv2d_13[0][0]
                                                                 activation_5[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 56, 56, 512)  0           concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 56, 56, 256)  1179904     dropout_6[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 56, 56, 256)  1024        conv2d_14[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 56, 56, 256)  0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 56, 56, 256)  590080      activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 56, 56, 256)  1024        conv2d_15[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 56, 56, 256)  0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 112, 112, 256 0           activation_13[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 112, 112, 128 131200      up_sampling2d_2[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 112, 112, 256 0           conv2d_16[0][0]
                                                                 activation_3[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 112, 112, 256 0           concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 112, 112, 128 295040      dropout_7[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 112, 112, 128 512         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 112, 112, 128 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 112, 112, 128 147584      activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 112, 112, 128 512         conv2d_18[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 112, 112, 128 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 224, 224, 128 0           activation_15[0][0]
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 224, 224, 64) 32832       up_sampling2d_3[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 224, 224, 128 0           conv2d_19[0][0]
                                                                 activation_1[0][0]
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 224, 224, 128 0           concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 224, 224, 64) 73792       dropout_8[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 224, 224, 64) 256         conv2d_20[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 224, 224, 64) 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 224, 224, 64) 36928       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 224, 224, 64) 256         conv2d_21[0][0]
__________________________________________________________________________________________________WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

activation_17 (Activation)      (None, 224, 224, 64) 0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 224, 224, 12) 6924        activation_17[0][0]
==================================================================================================
Total params: 31,062,156
Trainable params: 31,050,380
Non-trainable params: 11,776
__________________________________________________________________________________________________
(311, 224, 224, 3) (311, 224, 224, 12)
(56, 224, 224, 3) (56, 224, 224, 12)
Train on 311 samples, validate on 56 samples
Epoch 1/200
311/311 - 26s - loss: 2.9620 - acc: 0.3005 - val_loss: 2.0296 - val_acc: 0.3157
Epoch 2/200
311/311 - 13s - loss: 1.8007 - acc: 0.3852 - val_loss: 1.9412 - val_acc: 0.3157
Epoch 3/200
311/311 - 13s - loss: 1.6973 - acc: 0.4346 - val_loss: 1.9311 - val_acc: 0.3157
Epoch 4/200
311/311 - 13s - loss: 1.6334 - acc: 0.4468 - val_loss: 1.9251 - val_acc: 0.3157
Epoch 5/200
311/311 - 13s - loss: 1.5739 - acc: 0.4499 - val_loss: 1.9227 - val_acc: 0.3157
Epoch 6/200
311/311 - 13s - loss: 1.5226 - acc: 0.4517 - val_loss: 1.9203 - val_acc: 0.3157
Epoch 7/200
311/311 - 13s - loss: 1.4705 - acc: 0.4546 - val_loss: 1.9145 - val_acc: 0.3157
Epoch 8/200
311/311 - 13s - loss: 1.4231 - acc: 0.5305 - val_loss: 1.9086 - val_acc: 0.3153
Epoch 9/200
311/311 - 13s - loss: 1.3708 - acc: 0.5879 - val_loss: 1.8996 - val_acc: 0.3123
Epoch 10/200
311/311 - 13s - loss: 1.3206 - acc: 0.6058 - val_loss: 1.8842 - val_acc: 0.3056
Epoch 11/200
311/311 - 13s - loss: 1.2744 - acc: 0.6187 - val_loss: 1.8768 - val_acc: 0.2930
Epoch 12/200
311/311 - 13s - loss: 1.2323 - acc: 0.6256 - val_loss: 1.8442 - val_acc: 0.2888
Epoch 13/200
311/311 - 14s - loss: 1.1820 - acc: 0.6359 - val_loss: 1.7863 - val_acc: 0.3028
Epoch 14/200
311/311 - 13s - loss: 1.1354 - acc: 0.6425 - val_loss: 1.7215 - val_acc: 0.3352
Epoch 15/200
311/311 - 14s - loss: 1.1118 - acc: 0.6477 - val_loss: 1.6475 - val_acc: 0.3797
Epoch 16/200
311/311 - 13s - loss: 1.1078 - acc: 0.6516 - val_loss: 1.5337 - val_acc: 0.4488
Epoch 17/200
311/311 - 14s - loss: 1.0794 - acc: 0.6564 - val_loss: 1.2750 - val_acc: 0.6230
Epoch 18/200
311/311 - 13s - loss: 1.0393 - acc: 0.6619 - val_loss: 1.1880 - val_acc: 0.6473
Epoch 19/200
311/311 - 13s - loss: 1.0167 - acc: 0.6641 - val_loss: 1.1334 - val_acc: 0.6647
Epoch 20/200
311/311 - 13s - loss: 0.9992 - acc: 0.6662 - val_loss: 1.1042 - val_acc: 0.6735
Epoch 21/200
311/311 - 14s - loss: 0.9951 - acc: 0.6675 - val_loss: 1.1290 - val_acc: 0.6766
Epoch 22/200
311/311 - 13s - loss: 0.9810 - acc: 0.6691 - val_loss: 1.0457 - val_acc: 0.6852
Epoch 23/200
311/311 - 13s - loss: 0.9649 - acc: 0.6711 - val_loss: 1.0240 - val_acc: 0.6868
Epoch 24/200
311/311 - 13s - loss: 0.9494 - acc: 0.6743 - val_loss: 0.9737 - val_acc: 0.6950
Epoch 25/200
311/311 - 13s - loss: 0.9353 - acc: 0.6756 - val_loss: 0.9539 - val_acc: 0.7041
Epoch 26/200
311/311 - 13s - loss: 0.9267 - acc: 0.6772 - val_loss: 0.9397 - val_acc: 0.6984
Epoch 27/200
311/311 - 13s - loss: 0.9287 - acc: 0.6762 - val_loss: 1.3699 - val_acc: 0.6359
Epoch 28/200
311/311 - 13s - loss: 0.9610 - acc: 0.6751 - val_loss: 1.1818 - val_acc: 0.7027
Epoch 29/200
311/311 - 13s - loss: 0.9235 - acc: 0.6779 - val_loss: 1.0547 - val_acc: 0.7070
Epoch 30/200
311/311 - 13s - loss: 0.9006 - acc: 0.6852 - val_loss: 1.0431 - val_acc: 0.7082
Epoch 31/200
311/311 - 13s - loss: 0.9186 - acc: 0.6903 - val_loss: 1.2654 - val_acc: 0.6740
Epoch 32/200
311/311 - 13s - loss: 0.9172 - acc: 0.6907 - val_loss: 1.0747 - val_acc: 0.7006
Epoch 33/200
311/311 - 14s - loss: 0.9171 - acc: 0.6975 - val_loss: 0.8959 - val_acc: 0.7206
Epoch 34/200
311/311 - 13s - loss: 0.8887 - acc: 0.7051 - val_loss: 0.9068 - val_acc: 0.7183
Epoch 35/200
311/311 - 13s - loss: 0.9959 - acc: 0.6875 - val_loss: 3.3564 - val_acc: 0.5503
Epoch 36/200
311/311 - 13s - loss: 1.0683 - acc: 0.6585 - val_loss: 3.4950 - val_acc: 0.5383
Epoch 37/200
311/311 - 13s - loss: 1.0160 - acc: 0.6774 - val_loss: 3.1995 - val_acc: 0.5583
Epoch 38/200
311/311 - 13s - loss: 0.9721 - acc: 0.6934 - val_loss: 2.2626 - val_acc: 0.6218
Epoch 39/200
311/311 - 13s - loss: 0.9347 - acc: 0.7042 - val_loss: 1.6911 - val_acc: 0.6627
Epoch 40/200
311/311 - 13s - loss: 0.9009 - acc: 0.7094 - val_loss: 1.2897 - val_acc: 0.6906
Epoch 41/200
311/311 - 13s - loss: 0.8841 - acc: 0.7137 - val_loss: 1.1348 - val_acc: 0.7042
Epoch 42/200
311/311 - 13s - loss: 0.8710 - acc: 0.7143 - val_loss: 0.9975 - val_acc: 0.7125
Epoch 43/200
311/311 - 13s - loss: 0.8648 - acc: 0.7170 - val_loss: 0.9567 - val_acc: 0.7132
Epoch 44/200
311/311 - 13s - loss: 0.8595 - acc: 0.7184 - val_loss: 0.9134 - val_acc: 0.7201
Epoch 45/200
311/311 - 13s - loss: 0.8539 - acc: 0.7173 - val_loss: 0.9205 - val_acc: 0.7127
Epoch 46/200
311/311 - 13s - loss: 0.8620 - acc: 0.7174 - val_loss: 0.9468 - val_acc: 0.7181
Epoch 47/200
311/311 - 13s - loss: 0.8513 - acc: 0.7194 - val_loss: 0.8688 - val_acc: 0.7275
Epoch 48/200
311/311 - 13s - loss: 0.8394 - acc: 0.7224 - val_loss: 0.9196 - val_acc: 0.7261
Epoch 49/200
311/311 - 13s - loss: 0.8313 - acc: 0.7268 - val_loss: 0.9026 - val_acc: 0.7321
Epoch 50/200
311/311 - 13s - loss: 0.8195 - acc: 0.7330 - val_loss: 0.8477 - val_acc: 0.7404
Epoch 51/200
311/311 - 13s - loss: 0.8078 - acc: 0.7356 - val_loss: 0.9113 - val_acc: 0.7365
Epoch 52/200
311/311 - 13s - loss: 0.8032 - acc: 0.7365 - val_loss: 0.9021 - val_acc: 0.7389
Epoch 53/200
311/311 - 13s - loss: 0.8320 - acc: 0.7298 - val_loss: 1.0903 - val_acc: 0.6703
Epoch 54/200
311/311 - 13s - loss: 0.8287 - acc: 0.7337 - val_loss: 1.0764 - val_acc: 0.6849
Epoch 55/200
311/311 - 13s - loss: 0.8715 - acc: 0.7202 - val_loss: 1.1313 - val_acc: 0.6767
Epoch 56/200
311/311 - 13s - loss: 0.8370 - acc: 0.7263 - val_loss: 0.9017 - val_acc: 0.7201
Epoch 57/200
311/311 - 13s - loss: 0.8122 - acc: 0.7345 - val_loss: 0.8253 - val_acc: 0.7494
Epoch 58/200
311/311 - 13s - loss: 0.8109 - acc: 0.7383 - val_loss: 0.8228 - val_acc: 0.7541
Epoch 59/200
311/311 - 13s - loss: 0.7986 - acc: 0.7428 - val_loss: 0.8189 - val_acc: 0.7519
Epoch 60/200
311/311 - 13s - loss: 0.7857 - acc: 0.7456 - val_loss: 0.8492 - val_acc: 0.7423
Epoch 61/200
311/311 - 13s - loss: 0.7840 - acc: 0.7467 - val_loss: 0.9258 - val_acc: 0.7328
Epoch 62/200
311/311 - 13s - loss: 0.7829 - acc: 0.7488 - val_loss: 0.8873 - val_acc: 0.7443
Epoch 63/200
311/311 - 13s - loss: 0.7804 - acc: 0.7486 - val_loss: 0.8751 - val_acc: 0.7475
Epoch 64/200
311/311 - 13s - loss: 0.7800 - acc: 0.7467 - val_loss: 0.9629 - val_acc: 0.7216
Epoch 65/200
311/311 - 13s - loss: 0.7726 - acc: 0.7506 - val_loss: 0.9392 - val_acc: 0.7288
Epoch 66/200
311/311 - 13s - loss: 0.7635 - acc: 0.7508 - val_loss: 0.9426 - val_acc: 0.7322
Epoch 67/200
311/311 - 13s - loss: 0.7539 - acc: 0.7519 - val_loss: 0.9992 - val_acc: 0.7215
Epoch 68/200
311/311 - 13s - loss: 0.7540 - acc: 0.7523 - val_loss: 1.0262 - val_acc: 0.7023
Epoch 69/200
311/311 - 13s - loss: 0.7622 - acc: 0.7509 - val_loss: 0.9459 - val_acc: 0.7193
Epoch 70/200
311/311 - 13s - loss: 0.7448 - acc: 0.7532 - val_loss: 0.9703 - val_acc: 0.7322
Epoch 71/200
311/311 - 13s - loss: 0.7427 - acc: 0.7545 - val_loss: 0.9002 - val_acc: 0.7444
Epoch 72/200
311/311 - 13s - loss: 0.7473 - acc: 0.7559 - val_loss: 0.9893 - val_acc: 0.7328
Epoch 73/200
311/311 - 13s - loss: 0.7320 - acc: 0.7575 - val_loss: 0.9524 - val_acc: 0.7369
Epoch 74/200
311/311 - 13s - loss: 0.7305 - acc: 0.7573 - val_loss: 1.0655 - val_acc: 0.7157
Epoch 75/200
311/311 - 13s - loss: 0.7272 - acc: 0.7589 - val_loss: 1.0030 - val_acc: 0.7022
Epoch 76/200
311/311 - 13s - loss: 0.7278 - acc: 0.7599 - val_loss: 1.0084 - val_acc: 0.7156
Epoch 77/200
311/311 - 13s - loss: 0.7162 - acc: 0.7616 - val_loss: 0.9395 - val_acc: 0.7443
Epoch 78/200
311/311 - 13s - loss: 0.7159 - acc: 0.7629 - val_loss: 0.8483 - val_acc: 0.7582
Epoch 79/200
311/311 - 13s - loss: 0.7110 - acc: 0.7654 - val_loss: 0.9004 - val_acc: 0.7513
Epoch 80/200
311/311 - 13s - loss: 0.7009 - acc: 0.7689 - val_loss: 0.8897 - val_acc: 0.7456
Epoch 81/200
311/311 - 13s - loss: 0.6944 - acc: 0.7728 - val_loss: 0.9999 - val_acc: 0.6807
Epoch 82/200
311/311 - 13s - loss: 0.7057 - acc: 0.7722 - val_loss: 0.9880 - val_acc: 0.6732
Epoch 83/200
311/311 - 13s - loss: 0.6991 - acc: 0.7753 - val_loss: 0.9481 - val_acc: 0.6952
Epoch 84/200
311/311 - 13s - loss: 0.6885 - acc: 0.7807 - val_loss: 0.9785 - val_acc: 0.7116
Epoch 85/200
311/311 - 13s - loss: 0.6867 - acc: 0.7901 - val_loss: 0.9142 - val_acc: 0.7016
Epoch 86/200
311/311 - 13s - loss: 0.6773 - acc: 0.7947 - val_loss: 0.9752 - val_acc: 0.6981
Epoch 87/200
311/311 - 13s - loss: 0.6742 - acc: 0.7998 - val_loss: 0.9352 - val_acc: 0.7020
Epoch 88/200
311/311 - 13s - loss: 0.6735 - acc: 0.7996 - val_loss: 0.9551 - val_acc: 0.7236
Epoch 89/200
311/311 - 13s - loss: 0.6686 - acc: 0.8012 - val_loss: 0.8477 - val_acc: 0.7574
Epoch 90/200
311/311 - 13s - loss: 0.6612 - acc: 0.8068 - val_loss: 0.7909 - val_acc: 0.7756
Epoch 91/200
311/311 - 13s - loss: 0.6585 - acc: 0.8070 - val_loss: 0.8076 - val_acc: 0.7765
Epoch 92/200
311/311 - 13s - loss: 0.7366 - acc: 0.7931 - val_loss: 1.2377 - val_acc: 0.5976
Epoch 93/200
311/311 - 13s - loss: 0.7483 - acc: 0.7830 - val_loss: 0.8151 - val_acc: 0.7512
Epoch 94/200
311/311 - 14s - loss: 0.7047 - acc: 0.7955 - val_loss: 0.8849 - val_acc: 0.7174
Epoch 95/200
311/311 - 13s - loss: 0.6798 - acc: 0.8018 - val_loss: 0.8578 - val_acc: 0.7168
Epoch 96/200
311/311 - 13s - loss: 0.6666 - acc: 0.8059 - val_loss: 0.8741 - val_acc: 0.7285
Epoch 97/200
311/311 - 13s - loss: 0.6479 - acc: 0.8139 - val_loss: 0.8350 - val_acc: 0.7434
Epoch 98/200
311/311 - 13s - loss: 0.6391 - acc: 0.8164 - val_loss: 0.8985 - val_acc: 0.7277
Epoch 99/200
311/311 - 13s - loss: 0.6489 - acc: 0.8144 - val_loss: 0.7328 - val_acc: 0.7940
Epoch 100/200
311/311 - 13s - loss: 0.6592 - acc: 0.8138 - val_loss: 0.8159 - val_acc: 0.7776
Epoch 101/200
311/311 - 13s - loss: 0.6473 - acc: 0.8168 - val_loss: 0.8320 - val_acc: 0.7716
Epoch 102/200
311/311 - 13s - loss: 0.6338 - acc: 0.8219 - val_loss: 0.8591 - val_acc: 0.7600
Epoch 103/200
311/311 - 13s - loss: 0.6214 - acc: 0.8252 - val_loss: 0.7687 - val_acc: 0.7898
Epoch 104/200
311/311 - 13s - loss: 0.6311 - acc: 0.8242 - val_loss: 0.7688 - val_acc: 0.7890
Epoch 105/200
311/311 - 13s - loss: 0.6288 - acc: 0.8265 - val_loss: 0.8132 - val_acc: 0.7816
Epoch 106/200
311/311 - 13s - loss: 0.6219 - acc: 0.8271 - val_loss: 0.7172 - val_acc: 0.8043
Epoch 107/200
311/311 - 13s - loss: 0.6152 - acc: 0.8271 - val_loss: 0.8199 - val_acc: 0.7827
Epoch 108/200
311/311 - 13s - loss: 0.6073 - acc: 0.8301 - val_loss: 0.7789 - val_acc: 0.7931
Epoch 109/200
311/311 - 13s - loss: 0.6036 - acc: 0.8312 - val_loss: 0.7564 - val_acc: 0.7982
Epoch 110/200
311/311 - 13s - loss: 0.6032 - acc: 0.8310 - val_loss: 0.8300 - val_acc: 0.7813
Epoch 111/200
311/311 - 13s - loss: 0.5971 - acc: 0.8317 - val_loss: 0.8968 - val_acc: 0.7706
Epoch 112/200
311/311 - 13s - loss: 0.6003 - acc: 0.8306 - val_loss: 0.7781 - val_acc: 0.7862
Epoch 113/200
311/311 - 13s - loss: 0.5951 - acc: 0.8324 - val_loss: 0.6828 - val_acc: 0.8036
Epoch 114/200
311/311 - 13s - loss: 0.5925 - acc: 0.8330 - val_loss: 0.7310 - val_acc: 0.8009
Epoch 115/200
311/311 - 13s - loss: 0.6174 - acc: 0.8269 - val_loss: 0.8242 - val_acc: 0.7786
Epoch 116/200
311/311 - 13s - loss: 0.6147 - acc: 0.8291 - val_loss: 0.7293 - val_acc: 0.7990
Epoch 117/200
311/311 - 13s - loss: 0.5923 - acc: 0.8339 - val_loss: 0.7168 - val_acc: 0.8034
Epoch 118/200
311/311 - 13s - loss: 0.5886 - acc: 0.8349 - val_loss: 0.7089 - val_acc: 0.8041
Epoch 119/200
311/311 - 13s - loss: 0.5788 - acc: 0.8368 - val_loss: 0.7269 - val_acc: 0.8038
Epoch 120/200
311/311 - 13s - loss: 0.5800 - acc: 0.8353 - val_loss: 0.7464 - val_acc: 0.8002
Epoch 121/200
311/311 - 13s - loss: 0.5657 - acc: 0.8383 - val_loss: 0.6805 - val_acc: 0.8141
Epoch 122/200
311/311 - 13s - loss: 0.5622 - acc: 0.8390 - val_loss: 0.7141 - val_acc: 0.8085
Epoch 123/200
311/311 - 13s - loss: 0.5583 - acc: 0.8399 - val_loss: 0.7287 - val_acc: 0.8026
Epoch 124/200
311/311 - 13s - loss: 0.5666 - acc: 0.8380 - val_loss: 0.7245 - val_acc: 0.8058
Epoch 125/200
311/311 - 13s - loss: 0.5557 - acc: 0.8401 - val_loss: 0.6940 - val_acc: 0.8114
Epoch 126/200
311/311 - 13s - loss: 0.5496 - acc: 0.8412 - val_loss: 0.7037 - val_acc: 0.8104
Epoch 127/200
311/311 - 13s - loss: 0.5565 - acc: 0.8409 - val_loss: 0.6705 - val_acc: 0.8157
Epoch 128/200
311/311 - 13s - loss: 0.5614 - acc: 0.8410 - val_loss: 0.7468 - val_acc: 0.8001
Epoch 129/200
311/311 - 13s - loss: 0.5621 - acc: 0.8397 - val_loss: 0.6767 - val_acc: 0.8122
Epoch 130/200
311/311 - 13s - loss: 0.5526 - acc: 0.8418 - val_loss: 0.7174 - val_acc: 0.8068
Epoch 131/200
311/311 - 13s - loss: 0.5458 - acc: 0.8425 - val_loss: 0.7116 - val_acc: 0.8095
Epoch 132/200
311/311 - 13s - loss: 0.5451 - acc: 0.8428 - val_loss: 0.6995 - val_acc: 0.8105
Epoch 133/200
311/311 - 13s - loss: 0.5530 - acc: 0.8418 - val_loss: 0.6562 - val_acc: 0.8193
Epoch 134/200
311/311 - 13s - loss: 0.5751 - acc: 0.8392 - val_loss: 0.6315 - val_acc: 0.8274
Epoch 135/200
311/311 - 13s - loss: 0.5612 - acc: 0.8409 - val_loss: 0.8922 - val_acc: 0.7587
Epoch 136/200
311/311 - 13s - loss: 0.5600 - acc: 0.8430 - val_loss: 0.7615 - val_acc: 0.8009
Epoch 137/200
311/311 - 13s - loss: 0.5555 - acc: 0.8458 - val_loss: 0.8609 - val_acc: 0.7821
Epoch 138/200
311/311 - 13s - loss: 0.5627 - acc: 0.8444 - val_loss: 0.7744 - val_acc: 0.8015
Epoch 139/200
311/311 - 13s - loss: 0.5580 - acc: 0.8478 - val_loss: 0.8878 - val_acc: 0.7588
Epoch 140/200
311/311 - 13s - loss: 0.5605 - acc: 0.8430 - val_loss: 0.7059 - val_acc: 0.8085
Epoch 141/200
311/311 - 13s - loss: 0.5517 - acc: 0.8442 - val_loss: 0.6680 - val_acc: 0.8214
Epoch 142/200
311/311 - 13s - loss: 0.5370 - acc: 0.8477 - val_loss: 0.6960 - val_acc: 0.8129
Epoch 143/200
311/311 - 13s - loss: 0.5522 - acc: 0.8451 - val_loss: 0.7623 - val_acc: 0.7988
Epoch 144/200
311/311 - 13s - loss: 0.5393 - acc: 0.8479 - val_loss: 0.7633 - val_acc: 0.8044
Epoch 145/200
311/311 - 13s - loss: 0.5342 - acc: 0.8480 - val_loss: 0.6948 - val_acc: 0.8097
Epoch 146/200
311/311 - 13s - loss: 0.5333 - acc: 0.8498 - val_loss: 0.7074 - val_acc: 0.8153
Epoch 147/200
311/311 - 13s - loss: 0.5329 - acc: 0.8502 - val_loss: 0.7486 - val_acc: 0.8031
Epoch 148/200
311/311 - 13s - loss: 0.5433 - acc: 0.8483 - val_loss: 0.7240 - val_acc: 0.8101
Epoch 149/200
311/311 - 13s - loss: 0.5471 - acc: 0.8444 - val_loss: 0.6319 - val_acc: 0.8258
Epoch 150/200
311/311 - 13s - loss: 0.5335 - acc: 0.8486 - val_loss: 0.6507 - val_acc: 0.8273
Epoch 151/200
311/311 - 13s - loss: 0.5296 - acc: 0.8501 - val_loss: 0.6940 - val_acc: 0.8213
Epoch 152/200
311/311 - 13s - loss: 0.5264 - acc: 0.8514 - val_loss: 0.6758 - val_acc: 0.8205
Epoch 153/200
311/311 - 13s - loss: 0.5176 - acc: 0.8536 - val_loss: 0.7048 - val_acc: 0.8199
Epoch 154/200
311/311 - 13s - loss: 0.5141 - acc: 0.8551 - val_loss: 0.6789 - val_acc: 0.8212
Epoch 155/200
311/311 - 13s - loss: 0.5147 - acc: 0.8540 - val_loss: 0.7269 - val_acc: 0.8147
Epoch 156/200
311/311 - 13s - loss: 0.5426 - acc: 0.8490 - val_loss: 0.7235 - val_acc: 0.8185
Epoch 157/200
311/311 - 13s - loss: 0.5306 - acc: 0.8514 - val_loss: 0.6020 - val_acc: 0.8379
Epoch 158/200
311/311 - 13s - loss: 0.5206 - acc: 0.8540 - val_loss: 0.6488 - val_acc: 0.8300
Epoch 159/200
311/311 - 13s - loss: 0.5096 - acc: 0.8566 - val_loss: 0.6950 - val_acc: 0.8228
Epoch 160/200
311/311 - 13s - loss: 0.5084 - acc: 0.8562 - val_loss: 0.6205 - val_acc: 0.8324
Epoch 161/200
311/311 - 13s - loss: 0.5050 - acc: 0.8574 - val_loss: 0.6205 - val_acc: 0.8345
Epoch 162/200
311/311 - 13s - loss: 0.5079 - acc: 0.8572 - val_loss: 0.6873 - val_acc: 0.8257
Epoch 163/200
311/311 - 13s - loss: 0.5078 - acc: 0.8566 - val_loss: 0.6305 - val_acc: 0.8319
Epoch 164/200
311/311 - 13s - loss: 0.5051 - acc: 0.8572 - val_loss: 0.6394 - val_acc: 0.8311
Epoch 165/200
311/311 - 13s - loss: 0.5081 - acc: 0.8564 - val_loss: 0.6422 - val_acc: 0.8278
Epoch 166/200
311/311 - 13s - loss: 0.4954 - acc: 0.8603 - val_loss: 0.6211 - val_acc: 0.8330
Epoch 167/200
311/311 - 13s - loss: 0.4979 - acc: 0.8604 - val_loss: 0.6627 - val_acc: 0.8265
Epoch 168/200
311/311 - 13s - loss: 0.5026 - acc: 0.8590 - val_loss: 0.6166 - val_acc: 0.8330
Epoch 169/200
311/311 - 13s - loss: 0.4968 - acc: 0.8611 - val_loss: 0.6317 - val_acc: 0.8307
Epoch 170/200
311/311 - 13s - loss: 0.5021 - acc: 0.8598 - val_loss: 0.6395 - val_acc: 0.8323
Epoch 171/200
311/311 - 13s - loss: 0.5005 - acc: 0.8599 - val_loss: 0.6130 - val_acc: 0.8352
Epoch 172/200
311/311 - 13s - loss: 0.4982 - acc: 0.8601 - val_loss: 0.6346 - val_acc: 0.8326
Epoch 173/200
311/311 - 13s - loss: 0.4963 - acc: 0.8611 - val_loss: 0.5941 - val_acc: 0.8390
Epoch 174/200
311/311 - 13s - loss: 0.4970 - acc: 0.8608 - val_loss: 0.6414 - val_acc: 0.8296
Epoch 175/200
311/311 - 13s - loss: 0.5051 - acc: 0.8598 - val_loss: 0.6222 - val_acc: 0.8346
Epoch 176/200
311/311 - 13s - loss: 0.5130 - acc: 0.8581 - val_loss: 0.6804 - val_acc: 0.8287
Epoch 177/200
311/311 - 13s - loss: 0.5066 - acc: 0.8591 - val_loss: 0.6579 - val_acc: 0.8323
Epoch 178/200
311/311 - 13s - loss: 0.5246 - acc: 0.8534 - val_loss: 0.7134 - val_acc: 0.8181
Epoch 179/200
311/311 - 13s - loss: 0.5440 - acc: 0.8518 - val_loss: 0.6550 - val_acc: 0.8270
Epoch 180/200
311/311 - 13s - loss: 0.5370 - acc: 0.8546 - val_loss: 0.6549 - val_acc: 0.8274
Epoch 181/200
311/311 - 13s - loss: 0.5154 - acc: 0.8578 - val_loss: 0.6012 - val_acc: 0.8378
Epoch 182/200
311/311 - 13s - loss: 0.5054 - acc: 0.8597 - val_loss: 0.6427 - val_acc: 0.8326
Epoch 183/200
311/311 - 13s - loss: 0.5015 - acc: 0.8603 - val_loss: 0.6395 - val_acc: 0.8319
Epoch 184/200
311/311 - 13s - loss: 0.4956 - acc: 0.8626 - val_loss: 0.6470 - val_acc: 0.8298
Epoch 185/200
311/311 - 13s - loss: 0.4924 - acc: 0.8628 - val_loss: 0.6329 - val_acc: 0.8324
Epoch 186/200
311/311 - 13s - loss: 0.4874 - acc: 0.8646 - val_loss: 0.6143 - val_acc: 0.8382
Epoch 187/200
311/311 - 13s - loss: 0.4901 - acc: 0.8648 - val_loss: 0.6470 - val_acc: 0.8331
Epoch 188/200
311/311 - 13s - loss: 0.4902 - acc: 0.8637 - val_loss: 0.6378 - val_acc: 0.8329
Epoch 189/200
311/311 - 13s - loss: 0.4869 - acc: 0.8643 - val_loss: 0.5896 - val_acc: 0.8426
Epoch 190/200
311/311 - 13s - loss: 0.4879 - acc: 0.8637 - val_loss: 0.6452 - val_acc: 0.8335
Epoch 191/200
311/311 - 13s - loss: 0.4807 - acc: 0.8651 - val_loss: 0.6066 - val_acc: 0.8381
Epoch 192/200
311/311 - 13s - loss: 0.4836 - acc: 0.8638 - val_loss: 0.6200 - val_acc: 0.8365
Epoch 193/200
311/311 - 13s - loss: 0.4844 - acc: 0.8643 - val_loss: 0.6140 - val_acc: 0.8371
Epoch 194/200
311/311 - 13s - loss: 0.4781 - acc: 0.8659 - val_loss: 0.5979 - val_acc: 0.8422
Epoch 195/200
311/311 - 13s - loss: 0.5298 - acc: 0.8507 - val_loss: 0.7104 - val_acc: 0.8167
Epoch 196/200
311/311 - 13s - loss: 0.5235 - acc: 0.8521 - val_loss: 0.6367 - val_acc: 0.8337
Epoch 197/200
311/311 - 13s - loss: 0.5153 - acc: 0.8538 - val_loss: 0.6786 - val_acc: 0.8188
Epoch 198/200
311/311 - 13s - loss: 0.5050 - acc: 0.8594 - val_loss: 0.6064 - val_acc: 0.8381
Epoch 199/200
311/311 - 13s - loss: 0.4966 - acc: 0.8617 - val_loss: 0.6133 - val_acc: 0.8369
Epoch 200/200
311/311 - 13s - loss: 0.4874 - acc: 0.8641 - val_loss: 0.5997 - val_acc: 0.8423


Elapsed time for Keras training (s):  2676.847177



End of UNET training

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  3
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 224, 224, 3) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 224, 224, 64) 1792        input_1[0][0]
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 224, 224, 64) 256         conv2d[0][0]
__________________________________________________________________________________________________
activation (Activation)         (None, 224, 224, 64) 0           batch_normalization[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 224, 224, 64) 36928       activation[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 224, 224, 64) 256         conv2d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 224, 224, 64) 0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 112, 112, 64) 0           activation_1[0][0]
__________________________________________________________________________________________________
dropout (Dropout)               (None, 112, 112, 64) 0           max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 112, 112, 128 73856       dropout[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 112, 112, 128 512         conv2d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 112, 112, 128 0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 112, 112, 128 147584      activation_2[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 112, 112, 128 512         conv2d_3[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 112, 112, 128 0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 128)  0           activation_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 56, 56, 128)  0           max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 56, 56, 256)  295168      dropout_1[0][0]
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 56, 56, 256)  1024        conv2d_4[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 56, 56, 256)  0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 56, 56, 256)  590080      activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 56, 56, 256)  1024        conv2d_5[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 56, 56, 256)  0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 256)  0           activation_5[0][0]
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 28, 28, 256)  0           max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 28, 28, 512)  1180160     dropout_2[0][0]
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 28, 28, 512)  2048        conv2d_6[0][0]
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 28, 28, 512)  0           batch_normalization_6[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 28, 28, 512)  2359808     activation_6[0][0]
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 28, 28, 512)  2048        conv2d_7[0][0]
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 28, 28, 512)  0           batch_normalization_7[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 14, 14, 512)  0           activation_7[0][0]
__________________________________________________________________________________________________
dropout_3 (Dropout)             (None, 14, 14, 512)  0           max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 14, 14, 1024) 4719616     dropout_3[0][0]
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 14, 14, 1024) 4096        conv2d_8[0][0]
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_8[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 14, 14, 1024) 9438208     activation_8[0][0]
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 14, 14, 1024) 4096        conv2d_9[0][0]
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 14, 14, 1024) 0           batch_normalization_9[0][0]
__________________________________________________________________________________________________
dropout_4 (Dropout)             (None, 14, 14, 1024) 0           activation_9[0][0]               WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where

__________________________________________________________________________________________________
conv2d_transpose (Conv2DTranspo (None, 28, 28, 512)  4719104     dropout_4[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 28, 28, 1024) 0           conv2d_transpose[0][0]
                                                                 activation_7[0][0]
__________________________________________________________________________________________________
dropout_5 (Dropout)             (None, 28, 28, 1024) 0           concatenate[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 28, 28, 512)  4719104     dropout_5[0][0]
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 28, 28, 512)  2048        conv2d_10[0][0]
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 28, 28, 512)  0           batch_normalization_10[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 28, 28, 512)  2359808     activation_10[0][0]
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 28, 28, 512)  2048        conv2d_11[0][0]
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 28, 28, 512)  0           batch_normalization_11[0][0]
__________________________________________________________________________________________________
conv2d_transpose_1 (Conv2DTrans (None, 56, 56, 256)  1179904     activation_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 56, 56, 512)  0           conv2d_transpose_1[0][0]
                                                                 activation_5[0][0]
__________________________________________________________________________________________________
dropout_6 (Dropout)             (None, 56, 56, 512)  0           concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 56, 56, 256)  1179904     dropout_6[0][0]
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 56, 56, 256)  1024        conv2d_12[0][0]
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 56, 56, 256)  0           batch_normalization_12[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 56, 56, 256)  590080      activation_12[0][0]
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 56, 56, 256)  1024        conv2d_13[0][0]
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 56, 56, 256)  0           batch_normalization_13[0][0]
__________________________________________________________________________________________________
conv2d_transpose_2 (Conv2DTrans (None, 112, 112, 128 295040      activation_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 112, 112, 256 0           conv2d_transpose_2[0][0]
                                                                 activation_3[0][0]
__________________________________________________________________________________________________
dropout_7 (Dropout)             (None, 112, 112, 256 0           concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 112, 112, 128 295040      dropout_7[0][0]
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 112, 112, 128 512         conv2d_14[0][0]
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 112, 112, 128 0           batch_normalization_14[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 112, 112, 128 147584      activation_14[0][0]
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 112, 112, 128 512         conv2d_15[0][0]
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 112, 112, 128 0           batch_normalization_15[0][0]
__________________________________________________________________________________________________
conv2d_transpose_3 (Conv2DTrans (None, 224, 224, 64) 73792       activation_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 224, 224, 128 0           conv2d_transpose_3[0][0]
                                                                 activation_1[0][0]
__________________________________________________________________________________________________
dropout_8 (Dropout)             (None, 224, 224, 128 0           concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 224, 224, 64) 73792       dropout_8[0][0]
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 224, 224, 64) 256         conv2d_16[0][0]
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 224, 224, 64) 0           batch_normalization_16[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 224, 224, 64) 36928       activation_16[0][0]
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 224, 224, 64) 256         conv2d_17[0][0]
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 224, 224, 64) 0           batch_normalization_17[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 224, 224, 12) 6924        activation_17[0][0]
==================================================================================================
Total params: 34,543,756
Trainable params: 34,531,980
Non-trainable params: 11,776
__________________________________________________________________________________________________
(311, 224, 224, 3) (311, 224, 224, 12)
(56, 224, 224, 3) (56, 224, 224, 12)
Train on 311 samples, validate on 56 samples
Epoch 1/200
311/311 - 24s - loss: 2.5959 - acc: 0.3641 - val_loss: 1.9753 - val_acc: 0.2458
Epoch 2/200
311/311 - 12s - loss: 1.6868 - acc: 0.4156 - val_loss: 1.9538 - val_acc: 0.2458
Epoch 3/200
311/311 - 12s - loss: 1.6210 - acc: 0.4818 - val_loss: 1.9459 - val_acc: 0.2468
Epoch 4/200
311/311 - 11s - loss: 1.5604 - acc: 0.5468 - val_loss: 1.9331 - val_acc: 0.2510
Epoch 5/200
311/311 - 12s - loss: 1.4955 - acc: 0.5595 - val_loss: 1.8909 - val_acc: 0.2699
Epoch 6/200
311/311 - 12s - loss: 1.4487 - acc: 0.5724 - val_loss: 1.8114 - val_acc: 0.3119
Epoch 7/200
311/311 - 12s - loss: 1.4067 - acc: 0.5841 - val_loss: 1.6996 - val_acc: 0.3846
Epoch 8/200
311/311 - 11s - loss: 1.3666 - acc: 0.6013 - val_loss: 1.6048 - val_acc: 0.4991
Epoch 9/200
311/311 - 12s - loss: 1.3297 - acc: 0.6146 - val_loss: 1.5336 - val_acc: 0.5398
Epoch 10/200
311/311 - 12s - loss: 1.2929 - acc: 0.6256 - val_loss: 1.4902 - val_acc: 0.5481
Epoch 11/200
311/311 - 12s - loss: 1.2564 - acc: 0.6330 - val_loss: 1.4585 - val_acc: 0.5585
Epoch 12/200
311/311 - 12s - loss: 1.2220 - acc: 0.6380 - val_loss: 1.4417 - val_acc: 0.5691
Epoch 13/200
311/311 - 12s - loss: 1.1952 - acc: 0.6431 - val_loss: 1.4467 - val_acc: 0.5726
Epoch 14/200
311/311 - 12s - loss: 1.1753 - acc: 0.6480 - val_loss: 1.5471 - val_acc: 0.5605
Epoch 15/200
311/311 - 12s - loss: 1.1585 - acc: 0.6513 - val_loss: 1.4643 - val_acc: 0.5831
Epoch 16/200
311/311 - 12s - loss: 1.1386 - acc: 0.6559 - val_loss: 1.4106 - val_acc: 0.5964
Epoch 17/200
311/311 - 12s - loss: 1.1170 - acc: 0.6615 - val_loss: 1.5529 - val_acc: 0.5870
Epoch 18/200
311/311 - 12s - loss: 1.1040 - acc: 0.6620 - val_loss: 1.4400 - val_acc: 0.6123
Epoch 19/200
311/311 - 12s - loss: 1.0929 - acc: 0.6641 - val_loss: 1.5923 - val_acc: 0.5724
Epoch 20/200
311/311 - 12s - loss: 1.0806 - acc: 0.6668 - val_loss: 1.9802 - val_acc: 0.5465
Epoch 21/200
311/311 - 12s - loss: 1.0677 - acc: 0.6692 - val_loss: 1.6973 - val_acc: 0.5732
Epoch 22/200
311/311 - 12s - loss: 1.0622 - acc: 0.6708 - val_loss: 2.1970 - val_acc: 0.5448
Epoch 23/200
311/311 - 12s - loss: 1.0437 - acc: 0.6726 - val_loss: 2.4439 - val_acc: 0.5649
Epoch 24/200
311/311 - 12s - loss: 1.0384 - acc: 0.6745 - val_loss: 2.1420 - val_acc: 0.5786
Epoch 25/200
311/311 - 11s - loss: 1.0237 - acc: 0.6771 - val_loss: 1.2800 - val_acc: 0.6632
Epoch 26/200
311/311 - 12s - loss: 1.0210 - acc: 0.6751 - val_loss: 1.8044 - val_acc: 0.5953
Epoch 27/200
311/311 - 12s - loss: 0.9991 - acc: 0.6792 - val_loss: 2.1380 - val_acc: 0.5977
Epoch 28/200
311/311 - 12s - loss: 0.9925 - acc: 0.6807 - val_loss: 1.6682 - val_acc: 0.6170
Epoch 29/200
311/311 - 12s - loss: 0.9968 - acc: 0.6809 - val_loss: 2.9045 - val_acc: 0.5982
Epoch 30/200
311/311 - 12s - loss: 0.9920 - acc: 0.6816 - val_loss: 1.5635 - val_acc: 0.6589
Epoch 31/200
311/311 - 12s - loss: 0.9799 - acc: 0.6824 - val_loss: 1.4245 - val_acc: 0.6721
Epoch 32/200
311/311 - 12s - loss: 0.9667 - acc: 0.6840 - val_loss: 1.9652 - val_acc: 0.6465
Epoch 33/200
311/311 - 12s - loss: 0.9603 - acc: 0.6848 - val_loss: 2.2608 - val_acc: 0.6242
Epoch 34/200
311/311 - 12s - loss: 0.9689 - acc: 0.6837 - val_loss: 1.6267 - val_acc: 0.6537
Epoch 35/200
311/311 - 12s - loss: 0.9575 - acc: 0.6854 - val_loss: 2.0700 - val_acc: 0.6415
Epoch 36/200
311/311 - 12s - loss: 0.9463 - acc: 0.6870 - val_loss: 2.1592 - val_acc: 0.6505
Epoch 37/200
311/311 - 12s - loss: 0.9419 - acc: 0.6875 - val_loss: 1.8096 - val_acc: 0.6606
Epoch 38/200
311/311 - 12s - loss: 0.9411 - acc: 0.6881 - val_loss: 1.8019 - val_acc: 0.6645
Epoch 39/200
311/311 - 12s - loss: 0.9326 - acc: 0.6885 - val_loss: 1.3017 - val_acc: 0.6913
Epoch 40/200
311/311 - 12s - loss: 0.9289 - acc: 0.6886 - val_loss: 1.5188 - val_acc: 0.6821
Epoch 41/200
311/311 - 12s - loss: 0.9277 - acc: 0.6891 - val_loss: 1.7651 - val_acc: 0.6742
Epoch 42/200
311/311 - 12s - loss: 0.9248 - acc: 0.6889 - val_loss: 1.1935 - val_acc: 0.6980
Epoch 43/200
311/311 - 12s - loss: 0.9186 - acc: 0.6894 - val_loss: 1.3283 - val_acc: 0.6927
Epoch 44/200
311/311 - 11s - loss: 0.9097 - acc: 0.6907 - val_loss: 1.2041 - val_acc: 0.6980
Epoch 45/200
311/311 - 12s - loss: 0.9081 - acc: 0.6900 - val_loss: 1.3352 - val_acc: 0.6936
Epoch 46/200
311/311 - 12s - loss: 0.9054 - acc: 0.6911 - val_loss: 1.4307 - val_acc: 0.6908
Epoch 47/200
311/311 - 12s - loss: 0.9118 - acc: 0.6900 - val_loss: 2.4493 - val_acc: 0.6551
Epoch 48/200
311/311 - 12s - loss: 0.9119 - acc: 0.6915 - val_loss: 1.6261 - val_acc: 0.6760
Epoch 49/200
311/311 - 12s - loss: 0.9008 - acc: 0.6921 - val_loss: 1.6876 - val_acc: 0.6798
Epoch 50/200
311/311 - 12s - loss: 0.8913 - acc: 0.6928 - val_loss: 1.4187 - val_acc: 0.6953
Epoch 51/200
311/311 - 12s - loss: 0.9071 - acc: 0.6938 - val_loss: 1.2610 - val_acc: 0.6966
Epoch 52/200
311/311 - 12s - loss: 0.9093 - acc: 0.6940 - val_loss: 0.9658 - val_acc: 0.7107
Epoch 53/200
311/311 - 12s - loss: 0.9007 - acc: 0.6950 - val_loss: 1.4755 - val_acc: 0.6959
Epoch 54/200
311/311 - 12s - loss: 0.8920 - acc: 0.6970 - val_loss: 1.0358 - val_acc: 0.7005
Epoch 55/200
311/311 - 12s - loss: 0.8739 - acc: 0.6996 - val_loss: 1.3658 - val_acc: 0.5672
Epoch 56/200
311/311 - 12s - loss: 0.8995 - acc: 0.6977 - val_loss: 1.2076 - val_acc: 0.6954
Epoch 57/200
311/311 - 12s - loss: 0.8799 - acc: 0.7014 - val_loss: 1.1502 - val_acc: 0.6973
Epoch 58/200
311/311 - 12s - loss: 0.8685 - acc: 0.7055 - val_loss: 1.2725 - val_acc: 0.6965
Epoch 59/200
311/311 - 12s - loss: 0.8605 - acc: 0.7069 - val_loss: 1.0566 - val_acc: 0.7005
Epoch 60/200
311/311 - 12s - loss: 0.8527 - acc: 0.7097 - val_loss: 1.2487 - val_acc: 0.6918
Epoch 61/200
311/311 - 12s - loss: 0.8649 - acc: 0.7111 - val_loss: 1.0749 - val_acc: 0.6789
Epoch 62/200
311/311 - 12s - loss: 0.8827 - acc: 0.7106 - val_loss: 1.2538 - val_acc: 0.7035
Epoch 63/200
311/311 - 12s - loss: 0.8968 - acc: 0.7062 - val_loss: 0.9588 - val_acc: 0.7105
Epoch 64/200
311/311 - 12s - loss: 0.8908 - acc: 0.7064 - val_loss: 1.3976 - val_acc: 0.7025
Epoch 65/200
311/311 - 12s - loss: 0.9778 - acc: 0.6921 - val_loss: 1.4971 - val_acc: 0.5772
Epoch 66/200
311/311 - 12s - loss: 1.0114 - acc: 0.6687 - val_loss: 1.2788 - val_acc: 0.6018
Epoch 67/200
311/311 - 12s - loss: 0.9381 - acc: 0.6961 - val_loss: 1.0635 - val_acc: 0.6662
Epoch 68/200
311/311 - 12s - loss: 0.8966 - acc: 0.7050 - val_loss: 1.0643 - val_acc: 0.6595
Epoch 69/200
311/311 - 12s - loss: 0.8623 - acc: 0.7131 - val_loss: 0.9698 - val_acc: 0.6839
Epoch 70/200
311/311 - 12s - loss: 0.8454 - acc: 0.7174 - val_loss: 0.9631 - val_acc: 0.6881
Epoch 71/200
311/311 - 12s - loss: 0.8465 - acc: 0.7202 - val_loss: 0.9499 - val_acc: 0.6957
Epoch 72/200
311/311 - 12s - loss: 0.8429 - acc: 0.7214 - val_loss: 0.8893 - val_acc: 0.7117
Epoch 73/200
311/311 - 12s - loss: 0.8318 - acc: 0.7232 - val_loss: 0.8832 - val_acc: 0.7131
Epoch 74/200
311/311 - 12s - loss: 0.8236 - acc: 0.7244 - val_loss: 0.8776 - val_acc: 0.7155
Epoch 75/200
311/311 - 12s - loss: 0.8175 - acc: 0.7270 - val_loss: 0.9213 - val_acc: 0.7076
Epoch 76/200
311/311 - 12s - loss: 0.8132 - acc: 0.7288 - val_loss: 0.8944 - val_acc: 0.7209
Epoch 77/200
311/311 - 12s - loss: 0.8095 - acc: 0.7310 - val_loss: 0.9352 - val_acc: 0.7147
Epoch 78/200
311/311 - 12s - loss: 0.8093 - acc: 0.7324 - val_loss: 1.0411 - val_acc: 0.7156
Epoch 79/200
311/311 - 12s - loss: 0.8031 - acc: 0.7335 - val_loss: 0.9020 - val_acc: 0.7225
Epoch 80/200
311/311 - 12s - loss: 0.8360 - acc: 0.7219 - val_loss: 0.9460 - val_acc: 0.7202
Epoch 81/200
311/311 - 12s - loss: 0.8129 - acc: 0.7327 - val_loss: 1.1476 - val_acc: 0.7171
Epoch 82/200
311/311 - 12s - loss: 0.8047 - acc: 0.7341 - val_loss: 1.0890 - val_acc: 0.7176
Epoch 83/200
311/311 - 12s - loss: 0.7964 - acc: 0.7379 - val_loss: 1.0718 - val_acc: 0.7153
Epoch 84/200
311/311 - 12s - loss: 0.7921 - acc: 0.7381 - val_loss: 0.9918 - val_acc: 0.7235
Epoch 85/200
311/311 - 12s - loss: 0.7826 - acc: 0.7426 - val_loss: 1.3270 - val_acc: 0.6860
Epoch 86/200
311/311 - 12s - loss: 0.8086 - acc: 0.7374 - val_loss: 1.1058 - val_acc: 0.7116
Epoch 87/200
311/311 - 12s - loss: 0.8593 - acc: 0.7261 - val_loss: 1.2921 - val_acc: 0.6926
Epoch 88/200
311/311 - 12s - loss: 0.8283 - acc: 0.7385 - val_loss: 1.0117 - val_acc: 0.7324
Epoch 89/200
311/311 - 12s - loss: 0.8034 - acc: 0.7416 - val_loss: 0.9788 - val_acc: 0.7300
Epoch 90/200
311/311 - 12s - loss: 0.7840 - acc: 0.7482 - val_loss: 0.9865 - val_acc: 0.7117
Epoch 91/200
311/311 - 12s - loss: 0.7895 - acc: 0.7472 - val_loss: 1.1480 - val_acc: 0.7065
Epoch 92/200
311/311 - 12s - loss: 0.7813 - acc: 0.7480 - val_loss: 0.9858 - val_acc: 0.7210
Epoch 93/200
311/311 - 12s - loss: 0.7676 - acc: 0.7531 - val_loss: 0.9295 - val_acc: 0.7251
Epoch 94/200
311/311 - 12s - loss: 0.7650 - acc: 0.7548 - val_loss: 0.9403 - val_acc: 0.7280
Epoch 95/200
311/311 - 12s - loss: 0.7585 - acc: 0.7577 - val_loss: 0.8564 - val_acc: 0.7407
Epoch 96/200
311/311 - 12s - loss: 0.7666 - acc: 0.7568 - val_loss: 0.9676 - val_acc: 0.7302
Epoch 97/200
311/311 - 12s - loss: 0.7568 - acc: 0.7587 - val_loss: 0.8819 - val_acc: 0.7435
Epoch 98/200
311/311 - 12s - loss: 0.7593 - acc: 0.7606 - val_loss: 0.8237 - val_acc: 0.7488
Epoch 99/200
311/311 - 12s - loss: 0.7499 - acc: 0.7667 - val_loss: 0.8341 - val_acc: 0.7424
Epoch 100/200
311/311 - 12s - loss: 0.7426 - acc: 0.7690 - val_loss: 0.9278 - val_acc: 0.7443
Epoch 101/200
311/311 - 12s - loss: 0.7410 - acc: 0.7701 - val_loss: 0.9826 - val_acc: 0.7283
Epoch 102/200
311/311 - 12s - loss: 0.7462 - acc: 0.7686 - val_loss: 0.8471 - val_acc: 0.7556
Epoch 103/200
311/311 - 12s - loss: 0.7469 - acc: 0.7756 - val_loss: 0.8296 - val_acc: 0.7596
Epoch 104/200
311/311 - 12s - loss: 0.7359 - acc: 0.7769 - val_loss: 0.8261 - val_acc: 0.7544
Epoch 105/200
311/311 - 12s - loss: 0.7351 - acc: 0.7778 - val_loss: 0.8955 - val_acc: 0.7571
Epoch 106/200
311/311 - 12s - loss: 0.7388 - acc: 0.7793 - val_loss: 0.8406 - val_acc: 0.7574
Epoch 107/200
311/311 - 12s - loss: 0.7300 - acc: 0.7832 - val_loss: 0.7602 - val_acc: 0.7776
Epoch 108/200
311/311 - 12s - loss: 0.7243 - acc: 0.7872 - val_loss: 0.7708 - val_acc: 0.7790
Epoch 109/200
311/311 - 12s - loss: 0.7214 - acc: 0.7896 - val_loss: 0.7723 - val_acc: 0.7829
Epoch 110/200
311/311 - 12s - loss: 0.7186 - acc: 0.7913 - val_loss: 0.8009 - val_acc: 0.7767
Epoch 111/200
311/311 - 12s - loss: 0.7163 - acc: 0.7918 - val_loss: 0.8166 - val_acc: 0.7731
Epoch 112/200
311/311 - 12s - loss: 0.7157 - acc: 0.7913 - val_loss: 0.7582 - val_acc: 0.7843
Epoch 113/200
311/311 - 12s - loss: 0.7062 - acc: 0.7959 - val_loss: 0.8105 - val_acc: 0.7788
Epoch 114/200
311/311 - 12s - loss: 0.7090 - acc: 0.7948 - val_loss: 0.7665 - val_acc: 0.7867
Epoch 115/200
311/311 - 12s - loss: 0.7149 - acc: 0.7919 - val_loss: 1.0239 - val_acc: 0.7608
Epoch 116/200
311/311 - 12s - loss: 0.7135 - acc: 0.7900 - val_loss: 0.7224 - val_acc: 0.7955
Epoch 117/200
311/311 - 12s - loss: 0.7038 - acc: 0.7975 - val_loss: 0.8824 - val_acc: 0.7432
Epoch 118/200
311/311 - 12s - loss: 0.7890 - acc: 0.7654 - val_loss: 1.0489 - val_acc: 0.7460
Epoch 119/200
311/311 - 12s - loss: 0.7568 - acc: 0.7698 - val_loss: 0.9323 - val_acc: 0.7586
Epoch 120/200
311/311 - 12s - loss: 0.7395 - acc: 0.7850 - val_loss: 0.7872 - val_acc: 0.7779
Epoch 121/200
311/311 - 12s - loss: 0.7310 - acc: 0.7926 - val_loss: 0.8438 - val_acc: 0.7756
Epoch 122/200
311/311 - 12s - loss: 0.7179 - acc: 0.7949 - val_loss: 0.7955 - val_acc: 0.7783
Epoch 123/200
311/311 - 12s - loss: 0.7044 - acc: 0.8009 - val_loss: 0.7597 - val_acc: 0.7860
Epoch 124/200
311/311 - 12s - loss: 0.6942 - acc: 0.8055 - val_loss: 0.7544 - val_acc: 0.7890
Epoch 125/200
311/311 - 12s - loss: 0.6851 - acc: 0.8065 - val_loss: 0.7974 - val_acc: 0.7850
Epoch 126/200
311/311 - 12s - loss: 0.6877 - acc: 0.8065 - val_loss: 0.7290 - val_acc: 0.7964
Epoch 127/200
311/311 - 12s - loss: 0.6848 - acc: 0.8071 - val_loss: 0.7319 - val_acc: 0.7962
Epoch 128/200
311/311 - 12s - loss: 0.6808 - acc: 0.8091 - val_loss: 0.7844 - val_acc: 0.7922
Epoch 129/200
311/311 - 12s - loss: 0.6785 - acc: 0.8106 - val_loss: 0.7591 - val_acc: 0.7946
Epoch 130/200
311/311 - 12s - loss: 0.6764 - acc: 0.8103 - val_loss: 0.7410 - val_acc: 0.7972
Epoch 131/200
311/311 - 12s - loss: 0.6697 - acc: 0.8128 - val_loss: 0.7862 - val_acc: 0.7882
Epoch 132/200
311/311 - 12s - loss: 0.6715 - acc: 0.8121 - val_loss: 0.7631 - val_acc: 0.7967
Epoch 133/200
311/311 - 12s - loss: 0.6642 - acc: 0.8159 - val_loss: 0.8712 - val_acc: 0.7841
Epoch 134/200
311/311 - 12s - loss: 0.6647 - acc: 0.8148 - val_loss: 0.7020 - val_acc: 0.8072
Epoch 135/200
311/311 - 12s - loss: 0.6568 - acc: 0.8172 - val_loss: 0.7402 - val_acc: 0.7982
Epoch 136/200
311/311 - 12s - loss: 0.6558 - acc: 0.8170 - val_loss: 0.7316 - val_acc: 0.8008
Epoch 137/200
311/311 - 12s - loss: 0.6521 - acc: 0.8186 - val_loss: 0.7086 - val_acc: 0.8054
Epoch 138/200
311/311 - 12s - loss: 0.6615 - acc: 0.8180 - val_loss: 0.7543 - val_acc: 0.7935
Epoch 139/200
311/311 - 12s - loss: 0.6518 - acc: 0.8197 - val_loss: 0.6948 - val_acc: 0.8063
Epoch 140/200
311/311 - 12s - loss: 0.6795 - acc: 0.8134 - val_loss: 0.7183 - val_acc: 0.8017
Epoch 141/200
311/311 - 12s - loss: 0.6872 - acc: 0.8113 - val_loss: 0.7487 - val_acc: 0.7832
Epoch 142/200
311/311 - 12s - loss: 0.6640 - acc: 0.8164 - val_loss: 0.6810 - val_acc: 0.8069
Epoch 143/200
311/311 - 12s - loss: 0.6547 - acc: 0.8173 - val_loss: 0.6718 - val_acc: 0.8100
Epoch 144/200
311/311 - 12s - loss: 0.6468 - acc: 0.8194 - val_loss: 0.6539 - val_acc: 0.8154
Epoch 145/200
311/311 - 11s - loss: 0.6382 - acc: 0.8224 - val_loss: 0.7025 - val_acc: 0.8081
Epoch 146/200
311/311 - 12s - loss: 0.6338 - acc: 0.8238 - val_loss: 0.6820 - val_acc: 0.8122
Epoch 147/200
311/311 - 12s - loss: 0.6330 - acc: 0.8241 - val_loss: 0.6962 - val_acc: 0.8086
Epoch 148/200
311/311 - 12s - loss: 0.6356 - acc: 0.8219 - val_loss: 0.6724 - val_acc: 0.8141
Epoch 149/200
311/311 - 12s - loss: 0.6272 - acc: 0.8260 - val_loss: 0.7138 - val_acc: 0.8063
Epoch 150/200
311/311 - 12s - loss: 0.6280 - acc: 0.8249 - val_loss: 0.7052 - val_acc: 0.8074
Epoch 151/200
311/311 - 12s - loss: 0.6238 - acc: 0.8262 - val_loss: 0.7163 - val_acc: 0.8064
Epoch 152/200
311/311 - 12s - loss: 0.6218 - acc: 0.8269 - val_loss: 0.7676 - val_acc: 0.8046
Epoch 153/200
311/311 - 12s - loss: 0.6171 - acc: 0.8283 - val_loss: 0.7009 - val_acc: 0.8119
Epoch 154/200
311/311 - 12s - loss: 0.6222 - acc: 0.8265 - val_loss: 0.7532 - val_acc: 0.8033
Epoch 155/200
311/311 - 12s - loss: 0.6197 - acc: 0.8280 - val_loss: 0.7047 - val_acc: 0.8091
Epoch 156/200
311/311 - 12s - loss: 0.6144 - acc: 0.8295 - val_loss: 0.7139 - val_acc: 0.8066
Epoch 157/200
311/311 - 12s - loss: 0.6155 - acc: 0.8277 - val_loss: 0.7180 - val_acc: 0.8073
Epoch 158/200
311/311 - 12s - loss: 0.6217 - acc: 0.8274 - val_loss: 0.6974 - val_acc: 0.8083
Epoch 159/200
311/311 - 12s - loss: 0.6131 - acc: 0.8302 - val_loss: 0.6997 - val_acc: 0.8083
Epoch 160/200
311/311 - 12s - loss: 0.6065 - acc: 0.8316 - val_loss: 0.7773 - val_acc: 0.7939
Epoch 161/200
311/311 - 12s - loss: 0.6142 - acc: 0.8288 - val_loss: 0.6565 - val_acc: 0.8171
Epoch 162/200
311/311 - 12s - loss: 0.6146 - acc: 0.8298 - val_loss: 0.7454 - val_acc: 0.7990
Epoch 163/200
311/311 - 12s - loss: 0.6066 - acc: 0.8310 - val_loss: 0.7114 - val_acc: 0.8058
Epoch 164/200
311/311 - 12s - loss: 0.6044 - acc: 0.8330 - val_loss: 0.6737 - val_acc: 0.8134
Epoch 165/200
311/311 - 12s - loss: 0.6035 - acc: 0.8308 - val_loss: 0.6601 - val_acc: 0.8171
Epoch 166/200
311/311 - 12s - loss: 0.5944 - acc: 0.8344 - val_loss: 0.6797 - val_acc: 0.8139
Epoch 167/200
311/311 - 12s - loss: 0.5945 - acc: 0.8347 - val_loss: 0.6553 - val_acc: 0.8184
Epoch 168/200
311/311 - 12s - loss: 0.5934 - acc: 0.8343 - val_loss: 0.6761 - val_acc: 0.8157
Epoch 169/200
311/311 - 12s - loss: 0.5882 - acc: 0.8357 - val_loss: 0.7671 - val_acc: 0.8086
Epoch 170/200
311/311 - 12s - loss: 0.5885 - acc: 0.8362 - val_loss: 0.6415 - val_acc: 0.8226
Epoch 171/200
311/311 - 12s - loss: 0.5902 - acc: 0.8348 - val_loss: 0.7413 - val_acc: 0.8106
Epoch 172/200
311/311 - 12s - loss: 0.6028 - acc: 0.8330 - val_loss: 0.7507 - val_acc: 0.8090
Epoch 173/200
311/311 - 12s - loss: 0.5961 - acc: 0.8344 - val_loss: 0.6445 - val_acc: 0.8233
Epoch 174/200
311/311 - 12s - loss: 0.5885 - acc: 0.8358 - val_loss: 0.6215 - val_acc: 0.8272
Epoch 175/200
311/311 - 12s - loss: 0.5809 - acc: 0.8377 - val_loss: 0.6138 - val_acc: 0.8286
Epoch 176/200
311/311 - 12s - loss: 0.5804 - acc: 0.8376 - val_loss: 0.6682 - val_acc: 0.8196
Epoch 177/200
311/311 - 12s - loss: 0.5893 - acc: 0.8367 - val_loss: 0.9157 - val_acc: 0.8036
Epoch 178/200
311/311 - 12s - loss: 0.5976 - acc: 0.8333 - val_loss: 0.6634 - val_acc: 0.8165
Epoch 179/200
311/311 - 12s - loss: 0.6017 - acc: 0.8303 - val_loss: 0.6393 - val_acc: 0.8231
Epoch 180/200
311/311 - 12s - loss: 0.5919 - acc: 0.8343 - val_loss: 0.7341 - val_acc: 0.8087
Epoch 181/200
311/311 - 12s - loss: 0.5840 - acc: 0.8369 - val_loss: 0.6540 - val_acc: 0.8208
Epoch 182/200
311/311 - 12s - loss: 0.5778 - acc: 0.8389 - val_loss: 0.6768 - val_acc: 0.8213
Epoch 183/200
311/311 - 12s - loss: 0.5804 - acc: 0.8372 - val_loss: 0.6527 - val_acc: 0.8205
Epoch 184/200
311/311 - 12s - loss: 0.5758 - acc: 0.8394 - val_loss: 0.6724 - val_acc: 0.8168
Epoch 185/200
311/311 - 12s - loss: 0.5948 - acc: 0.8329 - val_loss: 0.8744 - val_acc: 0.7898
Epoch 186/200
311/311 - 12s - loss: 0.5927 - acc: 0.8319 - val_loss: 0.9798 - val_acc: 0.7851
Epoch 187/200
311/311 - 12s - loss: 0.5885 - acc: 0.8344 - val_loss: 0.7005 - val_acc: 0.8121
Epoch 188/200
311/311 - 12s - loss: 0.5765 - acc: 0.8380 - val_loss: 0.6453 - val_acc: 0.8190
Epoch 189/200
311/311 - 12s - loss: 0.5719 - acc: 0.8401 - val_loss: 0.6305 - val_acc: 0.8234
Epoch 190/200
311/311 - 12s - loss: 0.5705 - acc: 0.8397 - val_loss: 0.6016 - val_acc: 0.8297
Epoch 191/200
311/311 - 12s - loss: 0.5759 - acc: 0.8385 - val_loss: 0.6450 - val_acc: 0.8228
Epoch 192/200
311/311 - 12s - loss: 0.5786 - acc: 0.8367 - val_loss: 0.6246 - val_acc: 0.8252
Epoch 193/200
311/311 - 12s - loss: 0.5677 - acc: 0.8409 - val_loss: 0.6816 - val_acc: 0.8174
Epoch 194/200
311/311 - 12s - loss: 0.5696 - acc: 0.8395 - val_loss: 0.6615 - val_acc: 0.8197
Epoch 195/200
311/311 - 12s - loss: 0.5688 - acc: 0.8406 - val_loss: 0.6423 - val_acc: 0.8210
Epoch 196/200
311/311 - 12s - loss: 0.5689 - acc: 0.8409 - val_loss: 0.6509 - val_acc: 0.8214
Epoch 197/200
311/311 - 12s - loss: 0.5664 - acc: 0.8417 - val_loss: 0.6317 - val_acc: 0.8253
Epoch 198/200
311/311 - 12s - loss: 0.5691 - acc: 0.8401 - val_loss: 0.6113 - val_acc: 0.8291
Epoch 199/200
311/311 - 12s - loss: 0.5646 - acc: 0.8414 - val_loss: 0.6319 - val_acc: 0.8257
Epoch 200/200
311/311 - 12s - loss: 0.5668 - acc: 0.8401 - val_loss: 0.6347 - val_acc: 0.8262


Elapsed time for Keras training (s):  2331.947407



End of UNET training


##################################################################################
Step2b: MAKING PREDICTIONS
##################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  1


validation data (X) (Y) shapes: (56, 224, 224, 3) (56, 224, 224, 12)
testing    data (X) (Y) shapes (101, 224, 224, 3) (101, 224, 224, 12)



now computing IoU over testing data set:
class ( 0)          Sky: #TP= 437505, #FP=  18686, #FN=  18468, IoU=0.922
class ( 1)         Wall: #TP=1029543, #FP= 250302, #FN= 274403, IoU=0.662
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1425027, #FP= 108148, #FN=  49992, IoU=0.900
class ( 4)     Sidewalk: #TP= 344111, #FP=  86091, #FN= 104322, IoU=0.644
class ( 5)   Vegetation: #TP= 796488, #FP= 322651, #FN=  30057, IoU=0.693
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  64731, #FP=   6462, #FN=  29329, IoU=0.644
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  25425, #FP= 152606, #FN=  44323, IoU=0.114
_________________
Mean IoU: 0.382

now computing IoU over validation data set:
class ( 0)          Sky: #TP= 473064, #FP=  27260, #FN=  14711, IoU=0.919
class ( 1)         Wall: #TP= 562942, #FP= 105869, #FN= 127624, IoU=0.707
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  30995, IoU=0.000
class ( 3)         Road: #TP= 858870, #FP=  64577, #FN=  28125, IoU=0.903
class ( 4)     Sidewalk: #TP=  82203, #FP=  32656, #FN=  58524, IoU=0.474
class ( 5)   Vegetation: #TP= 201484, #FP= 198400, #FN=  21400, IoU=0.478
class ( 6)         Sign: #TP=     23, #FP=      2, #FN=  30093, IoU=0.001
class ( 7)        Fence: #TP=      0, #FP=      0, #FN=  44657, IoU=0.000
class ( 8)      vehicle: #TP=  82553, #FP=   6021, #FN=  65846, IoU=0.535
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  20031, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN=  10125, IoU=0.000
class (11)  miscellanea: #TP=  37261, #FP=  76671, #FN=  59325, IoU=0.215
_________________
Mean IoU: 0.353
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  2


validation data (X) (Y) shapes: (56, 224, 224, 3) (56, 224, 224, 12)
testing    data (X) (Y) shapes (101, 224, 224, 3) (101, 224, 224, 12)



now computing IoU over testing data set:
class ( 0)          Sky: #TP= 440542, #FP=  18524, #FN=  15431, IoU=0.928
class ( 1)         Wall: #TP=1209914, #FP= 392549, #FN=  94032, IoU=0.713
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1410124, #FP=  56226, #FN=  64895, IoU=0.921
class ( 4)     Sidewalk: #TP= 380314, #FP=  77957, #FN=  68119, IoU=0.722
class ( 5)   Vegetation: #TP= 738583, #FP=  94742, #FN=  87962, IoU=0.802
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  74339, #FP=  84628, #FN=  19721, IoU=0.416
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  21498, #FP=  67836, #FN=  48250, IoU=0.156
_________________
Mean IoU: 0.388

now computing IoU over validation data set:
class ( 0)          Sky: #TP= 461050, #FP=  20336, #FN=  26725, IoU=0.907
class ( 1)         Wall: #TP= 640355, #FP= 176543, #FN=  50211, IoU=0.738
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  30995, IoU=0.000
class ( 3)         Road: #TP= 853462, #FP=  34984, #FN=  33533, IoU=0.926
class ( 4)     Sidewalk: #TP=  97642, #FP=  41808, #FN=  43085, IoU=0.535
class ( 5)   Vegetation: #TP= 184537, #FP= 107111, #FN=  38347, IoU=0.559
class ( 6)         Sign: #TP=      2, #FP=      0, #FN=  30114, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN=  44657, IoU=0.000
class ( 8)      vehicle: #TP= 103028, #FP=  27444, #FN=  45371, IoU=0.586
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  20031, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN=  10125, IoU=0.000
class (11)  miscellanea: #TP=  26762, #FP=  34792, #FN=  69824, IoU=0.204
_________________
Mean IoU: 0.371
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
UNET MODEL =  3


validation data (X) (Y) shapes: (56, 224, 224, 3) (56, 224, 224, 12)
testing    data (X) (Y) shapes (101, 224, 224, 3) (101, 224, 224, 12)



now computing IoU over testing data set:
class ( 0)          Sky: #TP= 449175, #FP=  32227, #FN=   6798, IoU=0.920
class ( 1)         Wall: #TP=1176967, #FP= 322223, #FN= 126979, IoU=0.724
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1432592, #FP=  91626, #FN=  42427, IoU=0.914
class ( 4)     Sidewalk: #TP= 367778, #FP=  90986, #FN=  80655, IoU=0.682
class ( 5)   Vegetation: #TP= 775904, #FP= 132471, #FN=  50641, IoU=0.809
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  79413, #FP= 110281, #FN=  14647, IoU=0.389
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=   2130, #FP=   4003, #FN=  67618, IoU=0.029
_________________
Mean IoU: 0.372

now computing IoU over validation data set:
class ( 0)          Sky: #TP= 467471, #FP=  31221, #FN=  20304, IoU=0.901
class ( 1)         Wall: #TP= 633159, #FP= 172852, #FN=  57407, IoU=0.733
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  30995, IoU=0.000
class ( 3)         Road: #TP= 859379, #FP=  60210, #FN=  27616, IoU=0.907
class ( 4)     Sidewalk: #TP=  82209, #FP=  55260, #FN=  58518, IoU=0.419
class ( 5)   Vegetation: #TP= 177419, #FP= 104025, #FN=  45465, IoU=0.543
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  30116, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN=  44657, IoU=0.000
class ( 8)      vehicle: #TP=  99942, #FP=  62062, #FN=  48457, IoU=0.475
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  20031, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN=  10125, IoU=0.000
class (11)  miscellanea: #TP=   1953, #FP=   2694, #FN=  94633, IoU=0.020
_________________
Mean IoU: 0.333

#######################################################################################
Step3: KERAS to TENSORFLOW GRAPH CONVERSION
#######################################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From Keras2TF.py:96: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
model name =  unet1

 TF input node name:
[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'conv2d_18/Relu:0' shape=(?, 224, 224, 12) dtype=float32>]

FINISHED CREATING TF FILES

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From Keras2TF.py:96: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
model name =  unet2

 TF input node name:
[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'conv2d_22/Relu:0' shape=(?, 224, 224, 12) dtype=float32>]

FINISHED CREATING TF FILES

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From Keras2TF.py:96: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
model name =  unet3

 TF input node name:
[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'conv2d_18/Relu:0' shape=(?, 224, 224, 12) dtype=float32>]

FINISHED CREATING TF FILES


##############################################################################
Step4a: FREEZE TF GRAPHS
##############################################################################

WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0104 16:07:01.763715 140316925888320 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2021-01-04 16:07:01.767170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-01-04 16:07:01.788288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:01.788487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:01.789422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:01.790440: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:01.790685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:01.792087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:01.793366: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:01.796387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:01.798115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:01.798412: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-01-04 16:07:01.806897: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496220000 Hz
2021-01-04 16:07:01.807875: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c0846d590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:01.807888: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-04 16:07:01.876830: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c06e64e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:01.876862: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2021-01-04 16:07:01.878717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:01.878772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:01.878788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:01.878801: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:01.878815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:01.878828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:01.878840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:01.878854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:01.882047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:01.882094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:01.884624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-04 16:07:01.884639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2021-01-04 16:07:01.884647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2021-01-04 16:07:01.888644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22364 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/unet1/float_model.ckpt
I0104 16:07:02.255390 140316925888320 saver.py:1284] Restoring parameters from ./build/tf_chkpts/unet1/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0104 16:07:03.565593 140316925888320 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0104 16:07:03.565916 140316925888320 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 110 variables.
I0104 16:07:03.707628 140316925888320 graph_util_impl.py:334] Froze 110 variables.
INFO:tensorflow:Converted 110 variables to const ops.
I0104 16:07:03.922630 140316925888320 graph_util_impl.py:394] Converted 110 variables to const ops.
Loaded meta graph file './build/tf_chkpts/unet1/float_model.ckpt.meta
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0104 16:07:06.301819 139716267439936 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2021-01-04 16:07:06.305661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-01-04 16:07:06.344511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:06.344702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:06.345617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:06.346533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:06.346769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:06.347994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:06.348934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:06.351876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:06.353610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:06.353907: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-01-04 16:07:06.362382: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496220000 Hz
2021-01-04 16:07:06.363428: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563c25217b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:06.363439: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-04 16:07:06.431516: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563c23c0faf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:06.431547: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2021-01-04 16:07:06.433458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:06.433516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:06.433534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:06.433549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:06.433563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:06.433578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:06.433592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:06.433608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:06.436818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:06.436860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:06.439298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-04 16:07:06.439313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2021-01-04 16:07:06.439319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2021-01-04 16:07:06.443671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22364 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/unet2/float_model.ckpt
I0104 16:07:06.826378 139716267439936 saver.py:1284] Restoring parameters from ./build/tf_chkpts/unet2/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0104 16:07:08.161268 139716267439936 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0104 16:07:08.161571 139716267439936 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 118 variables.
I0104 16:07:08.310262 139716267439936 graph_util_impl.py:334] Froze 118 variables.
INFO:tensorflow:Converted 118 variables to const ops.
I0104 16:07:08.480669 139716267439936 graph_util_impl.py:394] Converted 118 variables to const ops.
Loaded meta graph file './build/tf_chkpts/unet2/float_model.ckpt.meta
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0104 16:07:10.895467 140694807783232 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2021-01-04 16:07:10.899021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-01-04 16:07:10.919756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:10.919964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:10.920855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:10.921846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:10.922103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:10.923490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:10.924509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:10.927452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:10.929165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:10.929479: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2021-01-04 16:07:10.937814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496220000 Hz
2021-01-04 16:07:10.938810: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d9a68d1b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:10.938822: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-01-04 16:07:11.010191: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562d99094620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-04 16:07:11.010222: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2021-01-04 16:07:11.012131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2021-01-04 16:07:11.012188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:11.012203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-01-04 16:07:11.012216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-01-04 16:07:11.012229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-01-04 16:07:11.012242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-01-04 16:07:11.012255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-01-04 16:07:11.012268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-01-04 16:07:11.015533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2021-01-04 16:07:11.015592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-01-04 16:07:11.018160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-04 16:07:11.018177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0
2021-01-04 16:07:11.018184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N
2021-01-04 16:07:11.022576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22364 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/unet3/float_model.ckpt
I0104 16:07:11.416697 140694807783232 saver.py:1284] Restoring parameters from ./build/tf_chkpts/unet3/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0104 16:07:12.761447 140694807783232 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0104 16:07:12.761764 140694807783232 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 118 variables.
I0104 16:07:12.913739 140694807783232 graph_util_impl.py:334] Froze 118 variables.
INFO:tensorflow:Converted 118 variables to const ops.
I0104 16:07:13.101655 140694807783232 graph_util_impl.py:394] Converted 118 variables to const ops.
Loaded meta graph file './build/tf_chkpts/unet3/float_model.ckpt.meta

##############################################################################
Step4a: INSPECT FROZEN GRAPH
##############################################################################

Op types used: 130 Const, 119 Identity, 19 BiasAdd, 19 Conv2D, 19 Relu, 18 FusedBatchNormV3, 4 ConcatV2, 4 MaxPool, 4 Mul, 4 ResizeBilinear, 4 Shape, 4 StridedSlice, 1 Placeholder

Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3])
Found 1 possible outputs: (name=conv2d_18/Relu, op=Relu)
Op types used: 138 Const, 127 Identity, 23 BiasAdd, 23 Conv2D, 23 Relu, 18 FusedBatchNormV3, 4 ConcatV2, 4 MaxPool, 4 Mul, 4 ResizeBilinear, 4 Shape, 4 StridedSlice, 1 Placeholder

Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3])
Found 1 possible outputs: (name=conv2d_22/Relu, op=Relu)
Op types used: 170 Const, 127 Identity, 23 BiasAdd, 19 Conv2D, 19 Relu, 18 FusedBatchNormV3, 12 StridedSlice, 8 Mul, 4 ConcatV2, 4 Conv2DBackpropInput, 4 MaxPool, 4 Pack, 4 Shape, 1 Placeholder

Found 1 possible inputs: (name=input_1, type=float(1), shape=[?,224,224,3])
Found 1 possible outputs: (name=conv2d_18/Relu, op=Relu)

##############################################################################
Step4b: EVALUATING THE ORIGINAL GRAPH
##############################################################################

fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 437505, #FP=  18686, #FN=  18468, IoU=0.922
class ( 1)         Wall: #TP=1029544, #FP= 250302, #FN= 274402, IoU=0.662
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1425027, #FP= 108148, #FN=  49992, IoU=0.900
class ( 4)     Sidewalk: #TP= 344111, #FP=  86091, #FN= 104322, IoU=0.644
class ( 5)   Vegetation: #TP= 796488, #FP= 322649, #FN=  30057, IoU=0.693
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  64731, #FP=   6462, #FN=  29329, IoU=0.644
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  25425, #FP= 152607, #FN=  44323, IoU=0.114
_________________
Mean IoU: 0.382
FINISHED!
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 440542, #FP=  18524, #FN=  15431, IoU=0.928
class ( 1)         Wall: #TP=1209914, #FP= 392548, #FN=  94032, IoU=0.713
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1410124, #FP=  56226, #FN=  64895, IoU=0.921
class ( 4)     Sidewalk: #TP= 380314, #FP=  77957, #FN=  68119, IoU=0.722
class ( 5)   Vegetation: #TP= 738584, #FP=  94742, #FN=  87961, IoU=0.802
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  74339, #FP=  84628, #FN=  19721, IoU=0.416
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  21498, #FP=  67836, #FN=  48250, IoU=0.156
_________________
Mean IoU: 0.388
FINISHED!
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 449175, #FP=  32227, #FN=   6798, IoU=0.920
class ( 1)         Wall: #TP=1176967, #FP= 322223, #FN= 126979, IoU=0.724
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1432592, #FP=  91626, #FN=  42427, IoU=0.914
class ( 4)     Sidewalk: #TP= 367778, #FP=  90986, #FN=  80655, IoU=0.682
class ( 5)   Vegetation: #TP= 775904, #FP= 132471, #FN=  50641, IoU=0.809
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  79413, #FP= 110281, #FN=  14647, IoU=0.389
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=   2130, #FP=   4003, #FN=  67618, IoU=0.029
_________________
Mean IoU: 0.372
FINISHED!

##########################################################################
Step5a: QUANTIZATION
##########################################################################

N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--                                                                               31 20% (2 of 10) |#####                    | Elapsed Time: 0:00:06 ETA:   0:00:    30% (3 of 10) |#######                  | Elapsed Time: 0:00:10 ETA:   0:23                                                                            00:20                                                                                 60% (6 of 10) |###############          | Elapsed Time: 0:00:20 ETA:   0:00:13                                                                         10                                                                                80% (8 of 10) |####################     | Elapsed Time: 0:00:27 ETA:            90%                                                                   e:  0:00:34
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
script running on folder  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code/../build/dataset1/img_calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 10 iterations...
INFO: Calibration Done.
INFO: Generating Deploy Model...
INFO: Deploy Model Generated.
********************* Quantization Summary *********************
INFO: Output:
  quantize_eval_model: .././build/quantize_results/unet1/quantize_eval_model.pb
  deploy_model: .././build/quantize_results/unet1/deploy_model.pb
N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--                                                                               31 20% (2 of 10) |#####                    | Elapsed Time: 0:00:06 ETA:   0:00:    30% (3 of 10) |#######                  | Elapsed Time: 0:00:10 ETA:   0:23                                                                            00:20                                                                                 60% (6 of 10) |###############          | Elapsed Time: 0:00:20 ETA:   0:00:13                                                                         10                                                                                80% (8 of 10) |####################     | Elapsed Time: 0:00:27 ETA:            90%                                                                   e:  0:00:34
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
script running on folder  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code/../build/dataset1/img_calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 10 iterations...
INFO: Calibration Done.
INFO: Generating Deploy Model...
INFO: Deploy Model Generated.
********************* Quantization Summary *********************
INFO: Output:
  quantize_eval_model: .././build/quantize_results/unet2/quantize_eval_model.pb
  deploy_model: .././build/quantize_results/unet2/deploy_model.pb
N/A% (0 of 10) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--                                                                               25 20% (2 of 10) |#####                    | Elapsed Time: 0:00:05 ETA:   0:00:    30% (3 of 10) |#######                  | Elapsed Time: 0:00:08 ETA:   0:19                                                                            00:16                                                                                 60% (6 of 10) |###############          | Elapsed Time: 0:00:16 ETA:   0:00:11                                                                         08                                                                                80% (8 of 10) |####################     | Elapsed Time: 0:00:22 ETA:            90%                                                                   e:  0:00:27
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
script running on folder  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code/../build/dataset1/img_calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 10 iterations...
INFO: Calibration Done.
INFO: Generating Deploy Model...
INFO: Deploy Model Generated.
********************* Quantization Summary *********************
INFO: Output:
  quantize_eval_model: .././build/quantize_results/unet3/quantize_eval_model.pb
  deploy_model: .././build/quantize_results/unet3/deploy_model.pb

##############################################################################
Step5b: EVALUATE QUANTIZED GRAPH
##############################################################################

Using TensorFlow backend.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 438604, #FP=  19718, #FN=  17369, IoU=0.922
class ( 1)         Wall: #TP=1003692, #FP= 240681, #FN= 300254, IoU=0.650
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1424558, #FP= 107066, #FN=  50461, IoU=0.900
class ( 4)     Sidewalk: #TP= 343012, #FP=  81966, #FN= 105421, IoU=0.647
class ( 5)   Vegetation: #TP= 799932, #FP= 365105, #FN=  26613, IoU=0.671
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  61900, #FP=   5608, #FN=  32160, IoU=0.621
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  24722, #FP= 151212, #FN=  45026, IoU=0.112
_________________
Mean IoU: 0.377
FINISHED!
Using TensorFlow backend.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 441510, #FP=  19174, #FN=  14463, IoU=0.929
class ( 1)         Wall: #TP=1208685, #FP= 406622, #FN=  95261, IoU=0.707
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1407601, #FP=  55362, #FN=  67418, IoU=0.920
class ( 4)     Sidewalk: #TP= 384434, #FP=  91373, #FN=  63999, IoU=0.712
class ( 5)   Vegetation: #TP= 728930, #FP=  82697, #FN=  97615, IoU=0.802
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  74142, #FP=  83992, #FN=  19918, IoU=0.416
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=  21482, #FP=  61772, #FN=  48266, IoU=0.163
_________________
Mean IoU: 0.387
FINISHED!
Using TensorFlow backend.
fcn_config.py runs from  /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/code
X tensor shape:  (101, 224, 224, 3)  Y tensor shape:  (101, 224, 224, 12)
(101, 224, 224) (101, 224, 224)
class ( 0)          Sky: #TP= 449413, #FP=  32444, #FN=   6560, IoU=0.920
class ( 1)         Wall: #TP=1144229, #FP= 304474, #FN= 159717, IoU=0.711
class ( 2)         Pole: #TP=      0, #FP=      0, #FN=  36420, IoU=0.000
class ( 3)         Road: #TP=1436978, #FP= 123486, #FN=  38041, IoU=0.899
class ( 4)     Sidewalk: #TP= 340877, #FP=  95513, #FN= 107556, IoU=0.627
class ( 5)   Vegetation: #TP= 778360, #FP= 137178, #FN=  48185, IoU=0.808
class ( 6)         Sign: #TP=      0, #FP=      0, #FN=  53392, IoU=0.000
class ( 7)        Fence: #TP=      0, #FP=      0, #FN= 156403, IoU=0.000
class ( 8)      vehicle: #TP=  80292, #FP= 139039, #FN=  13768, IoU=0.344
class ( 9)   Pedestrian: #TP=      0, #FP=      0, #FN=  36865, IoU=0.000
class (10)    Bicyclist: #TP=      0, #FP=      0, #FN= 110972, IoU=0.000
class (11)  miscellanea: #TP=   1985, #FP=   3508, #FN=  67763, IoU=0.027
_________________
Mean IoU: 0.361
FINISHED!

##########################################################################
COMPILE UNET XMODEL FILE WITH Vitis AI for VCK190 TARGET
##########################################################################

bash: --arch: command not found
[INFO] parse raw model     :  0%|          | 0/95 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 95/95 [00:00<00:00, 22551.30it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 2788.69it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1336.67it/s]
[INFO] generate xmodel     :  0%|          | 0/87 [00:00<?, ?it/s][INFO] generate xmodel     : 45%|âââââ     | 39/87 [00:00<00:00, 319.84it/s][INFO] ge : 56%|ââââââ    | 49/87 [00:00<00:00, 97.69it/s]                 xmodel   | 87/87 [00:00<00:00, 202.43it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet1/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet1/unet1_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet1/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet1/unet1_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161102-6476"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 163
[UNILOG][INFO] Begin to compile...
[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_1/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_2/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_3/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[UNILOG][INFO] Total device subgraph number 15, DPU subgraph number 5
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/unet1.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is b98770c1b187835cae8dd476841e7f40, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 22635.32it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4360.92it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1945.04it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 352.54it/s][INFO] ge : 51%|âââââ     | 50/99 [00:00<00:00, 142.08it/s]                 xmodel    | 99/99 [00:00<00:00, 263.45it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161108-6512"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_1/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_2/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_3/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[UNILOG][INFO] Total device subgraph number 15, DPU subgraph number 5
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 57e5e72d8149ab8dcd5bf63e3ddc64bb, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 22414.87it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4323.74it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1911.87it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 348.78it/s][INFO] ge : 51%|âââââ     | 50/99 [00:00<00:00, 141.07it/s]                 xmodel    | 99/99 [00:00<00:00, 259.59it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/dbg_unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/dbg_unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161114-6548"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_1/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_2/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[0;33m[UNILOG][WARNING] xir::Op{name = up_sampling2d_3/resize/ResizeBilinear, type = upsample-fix} has been assigned to CPU: [DPU does not support BILINEAR mode. (only support NEAREST mode)].
[m[UNILOG][INFO] Total device subgraph number 15, DPU subgraph number 5
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/dbg_unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is e80e192a025584885b58569708f2dbb6, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/111 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 111/111 [00:00<00:00, 18919.37it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/123 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 123/123 [00:00<00:00, 4770.93it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1701.47it/s]
[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s][INFO] generate xmodel     : 42%|âââââ     | 35/83 [00:00<00:00, 316.54it/s][INFO] ge : 54%|ââââââ    | 45/83 [00:00<00:00, 96.29it/s]                 xmodel   | 83/83 [00:00<00:00, 194.75it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet3/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet3/unet3_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet3/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet3/unet3_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161121-6584"
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA0_B8192C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 175
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/unet3.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 07f3b6d1505bcf6a12e0ddf17b835169, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE UNET XMODEL FILE WITH Vitis AI for ZCU102 TARGET
##########################################################################

[INFO] parse raw model     :  0%|          | 0/95 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 95/95 [00:00<00:00, 20885.78it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 2723.31it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1308.22it/s]
[INFO] generate xmodel     :  0%|          | 0/87 [00:00<?, ?it/s][INFO] generate xmodel     : 45%|âââââ     | 39/87 [00:00<00:00, 259.59it/s][INFO] ge : 54%|ââââââ    | 47/87 [00:00<00:00, 112.51it/s]                 xmodel    | 54/87 [00:00<00:00, 89.98it/s]                  [INFO] generateâââââââââ:00, 193.60it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet1/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet1/unet1_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet1/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet1/unet1_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161128-6622"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 163
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/unet1.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is e2a4e29fde752571d1691b30d044de6d, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 20814.92it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4294.39it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1893.44it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 355.92it/s][INFO] ge : 51%|âââââ     | 50/99 [00:00<00:00, 143.39it/s]                 xmodel    | 99/99 [00:00<00:00, 262.86it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161147-6658"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is ff924e7ed8f5d1c0a5d32783fce90796, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 22925.55it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4458.05it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1948.10it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 352.12it/s][INFO] ge : 51%|âââââ     | 50/99 [00:00<00:00, 138.88it/s]                 xmodel    | 99/99 [00:00<00:00, 258.01it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/dbg_unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/dbg_unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161205-6694"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/dbg_unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 4c2cd4ec30322819234732357cfaf171, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/111 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 111/111 [00:00<00:00, 19890.11it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/123 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 123/123 [00:00<00:00, 4751.15it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1701.17it/s]
[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s][INFO] generate xmodel     : 42%|âââââ     | 35/83 [00:00<00:00, 324.47it/s][INFO] ge : 54%|ââââââ    | 45/83 [00:00<00:00, 94.09it/s]                 xmodel   | 83/83 [00:00<00:00, 192.26it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet3/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet3/unet3_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet3/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet3/unet3_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161222-6730"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 175
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/unet3.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is bc1a9a3d6b138837339db0b1094b77e6, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************

##########################################################################
COMPILE UNET XMODEL FILE WITH Vitis AI for ZCU104 TARGET
##########################################################################

[INFO] parse raw model     :  0%|          | 0/95 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 95/95 [00:00<00:00, 22234.19it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 2756.42it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1346.19it/s]
[INFO] generate xmodel     :  0%|          | 0/87 [00:00<?, ?it/s][INFO] generate xmodel     : 45%|âââââ     | 39/87 [00:00<00:00, 319.30it/s][INFO] ge : 56%|ââââââ    | 49/87 [00:00<00:00, 96.74it/s]                 xmodel   | 87/87 [00:00<00:00, 203.42it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet1/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet1/unet1_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet1/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet1/unet1_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161241-6768"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 163
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/unet1.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is cc3562bcabe30d57c347b14254f87208, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet1/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 22821.79it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4373.25it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1920.27it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 357.89it/s][INFO] ge : 51%|âââââ     | 50/99 [00:00<00:00, 138.47it/s]                 xmodel    | 99/99 [00:00<00:00, 257.75it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161300-6804"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 4a262c80f3587364b5a880fab0a5b16d, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/107 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 107/107 [00:00<00:00, 22594.30it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/111 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 111/111 [00:00<00:00, 4371.73it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/99 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 99/99 [00:00<00:00, 1929.35it/s]
[INFO] generate xmodel     :  0%|          | 0/99 [00:00<?, ?it/s][INFO] generate xmodel     : 39%|ââââ      | 39/99 [00:00<00:00, 320.83it/s][INFO] ge : 49%|âââââ     | 49/99 [00:00<00:00, 127.39it/s]                 xmodel    | 99/99 [00:00<00:00, 250.17it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet2/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet2/dbg_unet2_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet2/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet2/dbg_unet2_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161318-6840"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 191
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/dbg_unet2.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 67b3a81f867f6f91987c13de80e7f9e8, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet2/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
[INFO] parse raw model     :  0%|          | 0/111 [00:00<?, ?it/s][INFO] parse raw model     :100%|ââââââââââ| 111/111 [00:00<00:00, 19764.30it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/123 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 123/123 [00:00<00:00, 4810.88it/s]
[INFO] infer shape (NHWC)  :  0%|          | 0/87 [00:00<?, ?it/s][INFO] infer shape (NHWC)  :100%|ââââââââââ| 87/87 [00:00<00:00, 1712.92it/s]
[INFO] generate xmodel     :  0%|          | 0/83 [00:00<?, ?it/s][INFO] generate xmodel     : 42%|âââââ     | 35/83 [00:00<00:00, 309.56it/s][INFO] ge : 54%|ââââââ    | 45/83 [00:00<00:00, 93.17it/s]                 xmodel   | 83/83 [00:00<00:00, 189.26it/s]
[INFO] Namespace(inputs_shape=None, layout='NHWC', model_files=['./build/quantize_results/unet3/quantize_eval_model.pb'], model_type='tensorflow', out_filename='./build/compile/unet3/unet3_org.xmodel', proto=None)
[INFO] tensorflow model: build/quantize_results/unet3/quantize_eval_model.pb
[INFO] generate xmodel: /workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/build/compile/unet3/unet3_org.xmodel
[UNILOG][INFO] The compiler log will be dumped at "/tmp/vitis-ai-user/log/xcompiler-20210104-161336-6876"
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 175
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/unet3.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 81fecdf95274a24152270540e2a144d0, and been saved to "/workspace/tutorials/VAI-KERAS-FCN8-SEMSEG/files/./build/compile/unet3/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
./build/dataset1/img_test/
./build/dataset1/img_test/testing_48.png
./build/dataset1/img_test/testing_82.png
./build/dataset1/img_test/testing_60.png
./build/dataset1/img_test/testing_80.png
./build/dataset1/img_test/testing_34.png
./build/dataset1/img_test/testing_41.png
./build/dataset1/img_test/testing_8.png
./build/dataset1/img_test/testing_70.png
./build/dataset1/img_test/testing_3.png
./build/dataset1/img_test/testing_15.png
./build/dataset1/img_test/testing_81.png
./build/dataset1/img_test/testing_10.png
./build/dataset1/img_test/testing_32.png
./build/dataset1/img_test/testing_29.png
./build/dataset1/img_test/testing_61.png
./build/dataset1/img_test/testing_12.png
./build/dataset1/img_test/testing_77.png
./build/dataset1/img_test/testing_93.png
./build/dataset1/img_test/testing_40.png
./build/dataset1/img_test/testing_55.png
./build/dataset1/img_test/testing_100.png
./build/dataset1/img_test/testing_66.png
./build/dataset1/img_test/testing_87.png
./build/dataset1/img_test/testing_6.png
./build/dataset1/img_test/testing_39.png
./build/dataset1/img_test/testing_0.png
./build/dataset1/img_test/testing_53.png
./build/dataset1/img_test/testing_96.png
./build/dataset1/img_test/testing_58.png
./build/dataset1/img_test/testing_38.png
./build/dataset1/img_test/testing_49.png
./build/dataset1/img_test/testing_71.png
./build/dataset1/img_test/testing_86.png
./build/dataset1/img_test/testing_69.png
./build/dataset1/img_test/testing_83.png
./build/dataset1/img_test/testing_92.png
./build/dataset1/img_test/testing_45.png
./build/dataset1/img_test/testing_52.png
./build/dataset1/img_test/testing_97.png
./build/dataset1/img_test/testing_63.png
./build/dataset1/img_test/testing_33.png
./build/dataset1/img_test/testing_30.png
./build/dataset1/img_test/testing_24.png
./build/dataset1/img_test/testing_5.png
./build/dataset1/img_test/testing_90.png
./build/dataset1/img_test/testing_94.png
./build/dataset1/img_test/testing_18.png
./build/dataset1/img_test/testing_23.png
./build/dataset1/img_test/testing_35.png
./build/dataset1/img_test/testing_59.png
./build/dataset1/img_test/testing_20.png
./build/dataset1/img_test/testing_28.png
./build/dataset1/img_test/testing_31.png
./build/dataset1/img_test/testing_16.png
./build/dataset1/img_test/testing_57.png
./build/dataset1/img_test/testing_27.png
./build/dataset1/img_test/testing_50.png
./build/dataset1/img_test/testing_47.png
./build/dataset1/img_test/testing_1.png
./build/dataset1/img_test/testing_65.png
./build/dataset1/img_test/testing_51.png
./build/dataset1/img_test/testing_11.png
./build/dataset1/img_test/testing_76.png
./build/dataset1/img_test/testing_42.png
./build/dataset1/img_test/testing_22.png
./build/dataset1/img_test/testing_98.png
./build/dataset1/img_test/testing_95.png
./build/dataset1/img_test/testing_78.png
./build/dataset1/img_test/testing_19.png
./build/dataset1/img_test/testing_2.png
./build/dataset1/img_test/testing_85.png
./build/dataset1/img_test/testing_46.png
./build/dataset1/img_test/testing_37.png
./build/dataset1/img_test/testing_68.png
./build/dataset1/img_test/testing_99.png
./build/dataset1/img_test/testing_13.png
./build/dataset1/img_test/testing_25.png
./build/dataset1/img_test/testing_36.png
./build/dataset1/img_test/testing_26.png
./build/dataset1/img_test/testing_54.png
./build/dataset1/img_test/testing_56.png
./build/dataset1/img_test/testing_44.png
./build/dataset1/img_test/testing_72.png
./build/dataset1/img_test/testing_7.png
./build/dataset1/img_test/testing_21.png
./build/dataset1/img_test/testing_79.png
./build/dataset1/img_test/testing_73.png
./build/dataset1/img_test/testing_17.png
./build/dataset1/img_test/testing_64.png
./build/dataset1/img_test/testing_75.png
./build/dataset1/img_test/testing_62.png
./build/dataset1/img_test/testing_9.png
./build/dataset1/img_test/testing_14.png
./build/dataset1/img_test/testing_91.png
./build/dataset1/img_test/testing_74.png
./build/dataset1/img_test/testing_89.png
./build/dataset1/img_test/testing_67.png
./build/dataset1/img_test/testing_84.png
./build/dataset1/img_test/testing_4.png
./build/dataset1/img_test/testing_88.png
./build/dataset1/img_test/testing_43.png
./build/dataset1/seg_test/
./build/dataset1/seg_test/seg_tst_36.png
./build/dataset1/seg_test/seg_tst_3.png
./build/dataset1/seg_test/seg_tst_15.png
./build/dataset1/seg_test/seg_tst_14.png
./build/dataset1/seg_test/seg_tst_87.png
./build/dataset1/seg_test/seg_tst_42.png
./build/dataset1/seg_test/seg_tst_16.png
./build/dataset1/seg_test/seg_tst_43.png
./build/dataset1/seg_test/seg_tst_66.png
./build/dataset1/seg_test/seg_tst_71.png
./build/dataset1/seg_test/seg_tst_33.png
./build/dataset1/seg_test/seg_tst_55.png
./build/dataset1/seg_test/seg_tst_92.png
./build/dataset1/seg_test/seg_tst_30.png
./build/dataset1/seg_test/seg_tst_79.png
./build/dataset1/seg_test/seg_tst_65.png
./build/dataset1/seg_test/seg_tst_2.png
./build/dataset1/seg_test/seg_tst_28.png
./build/dataset1/seg_test/seg_tst_84.png
./build/dataset1/seg_test/seg_tst_75.png
./build/dataset1/seg_test/seg_tst_8.png
./build/dataset1/seg_test/seg_tst_23.png
./build/dataset1/seg_test/seg_tst_45.png
./build/dataset1/seg_test/seg_tst_38.png
./build/dataset1/seg_test/seg_tst_59.png
./build/dataset1/seg_test/seg_tst_5.png
./build/dataset1/seg_test/seg_tst_58.png
./build/dataset1/seg_test/seg_tst_39.png
./build/dataset1/seg_test/seg_tst_26.png
./build/dataset1/seg_test/seg_tst_20.png
./build/dataset1/seg_test/seg_tst_82.png
./build/dataset1/seg_test/seg_tst_40.png
./build/dataset1/seg_test/seg_tst_34.png
./build/dataset1/seg_test/seg_tst_49.png
./build/dataset1/seg_test/seg_tst_13.png
./build/dataset1/seg_test/seg_tst_70.png
./build/dataset1/seg_test/seg_tst_18.png
./build/dataset1/seg_test/seg_tst_22.png
./build/dataset1/seg_test/seg_tst_50.png
./build/dataset1/seg_test/seg_tst_77.png
./build/dataset1/seg_test/seg_tst_83.png
./build/dataset1/seg_test/seg_tst_35.png
./build/dataset1/seg_test/seg_tst_93.png
./build/dataset1/seg_test/seg_tst_1.png
./build/dataset1/seg_test/seg_tst_54.png
./build/dataset1/seg_test/seg_tst_99.png
./build/dataset1/seg_test/seg_tst_61.png
./build/dataset1/seg_test/seg_tst_69.png
./build/dataset1/seg_test/seg_tst_7.png
./build/dataset1/seg_test/seg_tst_56.png
./build/dataset1/seg_test/seg_tst_32.png
./build/dataset1/seg_test/seg_tst_24.png
./build/dataset1/seg_test/seg_tst_57.png
./build/dataset1/seg_test/seg_tst_88.png
./build/dataset1/seg_test/seg_tst_60.png
./build/dataset1/seg_test/seg_tst_97.png
./build/dataset1/seg_test/seg_tst_68.png
./build/dataset1/seg_test/seg_tst_12.png
./build/dataset1/seg_test/seg_tst_52.png
./build/dataset1/seg_test/seg_tst_95.png
./build/dataset1/seg_test/seg_tst_21.png
./build/dataset1/seg_test/seg_tst_17.png
./build/dataset1/seg_test/seg_tst_86.png
./build/dataset1/seg_test/seg_tst_27.png
./build/dataset1/seg_test/seg_tst_25.png
./build/dataset1/seg_test/seg_tst_51.png
./build/dataset1/seg_test/seg_tst_37.png
./build/dataset1/seg_test/seg_tst_46.png
./build/dataset1/seg_test/seg_tst_100.png
./build/dataset1/seg_test/seg_tst_48.png
./build/dataset1/seg_test/seg_tst_73.png
./build/dataset1/seg_test/seg_tst_72.png
./build/dataset1/seg_test/seg_tst_91.png
./build/dataset1/seg_test/seg_tst_41.png
./build/dataset1/seg_test/seg_tst_85.png
./build/dataset1/seg_test/seg_tst_67.png
./build/dataset1/seg_test/seg_tst_74.png
./build/dataset1/seg_test/seg_tst_78.png
./build/dataset1/seg_test/seg_tst_96.png
./build/dataset1/seg_test/seg_tst_6.png
./build/dataset1/seg_test/seg_tst_44.png
./build/dataset1/seg_test/seg_tst_4.png
./build/dataset1/seg_test/seg_tst_90.png
./build/dataset1/seg_test/seg_tst_98.png
./build/dataset1/seg_test/seg_tst_62.png
./build/dataset1/seg_test/seg_tst_10.png
./build/dataset1/seg_test/seg_tst_63.png
./build/dataset1/seg_test/seg_tst_9.png
./build/dataset1/seg_test/seg_tst_81.png
./build/dataset1/seg_test/seg_tst_94.png
./build/dataset1/seg_test/seg_tst_19.png
./build/dataset1/seg_test/seg_tst_53.png
./build/dataset1/seg_test/seg_tst_11.png
./build/dataset1/seg_test/seg_tst_76.png
./build/dataset1/seg_test/seg_tst_47.png
./build/dataset1/seg_test/seg_tst_29.png
./build/dataset1/seg_test/seg_tst_0.png
./build/dataset1/seg_test/seg_tst_89.png
./build/dataset1/seg_test/seg_tst_64.png
./build/dataset1/seg_test/seg_tst_31.png
./build/dataset1/seg_test/seg_tst_80.png
cp: -r not specified; omitting directory './build/../target_vck190/'
cp: -r not specified; omitting directory './build/../target_zcu102/'
tar: Removing leading `./build/../' from member names
./build/../target_vck190/
./build/../target_vck190/test.tar.gz
./build/../target_vck190/code/
./build/../target_vck190/code/run_cnn_py_fps.sh
./build/../target_vck190/code/src/
./build/../target_vck190/code/src/main.cc
./build/../target_vck190/code/src/app_mt.py
./build/../target_vck190/code/common/
./build/../target_vck190/code/common/common.cpp
./build/../target_vck190/code/common/common.h
./build/../target_vck190/code/build_app.sh
./build/../target_vck190/fcn8/
./build/../target_vck190/fcn8/model/
./build/../target_vck190/fcn8/model/fcn8.xmodel
./build/../target_vck190/unet/
./build/../target_vck190/unet/v1/
./build/../target_vck190/unet/v1/empty.md
./build/../target_vck190/unet/v3/
./build/../target_vck190/unet/v3/empty.md
./build/../target_vck190/unet/v2/
./build/../target_vck190/unet/v2/model/
./build/../target_vck190/unet/v2/model/unet2.xmodel
./build/../target_vck190/unet/v2/model/dbg_unet2.xmodel
./build/../target_vck190/fcn8ups/
./build/../target_vck190/fcn8ups/model/
./build/../target_vck190/fcn8ups/model/fcn8ups.xmodel
./build/../target_vck190/rpt/
./build/../target_vck190/rpt/logfile_target_vck190.txt
./build/../target_vck190/rpt/empty.md
./build/../target_vck190/rpt/logfile_py_unet2.txt
./build/../target_vck190/rpt/logfile_py_fps.txt
./build/../target_vck190/run_all_target.sh
./build/../target_vck190/target_zcu102/
./build/../target_vck190/target_zcu102/test.tar.gz
./build/../target_vck190/target_zcu102/code/
./build/../target_vck190/target_zcu102/code/build.sh
./build/../target_vck190/target_zcu102/code/run_cnn_py_fps.sh
./build/../target_vck190/target_zcu102/code/src/
./build/../target_vck190/target_zcu102/code/src/main.cc
./build/../target_vck190/target_zcu102/code/src/app_mt.py
./build/../target_vck190/target_zcu102/code/src/main_mt.cc
./build/../target_vck190/target_zcu102/code/common/
./build/../target_vck190/target_zcu102/code/common/common.cpp
./build/../target_vck190/target_zcu102/code/common/common.h
./build/../target_vck190/target_zcu102/code/build_app.sh
./build/../target_vck190/target_zcu102/fcn8/
./build/../target_vck190/target_zcu102/fcn8/model/
./build/../target_vck190/target_zcu102/fcn8/model/fcn8.xmodel
./build/../target_vck190/target_zcu102/fcn8/model/config/
./build/../target_vck190/target_zcu102/fcn8/model/config/dpu_fcn_config.py
./build/../target_vck190/target_zcu102/fcn8/model/config/__init__.py
./build/../target_vck190/target_zcu102/fcn8/model/dpu_fcn8.elf
./build/../target_vck190/target_zcu102/unet/
./build/../target_vck190/target_zcu102/unet/v1/
./build/../target_vck190/target_zcu102/unet/v1/empty.md
./build/../target_vck190/target_zcu102/unet/v1/model/
./build/../target_vck190/target_zcu102/unet/v1/model/empty.md
./build/../target_vck190/target_zcu102/unet/v3/
./build/../target_vck190/target_zcu102/unet/v3/empty.md
./build/../target_vck190/target_zcu102/unet/v3/model/
./build/../target_vck190/target_zcu102/unet/v3/model/empty.md
./build/../target_vck190/target_zcu102/unet/v2/
./build/../target_vck190/target_zcu102/unet/v2/model/
./build/../target_vck190/target_zcu102/unet/v2/model/unet2.xmodel
./build/../target_vck190/target_zcu102/unet/v2/model/dbg_unet2.xmodel
./build/../target_vck190/target_zcu102/unet/v2/model/dpu_unet2.elf
./build/../target_vck190/target_zcu102/fcn8ups/
./build/../target_vck190/target_zcu102/fcn8ups/model/
./build/../target_vck190/target_zcu102/fcn8ups/model/fcn8ups.xmodel
./build/../target_vck190/target_zcu102/fcn8ups/model/config/
./build/../target_vck190/target_zcu102/fcn8ups/model/config/dpu_fcn_config.py
./build/../target_vck190/target_zcu102/fcn8ups/model/config/__init__.py
./build/../target_vck190/target_zcu102/fcn8ups/model/dpu_fcn8ups.elf
./build/../target_vck190/target_zcu102/rpt/
./build/../target_vck190/target_zcu102/rpt/empty.md
./build/../target_vck190/target_zcu102/rpt/logfile_py_fps.txt
./build/../target_vck190/target_zcu102/run_all_target.sh
./build/../target_vck190/target_zcu102/run_all.sh
tar: Removing leading `./build/../' from member names
./build/../target_zcu102/
./build/../target_zcu102/test.tar.gz
./build/../target_zcu102/code/
./build/../target_zcu102/code/build.sh
./build/../target_zcu102/code/run_cnn_py_fps.sh
./build/../target_zcu102/code/src/
./build/../target_zcu102/code/src/main.cc
./build/../target_zcu102/code/src/app_mt.py
./build/../target_zcu102/code/src/main_mt.cc
./build/../target_zcu102/code/common/
./build/../target_zcu102/code/common/common.cpp
./build/../target_zcu102/code/common/common.h
./build/../target_zcu102/code/build_app.sh
./build/../target_zcu102/fcn8/
./build/../target_zcu102/fcn8/model/
./build/../target_zcu102/fcn8/model/fcn8.xmodel
./build/../target_zcu102/fcn8/model/config/
./build/../target_zcu102/fcn8/model/config/dpu_fcn_config.py
./build/../target_zcu102/fcn8/model/config/__init__.py
./build/../target_zcu102/fcn8/model/dpu_fcn8.elf
./build/../target_zcu102/unet/
./build/../target_zcu102/unet/v1/
./build/../target_zcu102/unet/v1/empty.md
./build/../target_zcu102/unet/v1/model/
./build/../target_zcu102/unet/v1/model/empty.md
./build/../target_zcu102/unet/v3/
./build/../target_zcu102/unet/v3/empty.md
./build/../target_zcu102/unet/v3/model/
./build/../target_zcu102/unet/v3/model/empty.md
./build/../target_zcu102/unet/v2/
./build/../target_zcu102/unet/v2/model/
./build/../target_zcu102/unet/v2/model/unet2.xmodel
./build/../target_zcu102/unet/v2/model/dbg_unet2.xmodel
./build/../target_zcu102/unet/v2/model/dpu_unet2.elf
./build/../target_zcu102/fcn8ups/
./build/../target_zcu102/fcn8ups/model/
./build/../target_zcu102/fcn8ups/model/fcn8ups.xmodel
./build/../target_zcu102/fcn8ups/model/config/
./build/../target_zcu102/fcn8ups/model/config/dpu_fcn_config.py
./build/../target_zcu102/fcn8ups/model/config/__init__.py
./build/../target_zcu102/fcn8ups/model/dpu_fcn8ups.elf
./build/../target_zcu102/rpt/
./build/../target_zcu102/rpt/empty.md
./build/../target_zcu102/rpt/logfile_py_fps.txt
./build/../target_zcu102/run_all_target.sh
./build/../target_zcu102/run_all.sh
tar: Removing leading `./build/../' from member names
./build/../target_zcu104/
./build/../target_zcu104/test.tar.gz
./build/../target_zcu104/code/
./build/../target_zcu104/code/run_cnn_py_fps.sh
./build/../target_zcu104/code/src/
./build/../target_zcu104/code/src/main.cc
./build/../target_zcu104/code/src/app_mt.py
./build/../target_zcu104/code/common/
./build/../target_zcu104/code/common/common.cpp
./build/../target_zcu104/code/common/common.h
./build/../target_zcu104/code/build_app.sh
./build/../target_zcu104/fcn8/
./build/../target_zcu104/fcn8/model/
./build/../target_zcu104/fcn8/model/fcn8.xmodel
./build/../target_zcu104/unet/
./build/../target_zcu104/unet/v1/
./build/../target_zcu104/unet/v1/empty.md
./build/../target_zcu104/unet/v3/
./build/../target_zcu104/unet/v3/empty.md
./build/../target_zcu104/unet/v2/
./build/../target_zcu104/unet/v2/model/
./build/../target_zcu104/unet/v2/model/unet2.xmodel
./build/../target_zcu104/unet/v2/model/dbg_unet2.xmodel
./build/../target_zcu104/fcn8ups/
./build/../target_zcu104/fcn8ups/model/
./build/../target_zcu104/fcn8ups/model/fcn8ups.xmodel
./build/../target_zcu104/rpt/
./build/../target_zcu104/rpt/empty.md
./build/../target_zcu104/run_all_target.sh
./build/../target_zcu104/target_zcu102/
./build/../target_zcu104/target_zcu102/test.tar.gz
./build/../target_zcu104/target_zcu102/code/
./build/../target_zcu104/target_zcu102/code/build.sh
./build/../target_zcu104/target_zcu102/code/run_cnn_py_fps.sh
./build/../target_zcu104/target_zcu102/code/src/
./build/../target_zcu104/target_zcu102/code/src/main.cc
./build/../target_zcu104/target_zcu102/code/src/app_mt.py
./build/../target_zcu104/target_zcu102/code/src/main_mt.cc
./build/../target_zcu104/target_zcu102/code/common/
./build/../target_zcu104/target_zcu102/code/common/common.cpp
./build/../target_zcu104/target_zcu102/code/common/common.h
./build/../target_zcu104/target_zcu102/code/build_app.sh
./build/../target_zcu104/target_zcu102/fcn8/
./build/../target_zcu104/target_zcu102/fcn8/model/
./build/../target_zcu104/target_zcu102/fcn8/model/fcn8.xmodel
./build/../target_zcu104/target_zcu102/fcn8/model/config/
./build/../target_zcu104/target_zcu102/fcn8/model/config/dpu_fcn_config.py
./build/../target_zcu104/target_zcu102/fcn8/model/config/__init__.py
./build/../target_zcu104/target_zcu102/fcn8/model/dpu_fcn8.elf
./build/../target_zcu104/target_zcu102/unet/
./build/../target_zcu104/target_zcu102/unet/v1/
./build/../target_zcu104/target_zcu102/unet/v1/empty.md
./build/../target_zcu104/target_zcu102/unet/v1/model/
./build/../target_zcu104/target_zcu102/unet/v1/model/empty.md
./build/../target_zcu104/target_zcu102/unet/v3/
./build/../target_zcu104/target_zcu102/unet/v3/empty.md
./build/../target_zcu104/target_zcu102/unet/v3/model/
./build/../target_zcu104/target_zcu102/unet/v3/model/empty.md
./build/../target_zcu104/target_zcu102/unet/v2/
./build/../target_zcu104/target_zcu102/unet/v2/model/
./build/../target_zcu104/target_zcu102/unet/v2/model/unet2.xmodel
./build/../target_zcu104/target_zcu102/unet/v2/model/dbg_unet2.xmodel
./build/../target_zcu104/target_zcu102/unet/v2/model/dpu_unet2.elf
./build/../target_zcu104/target_zcu102/fcn8ups/
./build/../target_zcu104/target_zcu102/fcn8ups/model/
./build/../target_zcu104/target_zcu102/fcn8ups/model/fcn8ups.xmodel
./build/../target_zcu104/target_zcu102/fcn8ups/model/config/
./build/../target_zcu104/target_zcu102/fcn8ups/model/config/dpu_fcn_config.py
./build/../target_zcu104/target_zcu102/fcn8ups/model/config/__init__.py
./build/../target_zcu104/target_zcu102/fcn8ups/model/dpu_fcn8ups.elf
./build/../target_zcu104/target_zcu102/rpt/
./build/../target_zcu104/target_zcu102/rpt/empty.md
./build/../target_zcu104/target_zcu102/rpt/logfile_py_fps.txt
./build/../target_zcu104/target_zcu102/run_all_target.sh
./build/../target_zcu104/target_zcu102/run_all.sh
#####################################
MAIN UNET FLOW COMPLETED
#####################################
